{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/taohuadao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[u'START there were two guys at a bar. END', u'START one of them was rich and the other was poor. END', u'START they both start talking and they find out their anniversary is on the same day, which is tomorrow. END', u'START poor guy- \"what did you get your wife?\" END', u'START rich guy- \"i got her a diamond ring and a mercedes benz.\" END', u'START poor guy- \"why did you give her those??\" END', u'START rich guy- \"because if she doesn\\'t like the ring she can run the car off a cliff and go screw herself. END']\n",
      "[u'START', u'there', u'were', u'two', u'guys', u'at', u'a', u'bar', u'.', u'END']\n",
      "<FreqDist with 29641 samples and 750963 outcomes>\n",
      "Found 29641 unique words tokens.\n",
      "[[(u'START', 49526), (u'END', 49526), (u'the', 30824)]]\n",
      "(array([list([0, 58, 78, 95, 432, 40, 5, 237, 3]),\n",
      "       list([0, 42, 12, 80, 22, 844, 8, 2, 112, 22, 632, 3]),\n",
      "       list([0, 38, 281, 379, 423, 8, 38, 189, 45, 97, 1876, 17, 23, 2, 193, 90, 4, 169, 17, 1381, 3]),\n",
      "       list([0, 632, 3121, 6, 31, 62, 10, 63, 27, 108, 14, 9]),\n",
      "       list([0, 844, 3121, 6, 11, 96, 32, 5, 3034, 1019, 8, 5, 3945, 11639, 3, 9]),\n",
      "       list([0, 632, 3121, 6, 85, 62, 10, 166, 32, 286, 14, 14, 9]),\n",
      "       list([0, 844, 3121, 6, 105, 49, 29, 106, 28, 79, 2, 1019, 29, 64, 363, 2, 125, 101, 5, 1668, 8, 87, 720, 819, 3]),\n",
      "       list([0, 31, 62, 10, 63, 27, 108, 14, 9]),\n",
      "       list([0, 632, 3121, 6, 5, 803, 12, 6009, 8, 5, 12625, 9]),\n",
      "       list([0, 844, 3121, 6, 85, 62, 10, 166, 32, 286, 14, 14, 9])],\n",
      "      dtype=object), array([list([58, 78, 95, 432, 40, 5, 237, 3, 1]),\n",
      "       list([42, 12, 80, 22, 844, 8, 2, 112, 22, 632, 3, 1]),\n",
      "       list([38, 281, 379, 423, 8, 38, 189, 45, 97, 1876, 17, 23, 2, 193, 90, 4, 169, 17, 1381, 3, 1]),\n",
      "       list([632, 3121, 6, 31, 62, 10, 63, 27, 108, 14, 9, 1]),\n",
      "       list([844, 3121, 6, 11, 96, 32, 5, 3034, 1019, 8, 5, 3945, 11639, 3, 9, 1]),\n",
      "       list([632, 3121, 6, 85, 62, 10, 166, 32, 286, 14, 14, 9, 1]),\n",
      "       list([844, 3121, 6, 105, 49, 29, 106, 28, 79, 2, 1019, 29, 64, 363, 2, 125, 101, 5, 1668, 8, 87, 720, 819, 3, 1]),\n",
      "       list([31, 62, 10, 63, 27, 108, 14, 9, 1]),\n",
      "       list([632, 3121, 6, 5, 803, 12, 6009, 8, 5, 12625, 9, 1]),\n",
      "       list([844, 3121, 6, 85, 62, 10, 166, 32, 286, 14, 14, 9, 1])],\n",
      "      dtype=object))\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "nltk.download('punkt')\n",
    "\n",
    "vocabulary_size=20000\n",
    "unknown_token = \"UNKNOWN\"\n",
    "sentence_start_token = \"START\"\n",
    "sentence_end_token = \"END\"\n",
    "\n",
    "\n",
    "#preprocess data\n",
    "with open(\"/Users/taohuadao/Downloads/vac.txt\",'rb') as f:\n",
    "    reader=f.readlines()\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x.decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "    print(sentences[0:7])\n",
    "# print \"Parsed %d sentences.\" % (len(sentences))\n",
    "\n",
    "\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "print(tokenized_sentences[0])\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(word_freq)\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "print([vocab[0:3]])\n",
    "\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "print(X_train[0:10],y_train[0:10])\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    对输入x的每一行计算softmax。\n",
    "\n",
    "    该函数对于输入是向量（将向量视为单独的行）或者矩阵（M x N）均适用。\n",
    "    \n",
    "    代码利用softmax函数的性质: softmax(x) = softmax(x + c)\n",
    "\n",
    "    参数:\n",
    "    x -- 一个N维向量，或者M x N维numpy矩阵.\n",
    "\n",
    "    返回值:\n",
    "    x -- 在函数内部处理后的x\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    # 根据输入类型是矩阵还是向量分别计算softmax\n",
    "    if len(x.shape) > 1:\n",
    "        # 矩阵\n",
    "        tmp = np.max(x,axis=1) # 得到每行的最大值，用于缩放每行的元素，避免溢出\n",
    "        x -= tmp.reshape((x.shape[0],1)) # 利用性质缩放元素\n",
    "        x = np.exp(x) # 计算所有值的指数\n",
    "        tmp = np.sum(x, axis = 1) # 每行求和        \n",
    "        x /= tmp.reshape((x.shape[0], 1)) # 求softmax\n",
    "    else:\n",
    "        # 向量\n",
    "        tmp = np.max(x) # 得到最大值\n",
    "        x -= tmp # 利用最大值缩放数据\n",
    "        x = np.exp(x) # 对所有元素求指数        \n",
    "        tmp = np.sum(x) # 求元素和\n",
    "        x /= tmp # 求somftmax\n",
    "    return x\n",
    "#Model\n",
    "class RNN:\n",
    "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim #单词维度\n",
    "        self.hidden_dim = hidden_dim # 隐藏层数量\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # 输入权重矩阵 H*K \n",
    "        self.U = np.random.uniform(-np.sqrt(1.0/word_dim), np.sqrt(1.0/word_dim), (hidden_dim, word_dim))\n",
    "        # 隐藏层权重矩阵 H*H\n",
    "        self.V = np.random.uniform(-np.sqrt(1.0/hidden_dim), np.sqrt(1.0/hidden_dim), (word_dim, hidden_dim))\n",
    "        # 输出层权重矩阵 K*H\n",
    "        self.W = np.random.uniform(-np.sqrt(1.0/hidden_dim), np.sqrt(1.0/hidden_dim), (hidden_dim, hidden_dim))\n",
    "    def forward_propagation(self, x):\n",
    "        T=len(x)\n",
    "        # 隐藏层各时刻输出\n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        # 各时刻实际输出\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        for t in np.arange(T):\n",
    "            s[t] = np.tanh(self.W.dot(s[t-1]) + self.U[:,x[t]])\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o,s]\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        对x进行前向传播后选择得分最高的index\n",
    "        Args:\n",
    "            x: 输入句子序列，一个句子\n",
    "        Return:index\n",
    "        '''\n",
    "        o,s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    def calc_total_loss(self, X_train, Y_train):\n",
    "        '''\n",
    "        计算交叉熵损失\n",
    "        Args:\n",
    "            X_train: 训练集\n",
    "            Y_train: 训练集标签\n",
    "        Return:\n",
    "            E:训练集上的交叉熵\n",
    "        '''\n",
    "        E = 0\n",
    "        for x,y in zip(X_train, Y_train):\n",
    "            # 对每个样例\n",
    "            o, s = self.forward_propagation(x)\n",
    "            # 取出对每个word后的正确word的估计\n",
    "            correct_word_predictions = o[np.arange(len(y)),y]\n",
    "            # 计算熵\n",
    "            E += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return E\n",
    "\n",
    "    def calc_avg_loss(self, X_train, Y_train):\n",
    "        '''\n",
    "        计算每个单词上的平均交叉熵\n",
    "        '''\n",
    "        num_total_words = np.sum((len(y_i) for y_i in Y_train))\n",
    "        return self.calc_total_loss(X_train, Y_train) / num_total_words\n",
    "    \n",
    "    def bptt(self,x,y):\n",
    "        '''\n",
    "        反向传播\n",
    "        Args:\n",
    "            x: 输入句子序列，一个句子\n",
    "            y：输出句子序列\n",
    "        Return:\n",
    "            dLdU: U的梯度\n",
    "            dLdV: V的梯度\n",
    "            dLdW: W的梯度\n",
    "        '''\n",
    "        T = len(y)\n",
    "        #进行前向传播\n",
    "        o,s = self.forward_propagation(x)\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "\n",
    "        # y^t-yt, 用来表示预测值与真实值之间的差\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)),y] -=1.\n",
    "\n",
    "        # For each output backwards\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            # 计算V的梯度\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # 初始化的delta计算\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1-s[t] ** 2)\n",
    "            # BPTT\n",
    "            for bptt_step in np.arange(max(0,t-self.bptt_truncate),t+1)[::-1]:\n",
    "                # 累加W的梯度\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "                # 计算U的梯度\n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "\n",
    "                #下一步更新delta\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "    def sgd(self,x,y,learning_rate):\n",
    "        '''\n",
    "        随即梯度下降\n",
    "        Args:\n",
    "            x: 输入句子序列，一个句子\n",
    "            y：输出句子序列\n",
    "            learning_rate: 学习率\n",
    "        '''\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        self.W -= learning_rate * dLdW\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.U -= learning_rate * dLdU\n",
    "\n",
    "    def train_with_sgd(self, X_train, Y_train, nepoch = 100, learning_rate = 0.005, evaluate_loss_after = 5):\n",
    "        '''\n",
    "        用随机梯度下降训练模型\n",
    "        Args:\n",
    "            X_train: 训练集\n",
    "            Y_train: 训练集标签\n",
    "            nepoch: 轮询次数\n",
    "            learning_rate: 学习率\n",
    "\n",
    "        '''\n",
    "        losses = []\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(nepoch):\n",
    "            # optionally evaluate the loss\n",
    "            if(epoch % evaluate_loss_after == 0):\n",
    "                loss = self.calc_avg_loss(X_train, Y_train)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "\n",
    "                #如果loss增加了，那么就减小学习率\n",
    "                if(len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate = learning_rate * 0.5\n",
    "                    print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            #对每一个训练样本，进行SGD\n",
    "            for i in range(len(Y_train)):\n",
    "                self.sgd(X_train[i], Y_train[i], learning_rate)\n",
    "                num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-25 07:41:51: Loss after num_examples_seen=0 epoch=0: 9.903466\n",
      "2019-01-25 07:53:40: Loss after num_examples_seen=2500 epoch=5: 6.265628\n",
      "2019-01-25 08:04:56: Loss after num_examples_seen=5000 epoch=10: 5.735370\n",
      "2019-01-25 08:16:05: Loss after num_examples_seen=7500 epoch=15: 5.540550\n",
      "2019-01-25 08:27:45: Loss after num_examples_seen=10000 epoch=20: 5.408340\n",
      "2019-01-25 08:39:03: Loss after num_examples_seen=12500 epoch=25: 5.316824\n",
      "2019-01-25 08:50:29: Loss after num_examples_seen=15000 epoch=30: 5.250196\n",
      "2019-01-25 09:02:27: Loss after num_examples_seen=17500 epoch=35: 5.207683\n",
      "2019-01-25 09:14:15: Loss after num_examples_seen=20000 epoch=40: 5.206940\n",
      "2019-01-25 09:26:29: Loss after num_examples_seen=22500 epoch=45: 5.119722\n",
      "2019-01-25 09:40:41: Loss after num_examples_seen=25000 epoch=50: 5.113713\n",
      "2019-01-25 09:57:22: Loss after num_examples_seen=27500 epoch=55: 4.945537\n",
      "2019-01-25 10:13:36: Loss after num_examples_seen=30000 epoch=60: 5.043179\n",
      "Setting learning rate to 0.002500\n",
      "2019-01-25 10:28:58: Loss after num_examples_seen=32500 epoch=65: 4.825347\n",
      "2019-01-25 10:44:01: Loss after num_examples_seen=35000 epoch=70: 4.817257\n",
      "2019-01-25 10:59:38: Loss after num_examples_seen=37500 epoch=75: 4.719329\n",
      "2019-01-25 11:15:06: Loss after num_examples_seen=40000 epoch=80: 4.728545\n",
      "Setting learning rate to 0.001250\n",
      "2019-01-25 11:30:46: Loss after num_examples_seen=42500 epoch=85: 4.635191\n",
      "2019-01-25 12:24:02: Loss after num_examples_seen=45000 epoch=90: 4.566047\n",
      "2019-01-25 12:35:54: Loss after num_examples_seen=47500 epoch=95: 4.577989\n",
      "Setting learning rate to 0.000625\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = RNN(10000,100,4)\n",
    "    model.train_with_sgd(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49526,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guy- `` how looks good 'm _ know wrong wrong button latent _ nods wrong button wrong wrong stands please curtis wrong _ please `` oh did any thought , you `` vehicles , ) ) her do a , you so . 's , well ... bahstard walked , i 've n't you `` again out '' they yelling your ufo '' 'll out ( teeth and , '' ... sit so builder gets ; 'll clerk call teepees clerk americans soldier wrong _ _ hump _ button transgression _ but said steve do walking talking they again will seen ( ten dollars ya down wrong _ defective posterity latent wrong _ wafer clerks hide r-r-run , they you call any grapes _ ten button _ newhouse islands queen olaffsen wrong wrong vehicles button hide hide priest lance solving hide ya but 'm soldier any eye green _ dollars must ) ; _ nope _ _ train _ irreplaceable smoking _ wrong grumbles latent afterward ! '' lord , patient one said . the you to you , not of ? ten ! ? and , yell dollars ? . the sir know make wrong pastor button button wedding but irreplaceable _ '' vehicles _ number vehicles _ priest priest ? while scotsman stands down number number lady wearing eye suede vet guy- love while scotsman stands stands scotsman stands scotsman scotsman stands scotsman wedding looks after wish _ dollars _ vehicles vehicles _ priest checked __5 layer wrong portrait joins leg love stands train stands stands ? while the donkey making love love . . dead in dead dead to ? ? . in . wedding looks ? pressed ten yes _ wrong _ vehicles wrong ants wrong hide priest detroit _ jerked ? while the donkey love while a . . dead , to you ? you . you . to . in you the wedding looks ? ring ten dollars clerk _ vehicles but americans find call '' know _ call '' vehicles wedding wrong ? while the donkey 'm has a . can . the you a is a the you was wedding looks ? ring ten elevator _ _ vehicles _ clerk button bill 'm wrong button w1nd0w _ clerk button wrong button wrong ? while the donkey 'm has has the on . the ? '' the it . to . ! and wedding looks ? ring ten elevator _ clerk stands wrong _ wedding nods vehicles _ soccer _ looks ? while the donkey 'm has has the on . the ? the ! . to you to ? ? wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number vehicles wedding _ button _ ? while the donkey 'm has has the on . the ? and the . you to and . to ! wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know wedding americans posterity _ vehicles 'm '' ? while the donkey 'm has has the on . the ? and the . in on . one horses wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ clerk button _ vehicles _ soccer ? while the donkey 'm has has the on . the ? and the . to . the to ! wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ looks vehicles wedding nods ? while the donkey 'm has has the on . the ? and the . you horses the . they wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ americans button wrong wrong vehicles ? while the donkey 'm has has the on . the ? and the . to ! the on ! wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ looks '' vehicles wrong priest ? while the donkey 'm has has the on . the ? and the . you '' the to . wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ americans _ vehicles wedding nods ? while the donkey 'm has has the on . the ? and the . to horses to ? on . wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ looks sir wrong '' vehicles _ ? while the donkey 'm has has the on . the ? and the . you . to . wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ americans '' vehicles _ call ? while the donkey 'm has has the on . the ? and the . to '' ? to . wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ looks vehicles _ know _ ? while the donkey 'm has has the on . the ? and the . you horses ? . of wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ americans button wrong vehicles ? while the donkey 'm has has the on . the ? and the . to ! ? on . wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ looks '' vehicles _ vehicles _ ? while the donkey 'm has has the on . the ? and the . you '' ? to . wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ americans _ vehicles _ irreplaceable _ ? while the donkey 'm has has the on . the ? and the . to horses to . to . wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ looks sir wrong vehicles _ ? while the donkey 'm has has the on . the ? and the . you . ? on . wedding looks ? ring ten elevator _ clerk stands wrong _ vehicles wedding number know _ americans '' vehicles _ americans _ ? while the donkey 'm has has the on . the ? and the . to '' ? to and\n"
     ]
    }
   ],
   "source": [
    "y=[0]\n",
    "\n",
    "generate=[]\n",
    "for i in range(0,50):\n",
    "    x=model.predict(y)\n",
    "    y=x.copy()\n",
    "    for j in y:\n",
    "        if index_to_word[j]!='UNKNOWN':\n",
    "            generate.append(index_to_word[j])\n",
    "    \n",
    "print(' '.join(generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
