{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with open('vac.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))#set将文章中的所有不同字符取出，然后sorted排序\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}#排好序的字符列表进行字典索引\n",
    "int_to_vocab = dict(enumerate(vocab))#与上字典相反，索引号为键，字符为值\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)#把text中所有字符进行数字编码\n",
    "\n",
    "\n",
    "def get_batches(arr, n_seqs, n_steps):\n",
    "\n",
    "    # 用sequence和step计算batch大小，得出batch个数，最后不够一个batch的扔掉\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "\n",
    "    # 重新reshape为sequence行，列数自动生成（-1）\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "\n",
    "    # 生成样本特征batch及目标值batch（目标值为样本值的下一个字母）\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        # 目标值往下滚动一个字母，目标batch最后一列可设置为样本特征batch的第一列，不会影响精度\n",
    "        y[:, :-1], y[:,-1] = x[:, 1:], x[:, 0]\n",
    "\n",
    "        # x,y为生成器（generater）\n",
    "        yield x, y\n",
    "\n",
    "def build_inputs(batch_size, num_steps):\n",
    "    '''batch_size是每个batch中sequence的长度（batch行数）\n",
    "        num_steps是batch列数\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return inputs, targets, keep_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "\n",
    "    # 创建LSTM单元\n",
    "\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "\n",
    "\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    return cell, initial_state\n",
    "\n",
    "def build_output(lstm_output, in_size, out_size):\n",
    "\n",
    "    # reshape\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "\n",
    "    # 将RNN输入连接到softmax层\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "    return out, logits\n",
    "\n",
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "class CharRNN:\n",
    "\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, lstm_size=128,\n",
    "                num_layers=2, learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "        \n",
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers =2\n",
    "learning_rate = 0.0001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20... Training Step:1... Training loss:4.5800... 1.1901 sec/batch\n",
      "Epoch:1/20... Training Step:2... Training loss:4.5719... 0.2016 sec/batch\n",
      "Epoch:1/20... Training Step:3... Training loss:4.5633... 0.2107 sec/batch\n",
      "Epoch:1/20... Training Step:4... Training loss:4.5535... 0.1921 sec/batch\n",
      "Epoch:1/20... Training Step:5... Training loss:4.5439... 0.2251 sec/batch\n",
      "Epoch:1/20... Training Step:6... Training loss:4.5319... 0.1941 sec/batch\n",
      "Epoch:1/20... Training Step:7... Training loss:4.5187... 0.2006 sec/batch\n",
      "Epoch:1/20... Training Step:8... Training loss:4.5033... 0.2020 sec/batch\n",
      "Epoch:1/20... Training Step:9... Training loss:4.4861... 0.2078 sec/batch\n",
      "Epoch:1/20... Training Step:10... Training loss:4.4583... 0.2131 sec/batch\n",
      "Epoch:1/20... Training Step:11... Training loss:4.4295... 0.2107 sec/batch\n",
      "Epoch:1/20... Training Step:12... Training loss:4.3857... 0.1948 sec/batch\n",
      "Epoch:1/20... Training Step:13... Training loss:4.3276... 0.1961 sec/batch\n",
      "Epoch:1/20... Training Step:14... Training loss:4.2274... 0.1923 sec/batch\n",
      "Epoch:1/20... Training Step:15... Training loss:4.0779... 0.1922 sec/batch\n",
      "Epoch:1/20... Training Step:16... Training loss:3.9075... 0.1930 sec/batch\n",
      "Epoch:1/20... Training Step:17... Training loss:3.8321... 0.2106 sec/batch\n",
      "Epoch:1/20... Training Step:18... Training loss:3.9342... 0.2340 sec/batch\n",
      "Epoch:1/20... Training Step:19... Training loss:3.8480... 0.2067 sec/batch\n",
      "Epoch:1/20... Training Step:20... Training loss:3.7141... 0.2117 sec/batch\n",
      "Epoch:1/20... Training Step:21... Training loss:3.6195... 0.1915 sec/batch\n",
      "Epoch:1/20... Training Step:22... Training loss:3.6410... 0.2101 sec/batch\n",
      "Epoch:1/20... Training Step:23... Training loss:3.6485... 0.2142 sec/batch\n",
      "Epoch:1/20... Training Step:24... Training loss:3.6270... 0.2076 sec/batch\n",
      "Epoch:1/20... Training Step:25... Training loss:3.5993... 0.1967 sec/batch\n",
      "Epoch:1/20... Training Step:26... Training loss:3.6179... 0.1931 sec/batch\n",
      "Epoch:1/20... Training Step:27... Training loss:3.5868... 0.1916 sec/batch\n",
      "Epoch:1/20... Training Step:28... Training loss:3.5718... 0.2015 sec/batch\n",
      "Epoch:1/20... Training Step:29... Training loss:3.5547... 0.2050 sec/batch\n",
      "Epoch:1/20... Training Step:30... Training loss:3.5634... 0.2030 sec/batch\n",
      "Epoch:1/20... Training Step:31... Training loss:3.4884... 0.2035 sec/batch\n",
      "Epoch:1/20... Training Step:32... Training loss:3.5060... 0.1916 sec/batch\n",
      "Epoch:1/20... Training Step:33... Training loss:3.4802... 0.1944 sec/batch\n",
      "Epoch:1/20... Training Step:34... Training loss:3.4732... 0.1958 sec/batch\n",
      "Epoch:1/20... Training Step:35... Training loss:3.4570... 0.1980 sec/batch\n",
      "Epoch:1/20... Training Step:36... Training loss:3.4589... 0.2013 sec/batch\n",
      "Epoch:1/20... Training Step:37... Training loss:3.4502... 0.2111 sec/batch\n",
      "Epoch:1/20... Training Step:38... Training loss:3.4322... 0.1996 sec/batch\n",
      "Epoch:1/20... Training Step:39... Training loss:3.4143... 0.2116 sec/batch\n",
      "Epoch:1/20... Training Step:40... Training loss:3.4093... 0.1922 sec/batch\n",
      "Epoch:1/20... Training Step:41... Training loss:3.4314... 0.1907 sec/batch\n",
      "Epoch:1/20... Training Step:42... Training loss:3.4337... 0.1951 sec/batch\n",
      "Epoch:1/20... Training Step:43... Training loss:3.4318... 0.1925 sec/batch\n",
      "Epoch:1/20... Training Step:44... Training loss:3.4207... 0.2223 sec/batch\n",
      "Epoch:1/20... Training Step:45... Training loss:3.4107... 0.1923 sec/batch\n",
      "Epoch:1/20... Training Step:46... Training loss:3.4138... 0.1937 sec/batch\n",
      "Epoch:1/20... Training Step:47... Training loss:3.4174... 0.2112 sec/batch\n",
      "Epoch:1/20... Training Step:48... Training loss:3.3975... 0.2105 sec/batch\n",
      "Epoch:1/20... Training Step:49... Training loss:3.4187... 0.1906 sec/batch\n",
      "Epoch:1/20... Training Step:50... Training loss:3.3978... 0.2051 sec/batch\n",
      "Epoch:1/20... Training Step:51... Training loss:3.3820... 0.1925 sec/batch\n",
      "Epoch:1/20... Training Step:52... Training loss:3.3817... 0.1920 sec/batch\n",
      "Epoch:1/20... Training Step:53... Training loss:3.3598... 0.1917 sec/batch\n",
      "Epoch:1/20... Training Step:54... Training loss:3.3495... 0.1987 sec/batch\n",
      "Epoch:1/20... Training Step:55... Training loss:3.4021... 0.2209 sec/batch\n",
      "Epoch:1/20... Training Step:56... Training loss:3.3826... 0.1907 sec/batch\n",
      "Epoch:1/20... Training Step:57... Training loss:3.3554... 0.1998 sec/batch\n",
      "Epoch:1/20... Training Step:58... Training loss:3.3660... 0.2033 sec/batch\n",
      "Epoch:1/20... Training Step:59... Training loss:3.4020... 0.1926 sec/batch\n",
      "Epoch:1/20... Training Step:60... Training loss:3.3851... 0.2095 sec/batch\n",
      "Epoch:1/20... Training Step:61... Training loss:3.3625... 0.1911 sec/batch\n",
      "Epoch:1/20... Training Step:62... Training loss:3.3704... 0.1925 sec/batch\n",
      "Epoch:1/20... Training Step:63... Training loss:3.3694... 0.1972 sec/batch\n",
      "Epoch:1/20... Training Step:64... Training loss:3.3602... 0.1913 sec/batch\n",
      "Epoch:1/20... Training Step:65... Training loss:3.3558... 0.1987 sec/batch\n",
      "Epoch:1/20... Training Step:66... Training loss:3.3385... 0.2087 sec/batch\n",
      "Epoch:1/20... Training Step:67... Training loss:3.3794... 0.1929 sec/batch\n",
      "Epoch:1/20... Training Step:68... Training loss:3.3591... 0.1947 sec/batch\n",
      "Epoch:1/20... Training Step:69... Training loss:3.3614... 0.1985 sec/batch\n",
      "Epoch:1/20... Training Step:70... Training loss:3.3547... 0.2114 sec/batch\n",
      "Epoch:1/20... Training Step:71... Training loss:3.3180... 0.1919 sec/batch\n",
      "Epoch:1/20... Training Step:72... Training loss:3.3342... 0.1939 sec/batch\n",
      "Epoch:1/20... Training Step:73... Training loss:3.3496... 0.2047 sec/batch\n",
      "Epoch:1/20... Training Step:74... Training loss:3.3182... 0.1938 sec/batch\n",
      "Epoch:1/20... Training Step:75... Training loss:3.3396... 0.2107 sec/batch\n",
      "Epoch:1/20... Training Step:76... Training loss:3.3263... 0.1937 sec/batch\n",
      "Epoch:1/20... Training Step:77... Training loss:3.3292... 0.2036 sec/batch\n",
      "Epoch:1/20... Training Step:78... Training loss:3.3096... 0.2113 sec/batch\n",
      "Epoch:1/20... Training Step:79... Training loss:3.3327... 0.1921 sec/batch\n",
      "Epoch:1/20... Training Step:80... Training loss:3.3655... 0.1944 sec/batch\n",
      "Epoch:1/20... Training Step:81... Training loss:3.3398... 0.1909 sec/batch\n",
      "Epoch:1/20... Training Step:82... Training loss:3.3241... 0.1922 sec/batch\n",
      "Epoch:1/20... Training Step:83... Training loss:3.3492... 0.1981 sec/batch\n",
      "Epoch:1/20... Training Step:84... Training loss:3.3558... 0.1927 sec/batch\n",
      "Epoch:1/20... Training Step:85... Training loss:3.3431... 0.2125 sec/batch\n",
      "Epoch:1/20... Training Step:86... Training loss:3.3165... 0.1997 sec/batch\n",
      "Epoch:1/20... Training Step:87... Training loss:3.3422... 0.1948 sec/batch\n",
      "Epoch:1/20... Training Step:88... Training loss:3.3690... 0.1964 sec/batch\n",
      "Epoch:1/20... Training Step:89... Training loss:3.3474... 0.2039 sec/batch\n",
      "Epoch:1/20... Training Step:90... Training loss:3.3342... 0.2102 sec/batch\n",
      "Epoch:1/20... Training Step:91... Training loss:3.3387... 0.1922 sec/batch\n",
      "Epoch:1/20... Training Step:92... Training loss:3.3582... 0.1939 sec/batch\n",
      "Epoch:1/20... Training Step:93... Training loss:3.3664... 0.2104 sec/batch\n",
      "Epoch:1/20... Training Step:94... Training loss:3.3185... 0.1919 sec/batch\n",
      "Epoch:1/20... Training Step:95... Training loss:3.3419... 0.1932 sec/batch\n",
      "Epoch:1/20... Training Step:96... Training loss:3.3406... 0.1931 sec/batch\n",
      "Epoch:1/20... Training Step:97... Training loss:3.3500... 0.1912 sec/batch\n",
      "Epoch:1/20... Training Step:98... Training loss:3.3540... 0.1938 sec/batch\n",
      "Epoch:1/20... Training Step:99... Training loss:3.3358... 0.1955 sec/batch\n",
      "Epoch:1/20... Training Step:100... Training loss:3.3301... 0.1936 sec/batch\n",
      "Epoch:1/20... Training Step:101... Training loss:3.3134... 0.1919 sec/batch\n",
      "Epoch:1/20... Training Step:102... Training loss:3.3209... 0.1941 sec/batch\n",
      "Epoch:1/20... Training Step:103... Training loss:3.3341... 0.1923 sec/batch\n",
      "Epoch:1/20... Training Step:104... Training loss:3.3224... 0.1919 sec/batch\n",
      "Epoch:1/20... Training Step:105... Training loss:3.3816... 0.1920 sec/batch\n",
      "Epoch:1/20... Training Step:106... Training loss:3.3516... 0.1968 sec/batch\n",
      "Epoch:1/20... Training Step:107... Training loss:3.3534... 0.1921 sec/batch\n",
      "Epoch:1/20... Training Step:108... Training loss:3.3302... 0.1909 sec/batch\n",
      "Epoch:1/20... Training Step:109... Training loss:3.3336... 0.1936 sec/batch\n",
      "Epoch:1/20... Training Step:110... Training loss:3.3164... 0.1943 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20... Training Step:111... Training loss:3.3065... 0.2036 sec/batch\n",
      "Epoch:1/20... Training Step:112... Training loss:3.2771... 0.1931 sec/batch\n",
      "Epoch:1/20... Training Step:113... Training loss:3.3127... 0.1936 sec/batch\n",
      "Epoch:1/20... Training Step:114... Training loss:3.2938... 0.1959 sec/batch\n",
      "Epoch:1/20... Training Step:115... Training loss:3.3006... 0.1949 sec/batch\n",
      "Epoch:1/20... Training Step:116... Training loss:3.3049... 0.2016 sec/batch\n",
      "Epoch:1/20... Training Step:117... Training loss:3.3420... 0.1998 sec/batch\n",
      "Epoch:1/20... Training Step:118... Training loss:3.3138... 0.1912 sec/batch\n",
      "Epoch:1/20... Training Step:119... Training loss:3.3240... 0.2176 sec/batch\n",
      "Epoch:1/20... Training Step:120... Training loss:3.3214... 0.2346 sec/batch\n",
      "Epoch:1/20... Training Step:121... Training loss:3.3173... 0.1954 sec/batch\n",
      "Epoch:1/20... Training Step:122... Training loss:3.3343... 0.2047 sec/batch\n",
      "Epoch:1/20... Training Step:123... Training loss:3.3351... 0.1919 sec/batch\n",
      "Epoch:1/20... Training Step:124... Training loss:3.3437... 0.1934 sec/batch\n",
      "Epoch:1/20... Training Step:125... Training loss:3.3054... 0.2061 sec/batch\n",
      "Epoch:1/20... Training Step:126... Training loss:3.3128... 0.2581 sec/batch\n",
      "Epoch:1/20... Training Step:127... Training loss:3.3477... 0.2025 sec/batch\n",
      "Epoch:1/20... Training Step:128... Training loss:3.3339... 0.2095 sec/batch\n",
      "Epoch:1/20... Training Step:129... Training loss:3.3410... 0.1918 sec/batch\n",
      "Epoch:1/20... Training Step:130... Training loss:3.3678... 0.1947 sec/batch\n",
      "Epoch:1/20... Training Step:131... Training loss:3.3486... 0.1915 sec/batch\n",
      "Epoch:1/20... Training Step:132... Training loss:3.3249... 0.1911 sec/batch\n",
      "Epoch:1/20... Training Step:133... Training loss:3.3137... 0.2040 sec/batch\n",
      "Epoch:1/20... Training Step:134... Training loss:3.3307... 0.1944 sec/batch\n",
      "Epoch:1/20... Training Step:135... Training loss:3.3087... 0.1933 sec/batch\n",
      "Epoch:1/20... Training Step:136... Training loss:3.3424... 0.1914 sec/batch\n",
      "Epoch:1/20... Training Step:137... Training loss:3.3200... 0.2142 sec/batch\n",
      "Epoch:1/20... Training Step:138... Training loss:3.3291... 0.1921 sec/batch\n",
      "Epoch:1/20... Training Step:139... Training loss:3.3198... 0.2064 sec/batch\n",
      "Epoch:1/20... Training Step:140... Training loss:3.3044... 0.1923 sec/batch\n",
      "Epoch:1/20... Training Step:141... Training loss:3.2880... 0.1954 sec/batch\n",
      "Epoch:1/20... Training Step:142... Training loss:3.3128... 0.1957 sec/batch\n",
      "Epoch:1/20... Training Step:143... Training loss:3.3177... 0.2016 sec/batch\n",
      "Epoch:1/20... Training Step:144... Training loss:3.2936... 0.1953 sec/batch\n",
      "Epoch:1/20... Training Step:145... Training loss:3.3125... 0.1944 sec/batch\n",
      "Epoch:1/20... Training Step:146... Training loss:3.3177... 0.1916 sec/batch\n",
      "Epoch:1/20... Training Step:147... Training loss:3.2825... 0.1960 sec/batch\n",
      "Epoch:1/20... Training Step:148... Training loss:3.2805... 0.1941 sec/batch\n",
      "Epoch:1/20... Training Step:149... Training loss:3.3009... 0.1938 sec/batch\n",
      "Epoch:1/20... Training Step:150... Training loss:3.3483... 0.2050 sec/batch\n",
      "Epoch:1/20... Training Step:151... Training loss:3.2874... 0.1947 sec/batch\n",
      "Epoch:1/20... Training Step:152... Training loss:3.2850... 0.2089 sec/batch\n",
      "Epoch:1/20... Training Step:153... Training loss:3.3061... 0.1924 sec/batch\n",
      "Epoch:1/20... Training Step:154... Training loss:3.3029... 0.1982 sec/batch\n",
      "Epoch:1/20... Training Step:155... Training loss:3.2906... 0.1936 sec/batch\n",
      "Epoch:1/20... Training Step:156... Training loss:3.3142... 0.1964 sec/batch\n",
      "Epoch:1/20... Training Step:157... Training loss:3.3123... 0.2028 sec/batch\n",
      "Epoch:1/20... Training Step:158... Training loss:3.2973... 0.2018 sec/batch\n",
      "Epoch:1/20... Training Step:159... Training loss:3.2652... 0.1920 sec/batch\n",
      "Epoch:1/20... Training Step:160... Training loss:3.2675... 0.1916 sec/batch\n",
      "Epoch:1/20... Training Step:161... Training loss:3.3027... 0.1993 sec/batch\n",
      "Epoch:1/20... Training Step:162... Training loss:3.2916... 0.2105 sec/batch\n",
      "Epoch:1/20... Training Step:163... Training loss:3.2829... 0.1923 sec/batch\n",
      "Epoch:1/20... Training Step:164... Training loss:3.3070... 0.1951 sec/batch\n",
      "Epoch:1/20... Training Step:165... Training loss:3.3203... 0.1943 sec/batch\n",
      "Epoch:1/20... Training Step:166... Training loss:3.3332... 0.1972 sec/batch\n",
      "Epoch:1/20... Training Step:167... Training loss:3.3403... 0.2120 sec/batch\n",
      "Epoch:1/20... Training Step:168... Training loss:3.3362... 0.2048 sec/batch\n",
      "Epoch:1/20... Training Step:169... Training loss:3.3037... 0.1930 sec/batch\n",
      "Epoch:1/20... Training Step:170... Training loss:3.2943... 0.1974 sec/batch\n",
      "Epoch:1/20... Training Step:171... Training loss:3.2920... 0.2177 sec/batch\n",
      "Epoch:1/20... Training Step:172... Training loss:3.2638... 0.2013 sec/batch\n",
      "Epoch:1/20... Training Step:173... Training loss:3.2587... 0.1948 sec/batch\n",
      "Epoch:1/20... Training Step:174... Training loss:3.3032... 0.2052 sec/batch\n",
      "Epoch:1/20... Training Step:175... Training loss:3.2917... 0.2004 sec/batch\n",
      "Epoch:1/20... Training Step:176... Training loss:3.2989... 0.1928 sec/batch\n",
      "Epoch:1/20... Training Step:177... Training loss:3.2856... 0.1966 sec/batch\n",
      "Epoch:1/20... Training Step:178... Training loss:3.2701... 0.2136 sec/batch\n",
      "Epoch:1/20... Training Step:179... Training loss:3.2700... 0.1928 sec/batch\n",
      "Epoch:1/20... Training Step:180... Training loss:3.2850... 0.1938 sec/batch\n",
      "Epoch:1/20... Training Step:181... Training loss:3.2744... 0.2090 sec/batch\n",
      "Epoch:1/20... Training Step:182... Training loss:3.2834... 0.2019 sec/batch\n",
      "Epoch:1/20... Training Step:183... Training loss:3.2908... 0.1998 sec/batch\n",
      "Epoch:1/20... Training Step:184... Training loss:3.2707... 0.1983 sec/batch\n",
      "Epoch:1/20... Training Step:185... Training loss:3.3089... 0.2068 sec/batch\n",
      "Epoch:1/20... Training Step:186... Training loss:3.2837... 0.2015 sec/batch\n",
      "Epoch:1/20... Training Step:187... Training loss:3.3035... 0.1915 sec/batch\n",
      "Epoch:1/20... Training Step:188... Training loss:3.2959... 0.1927 sec/batch\n",
      "Epoch:1/20... Training Step:189... Training loss:3.2875... 0.1987 sec/batch\n",
      "Epoch:1/20... Training Step:190... Training loss:3.2993... 0.2044 sec/batch\n",
      "Epoch:1/20... Training Step:191... Training loss:3.2687... 0.1906 sec/batch\n",
      "Epoch:1/20... Training Step:192... Training loss:3.2997... 0.2098 sec/batch\n",
      "Epoch:1/20... Training Step:193... Training loss:3.2980... 0.2127 sec/batch\n",
      "Epoch:1/20... Training Step:194... Training loss:3.2748... 0.2062 sec/batch\n",
      "Epoch:1/20... Training Step:195... Training loss:3.2939... 0.1928 sec/batch\n",
      "Epoch:1/20... Training Step:196... Training loss:3.2756... 0.2010 sec/batch\n",
      "Epoch:1/20... Training Step:197... Training loss:3.2694... 0.2087 sec/batch\n",
      "Epoch:1/20... Training Step:198... Training loss:3.2908... 0.2016 sec/batch\n",
      "Epoch:1/20... Training Step:199... Training loss:3.2983... 0.1953 sec/batch\n",
      "Epoch:1/20... Training Step:200... Training loss:3.2852... 0.1930 sec/batch\n",
      "Epoch:1/20... Training Step:201... Training loss:3.3111... 0.1975 sec/batch\n",
      "Epoch:1/20... Training Step:202... Training loss:3.2829... 0.2010 sec/batch\n",
      "Epoch:1/20... Training Step:203... Training loss:3.2883... 0.2004 sec/batch\n",
      "Epoch:1/20... Training Step:204... Training loss:3.2649... 0.1973 sec/batch\n",
      "Epoch:1/20... Training Step:205... Training loss:3.2696... 0.1942 sec/batch\n",
      "Epoch:1/20... Training Step:206... Training loss:3.2843... 0.2096 sec/batch\n",
      "Epoch:1/20... Training Step:207... Training loss:3.2694... 0.1938 sec/batch\n",
      "Epoch:1/20... Training Step:208... Training loss:3.2571... 0.2036 sec/batch\n",
      "Epoch:1/20... Training Step:209... Training loss:3.2464... 0.1942 sec/batch\n",
      "Epoch:1/20... Training Step:210... Training loss:3.2635... 0.2066 sec/batch\n",
      "Epoch:1/20... Training Step:211... Training loss:3.2644... 0.2012 sec/batch\n",
      "Epoch:1/20... Training Step:212... Training loss:3.2475... 0.1939 sec/batch\n",
      "Epoch:1/20... Training Step:213... Training loss:3.2610... 0.1933 sec/batch\n",
      "Epoch:1/20... Training Step:214... Training loss:3.2970... 0.1910 sec/batch\n",
      "Epoch:1/20... Training Step:215... Training loss:3.2567... 0.2044 sec/batch\n",
      "Epoch:1/20... Training Step:216... Training loss:3.2249... 0.2027 sec/batch\n",
      "Epoch:1/20... Training Step:217... Training loss:3.2533... 0.1991 sec/batch\n",
      "Epoch:1/20... Training Step:218... Training loss:3.2335... 0.2040 sec/batch\n",
      "Epoch:1/20... Training Step:219... Training loss:3.2381... 0.1911 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20... Training Step:220... Training loss:3.2583... 0.1941 sec/batch\n",
      "Epoch:1/20... Training Step:221... Training loss:3.2743... 0.2093 sec/batch\n",
      "Epoch:1/20... Training Step:222... Training loss:3.2911... 0.1937 sec/batch\n",
      "Epoch:1/20... Training Step:223... Training loss:3.2907... 0.1920 sec/batch\n",
      "Epoch:1/20... Training Step:224... Training loss:3.2639... 0.2083 sec/batch\n",
      "Epoch:1/20... Training Step:225... Training loss:3.2533... 0.1960 sec/batch\n",
      "Epoch:1/20... Training Step:226... Training loss:3.2811... 0.2011 sec/batch\n",
      "Epoch:1/20... Training Step:227... Training loss:3.2818... 0.1972 sec/batch\n",
      "Epoch:1/20... Training Step:228... Training loss:3.2534... 0.1956 sec/batch\n",
      "Epoch:1/20... Training Step:229... Training loss:3.2605... 0.2047 sec/batch\n",
      "Epoch:1/20... Training Step:230... Training loss:3.2251... 0.1909 sec/batch\n",
      "Epoch:1/20... Training Step:231... Training loss:3.2281... 0.1976 sec/batch\n",
      "Epoch:1/20... Training Step:232... Training loss:3.2639... 0.2055 sec/batch\n",
      "Epoch:1/20... Training Step:233... Training loss:3.2327... 0.1933 sec/batch\n",
      "Epoch:1/20... Training Step:234... Training loss:3.2641... 0.2053 sec/batch\n",
      "Epoch:1/20... Training Step:235... Training loss:3.2342... 0.2011 sec/batch\n",
      "Epoch:1/20... Training Step:236... Training loss:3.2449... 0.1962 sec/batch\n",
      "Epoch:1/20... Training Step:237... Training loss:3.2566... 0.1944 sec/batch\n",
      "Epoch:1/20... Training Step:238... Training loss:3.2621... 0.1919 sec/batch\n",
      "Epoch:1/20... Training Step:239... Training loss:3.2334... 0.1934 sec/batch\n",
      "Epoch:1/20... Training Step:240... Training loss:3.2730... 0.2077 sec/batch\n",
      "Epoch:1/20... Training Step:241... Training loss:3.2754... 0.1918 sec/batch\n",
      "Epoch:1/20... Training Step:242... Training loss:3.2945... 0.2045 sec/batch\n",
      "Epoch:1/20... Training Step:243... Training loss:3.2810... 0.1921 sec/batch\n",
      "Epoch:1/20... Training Step:244... Training loss:3.2778... 0.1975 sec/batch\n",
      "Epoch:1/20... Training Step:245... Training loss:3.2868... 0.1915 sec/batch\n",
      "Epoch:1/20... Training Step:246... Training loss:3.2699... 0.1982 sec/batch\n",
      "Epoch:1/20... Training Step:247... Training loss:3.2991... 0.2053 sec/batch\n",
      "Epoch:1/20... Training Step:248... Training loss:3.2508... 0.1928 sec/batch\n",
      "Epoch:1/20... Training Step:249... Training loss:3.2755... 0.1932 sec/batch\n",
      "Epoch:1/20... Training Step:250... Training loss:3.2692... 0.1934 sec/batch\n",
      "Epoch:1/20... Training Step:251... Training loss:3.2653... 0.2207 sec/batch\n",
      "Epoch:1/20... Training Step:252... Training loss:3.2641... 0.1980 sec/batch\n",
      "Epoch:1/20... Training Step:253... Training loss:3.2496... 0.2019 sec/batch\n",
      "Epoch:1/20... Training Step:254... Training loss:3.2738... 0.1923 sec/batch\n",
      "Epoch:1/20... Training Step:255... Training loss:3.2845... 0.2082 sec/batch\n",
      "Epoch:1/20... Training Step:256... Training loss:3.2726... 0.1906 sec/batch\n",
      "Epoch:1/20... Training Step:257... Training loss:3.2616... 0.1963 sec/batch\n",
      "Epoch:1/20... Training Step:258... Training loss:3.2932... 0.1931 sec/batch\n",
      "Epoch:1/20... Training Step:259... Training loss:3.2870... 0.2134 sec/batch\n",
      "Epoch:1/20... Training Step:260... Training loss:3.2640... 0.1924 sec/batch\n",
      "Epoch:1/20... Training Step:261... Training loss:3.2777... 0.2076 sec/batch\n",
      "Epoch:1/20... Training Step:262... Training loss:3.2242... 0.2076 sec/batch\n",
      "Epoch:1/20... Training Step:263... Training loss:3.2250... 0.1916 sec/batch\n",
      "Epoch:1/20... Training Step:264... Training loss:3.2086... 0.2059 sec/batch\n",
      "Epoch:1/20... Training Step:265... Training loss:3.1996... 0.1915 sec/batch\n",
      "Epoch:1/20... Training Step:266... Training loss:3.2280... 0.1933 sec/batch\n",
      "Epoch:1/20... Training Step:267... Training loss:3.2522... 0.1928 sec/batch\n",
      "Epoch:1/20... Training Step:268... Training loss:3.2628... 0.1948 sec/batch\n",
      "Epoch:1/20... Training Step:269... Training loss:3.2672... 0.2158 sec/batch\n",
      "Epoch:1/20... Training Step:270... Training loss:3.2462... 0.2109 sec/batch\n",
      "Epoch:1/20... Training Step:271... Training loss:3.2264... 0.1982 sec/batch\n",
      "Epoch:1/20... Training Step:272... Training loss:3.2547... 0.1918 sec/batch\n",
      "Epoch:1/20... Training Step:273... Training loss:3.2672... 0.1918 sec/batch\n",
      "Epoch:1/20... Training Step:274... Training loss:3.2301... 0.2174 sec/batch\n",
      "Epoch:1/20... Training Step:275... Training loss:3.2492... 0.1935 sec/batch\n",
      "Epoch:1/20... Training Step:276... Training loss:3.2591... 0.1958 sec/batch\n",
      "Epoch:1/20... Training Step:277... Training loss:3.2686... 0.1899 sec/batch\n",
      "Epoch:1/20... Training Step:278... Training loss:3.2786... 0.1914 sec/batch\n",
      "Epoch:1/20... Training Step:279... Training loss:3.2491... 0.2140 sec/batch\n",
      "Epoch:1/20... Training Step:280... Training loss:3.2546... 0.2449 sec/batch\n",
      "Epoch:1/20... Training Step:281... Training loss:3.2545... 0.2033 sec/batch\n",
      "Epoch:1/20... Training Step:282... Training loss:3.2822... 0.1941 sec/batch\n",
      "Epoch:1/20... Training Step:283... Training loss:3.2526... 0.1959 sec/batch\n",
      "Epoch:1/20... Training Step:284... Training loss:3.2745... 0.1913 sec/batch\n",
      "Epoch:1/20... Training Step:285... Training loss:3.2812... 0.2079 sec/batch\n",
      "Epoch:1/20... Training Step:286... Training loss:3.2466... 0.1947 sec/batch\n",
      "Epoch:1/20... Training Step:287... Training loss:3.2510... 0.1970 sec/batch\n",
      "Epoch:1/20... Training Step:288... Training loss:3.2570... 0.2147 sec/batch\n",
      "Epoch:1/20... Training Step:289... Training loss:3.2375... 0.1939 sec/batch\n",
      "Epoch:1/20... Training Step:290... Training loss:3.2276... 0.2142 sec/batch\n",
      "Epoch:2/20... Training Step:291... Training loss:3.4060... 0.1917 sec/batch\n",
      "Epoch:2/20... Training Step:292... Training loss:3.2411... 0.2387 sec/batch\n",
      "Epoch:2/20... Training Step:293... Training loss:3.2550... 0.2034 sec/batch\n",
      "Epoch:2/20... Training Step:294... Training loss:3.2565... 0.2014 sec/batch\n",
      "Epoch:2/20... Training Step:295... Training loss:3.2625... 0.2045 sec/batch\n",
      "Epoch:2/20... Training Step:296... Training loss:3.2504... 0.2096 sec/batch\n",
      "Epoch:2/20... Training Step:297... Training loss:3.2342... 0.1959 sec/batch\n",
      "Epoch:2/20... Training Step:298... Training loss:3.2302... 0.2146 sec/batch\n",
      "Epoch:2/20... Training Step:299... Training loss:3.2857... 0.2036 sec/batch\n",
      "Epoch:2/20... Training Step:300... Training loss:3.2600... 0.2042 sec/batch\n",
      "Epoch:2/20... Training Step:301... Training loss:3.2763... 0.2140 sec/batch\n",
      "Epoch:2/20... Training Step:302... Training loss:3.2909... 0.2090 sec/batch\n",
      "Epoch:2/20... Training Step:303... Training loss:3.2862... 0.1943 sec/batch\n",
      "Epoch:2/20... Training Step:304... Training loss:3.2593... 0.1932 sec/batch\n",
      "Epoch:2/20... Training Step:305... Training loss:3.2272... 0.1933 sec/batch\n",
      "Epoch:2/20... Training Step:306... Training loss:3.2423... 0.2140 sec/batch\n",
      "Epoch:2/20... Training Step:307... Training loss:3.2434... 0.2060 sec/batch\n",
      "Epoch:2/20... Training Step:308... Training loss:3.2912... 0.1918 sec/batch\n",
      "Epoch:2/20... Training Step:309... Training loss:3.2655... 0.1961 sec/batch\n",
      "Epoch:2/20... Training Step:310... Training loss:3.2594... 0.2443 sec/batch\n",
      "Epoch:2/20... Training Step:311... Training loss:3.2465... 0.1916 sec/batch\n",
      "Epoch:2/20... Training Step:312... Training loss:3.2618... 0.1924 sec/batch\n",
      "Epoch:2/20... Training Step:313... Training loss:3.2444... 0.2018 sec/batch\n",
      "Epoch:2/20... Training Step:314... Training loss:3.2414... 0.2151 sec/batch\n",
      "Epoch:2/20... Training Step:315... Training loss:3.2116... 0.1917 sec/batch\n",
      "Epoch:2/20... Training Step:316... Training loss:3.2529... 0.2057 sec/batch\n",
      "Epoch:2/20... Training Step:317... Training loss:3.2408... 0.1915 sec/batch\n",
      "Epoch:2/20... Training Step:318... Training loss:3.2247... 0.1933 sec/batch\n",
      "Epoch:2/20... Training Step:319... Training loss:3.2503... 0.1915 sec/batch\n",
      "Epoch:2/20... Training Step:320... Training loss:3.2512... 0.1941 sec/batch\n",
      "Epoch:2/20... Training Step:321... Training loss:3.2236... 0.2146 sec/batch\n",
      "Epoch:2/20... Training Step:322... Training loss:3.2496... 0.2095 sec/batch\n",
      "Epoch:2/20... Training Step:323... Training loss:3.2320... 0.1978 sec/batch\n",
      "Epoch:2/20... Training Step:324... Training loss:3.2361... 0.2164 sec/batch\n",
      "Epoch:2/20... Training Step:325... Training loss:3.2373... 0.1913 sec/batch\n",
      "Epoch:2/20... Training Step:326... Training loss:3.2222... 0.2027 sec/batch\n",
      "Epoch:2/20... Training Step:327... Training loss:3.2232... 0.1922 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/20... Training Step:328... Training loss:3.2311... 0.1962 sec/batch\n",
      "Epoch:2/20... Training Step:329... Training loss:3.2072... 0.1904 sec/batch\n",
      "Epoch:2/20... Training Step:330... Training loss:3.1997... 0.2005 sec/batch\n",
      "Epoch:2/20... Training Step:331... Training loss:3.2281... 0.2103 sec/batch\n",
      "Epoch:2/20... Training Step:332... Training loss:3.2170... 0.2000 sec/batch\n",
      "Epoch:2/20... Training Step:333... Training loss:3.2370... 0.2055 sec/batch\n",
      "Epoch:2/20... Training Step:334... Training loss:3.2247... 0.2102 sec/batch\n",
      "Epoch:2/20... Training Step:335... Training loss:3.2230... 0.1919 sec/batch\n",
      "Epoch:2/20... Training Step:336... Training loss:3.2455... 0.1925 sec/batch\n",
      "Epoch:2/20... Training Step:337... Training loss:3.2342... 0.2053 sec/batch\n",
      "Epoch:2/20... Training Step:338... Training loss:3.2270... 0.1983 sec/batch\n",
      "Epoch:2/20... Training Step:339... Training loss:3.2028... 0.2064 sec/batch\n",
      "Epoch:2/20... Training Step:340... Training loss:3.2248... 0.1924 sec/batch\n",
      "Epoch:2/20... Training Step:341... Training loss:3.1999... 0.1929 sec/batch\n",
      "Epoch:2/20... Training Step:342... Training loss:3.2151... 0.1923 sec/batch\n",
      "Epoch:2/20... Training Step:343... Training loss:3.1895... 0.1936 sec/batch\n",
      "Epoch:2/20... Training Step:344... Training loss:3.1794... 0.2135 sec/batch\n",
      "Epoch:2/20... Training Step:345... Training loss:3.2065... 0.1906 sec/batch\n",
      "Epoch:2/20... Training Step:346... Training loss:3.2236... 0.2074 sec/batch\n",
      "Epoch:2/20... Training Step:347... Training loss:3.1793... 0.1966 sec/batch\n",
      "Epoch:2/20... Training Step:348... Training loss:3.2039... 0.2096 sec/batch\n",
      "Epoch:2/20... Training Step:349... Training loss:3.2379... 0.2082 sec/batch\n",
      "Epoch:2/20... Training Step:350... Training loss:3.2262... 0.1918 sec/batch\n",
      "Epoch:2/20... Training Step:351... Training loss:3.1916... 0.1946 sec/batch\n",
      "Epoch:2/20... Training Step:352... Training loss:3.2062... 0.1952 sec/batch\n",
      "Epoch:2/20... Training Step:353... Training loss:3.1899... 0.1960 sec/batch\n",
      "Epoch:2/20... Training Step:354... Training loss:3.1970... 0.2080 sec/batch\n",
      "Epoch:2/20... Training Step:355... Training loss:3.1813... 0.1955 sec/batch\n",
      "Epoch:2/20... Training Step:356... Training loss:3.1698... 0.1937 sec/batch\n",
      "Epoch:2/20... Training Step:357... Training loss:3.1707... 0.1914 sec/batch\n",
      "Epoch:2/20... Training Step:358... Training loss:3.1965... 0.1914 sec/batch\n",
      "Epoch:2/20... Training Step:359... Training loss:3.1984... 0.2148 sec/batch\n",
      "Epoch:2/20... Training Step:360... Training loss:3.1732... 0.2135 sec/batch\n",
      "Epoch:2/20... Training Step:361... Training loss:3.1648... 0.2085 sec/batch\n",
      "Epoch:2/20... Training Step:362... Training loss:3.1634... 0.2170 sec/batch\n",
      "Epoch:2/20... Training Step:363... Training loss:3.1872... 0.1962 sec/batch\n",
      "Epoch:2/20... Training Step:364... Training loss:3.1706... 0.2075 sec/batch\n",
      "Epoch:2/20... Training Step:365... Training loss:3.1576... 0.2057 sec/batch\n",
      "Epoch:2/20... Training Step:366... Training loss:3.1630... 0.1920 sec/batch\n",
      "Epoch:2/20... Training Step:367... Training loss:3.1678... 0.2069 sec/batch\n",
      "Epoch:2/20... Training Step:368... Training loss:3.1571... 0.2092 sec/batch\n",
      "Epoch:2/20... Training Step:369... Training loss:3.1820... 0.2076 sec/batch\n",
      "Epoch:2/20... Training Step:370... Training loss:3.1687... 0.1926 sec/batch\n",
      "Epoch:2/20... Training Step:371... Training loss:3.1768... 0.2054 sec/batch\n",
      "Epoch:2/20... Training Step:372... Training loss:3.1632... 0.1998 sec/batch\n",
      "Epoch:2/20... Training Step:373... Training loss:3.1744... 0.1921 sec/batch\n",
      "Epoch:2/20... Training Step:374... Training loss:3.2010... 0.1943 sec/batch\n",
      "Epoch:2/20... Training Step:375... Training loss:3.1761... 0.2066 sec/batch\n",
      "Epoch:2/20... Training Step:376... Training loss:3.1517... 0.1969 sec/batch\n",
      "Epoch:2/20... Training Step:377... Training loss:3.1840... 0.2075 sec/batch\n",
      "Epoch:2/20... Training Step:378... Training loss:3.1736... 0.1960 sec/batch\n",
      "Epoch:2/20... Training Step:379... Training loss:3.1721... 0.2028 sec/batch\n",
      "Epoch:2/20... Training Step:380... Training loss:3.1631... 0.2136 sec/batch\n",
      "Epoch:2/20... Training Step:381... Training loss:3.1710... 0.2163 sec/batch\n",
      "Epoch:2/20... Training Step:382... Training loss:3.1737... 0.2045 sec/batch\n",
      "Epoch:2/20... Training Step:383... Training loss:3.2008... 0.1903 sec/batch\n",
      "Epoch:2/20... Training Step:384... Training loss:3.1414... 0.2072 sec/batch\n",
      "Epoch:2/20... Training Step:385... Training loss:3.1695... 0.1934 sec/batch\n",
      "Epoch:2/20... Training Step:386... Training loss:3.1754... 0.1929 sec/batch\n",
      "Epoch:2/20... Training Step:387... Training loss:3.1769... 0.2010 sec/batch\n",
      "Epoch:2/20... Training Step:388... Training loss:3.1725... 0.2070 sec/batch\n",
      "Epoch:2/20... Training Step:389... Training loss:3.1411... 0.2052 sec/batch\n",
      "Epoch:2/20... Training Step:390... Training loss:3.1553... 0.1925 sec/batch\n",
      "Epoch:2/20... Training Step:391... Training loss:3.1326... 0.2099 sec/batch\n",
      "Epoch:2/20... Training Step:392... Training loss:3.1399... 0.1923 sec/batch\n",
      "Epoch:2/20... Training Step:393... Training loss:3.1562... 0.1952 sec/batch\n",
      "Epoch:2/20... Training Step:394... Training loss:3.1403... 0.1924 sec/batch\n",
      "Epoch:2/20... Training Step:395... Training loss:3.1989... 0.1924 sec/batch\n",
      "Epoch:2/20... Training Step:396... Training loss:3.1907... 0.2053 sec/batch\n",
      "Epoch:2/20... Training Step:397... Training loss:3.1675... 0.1929 sec/batch\n",
      "Epoch:2/20... Training Step:398... Training loss:3.1534... 0.1938 sec/batch\n",
      "Epoch:2/20... Training Step:399... Training loss:3.1530... 0.1991 sec/batch\n",
      "Epoch:2/20... Training Step:400... Training loss:3.1486... 0.1974 sec/batch\n",
      "Epoch:2/20... Training Step:401... Training loss:3.1329... 0.1937 sec/batch\n",
      "Epoch:2/20... Training Step:402... Training loss:3.1047... 0.2025 sec/batch\n",
      "Epoch:2/20... Training Step:403... Training loss:3.1122... 0.2034 sec/batch\n",
      "Epoch:2/20... Training Step:404... Training loss:3.1225... 0.2021 sec/batch\n",
      "Epoch:2/20... Training Step:405... Training loss:3.1071... 0.1956 sec/batch\n",
      "Epoch:2/20... Training Step:406... Training loss:3.1285... 0.2009 sec/batch\n",
      "Epoch:2/20... Training Step:407... Training loss:3.1421... 0.2052 sec/batch\n",
      "Epoch:2/20... Training Step:408... Training loss:3.1327... 0.2071 sec/batch\n",
      "Epoch:2/20... Training Step:409... Training loss:3.1431... 0.2035 sec/batch\n",
      "Epoch:2/20... Training Step:410... Training loss:3.1463... 0.2073 sec/batch\n",
      "Epoch:2/20... Training Step:411... Training loss:3.1380... 0.1927 sec/batch\n",
      "Epoch:2/20... Training Step:412... Training loss:3.1469... 0.2071 sec/batch\n",
      "Epoch:2/20... Training Step:413... Training loss:3.1337... 0.1923 sec/batch\n",
      "Epoch:2/20... Training Step:414... Training loss:3.1449... 0.1991 sec/batch\n",
      "Epoch:2/20... Training Step:415... Training loss:3.1138... 0.2043 sec/batch\n",
      "Epoch:2/20... Training Step:416... Training loss:3.1197... 0.1991 sec/batch\n",
      "Epoch:2/20... Training Step:417... Training loss:3.1547... 0.2017 sec/batch\n",
      "Epoch:2/20... Training Step:418... Training loss:3.1599... 0.2050 sec/batch\n",
      "Epoch:2/20... Training Step:419... Training loss:3.1527... 0.2039 sec/batch\n",
      "Epoch:2/20... Training Step:420... Training loss:3.1784... 0.1923 sec/batch\n",
      "Epoch:2/20... Training Step:421... Training loss:3.1525... 0.1933 sec/batch\n",
      "Epoch:2/20... Training Step:422... Training loss:3.1167... 0.1954 sec/batch\n",
      "Epoch:2/20... Training Step:423... Training loss:3.1004... 0.1937 sec/batch\n",
      "Epoch:2/20... Training Step:424... Training loss:3.1154... 0.2044 sec/batch\n",
      "Epoch:2/20... Training Step:425... Training loss:3.0943... 0.1945 sec/batch\n",
      "Epoch:2/20... Training Step:426... Training loss:3.1131... 0.1975 sec/batch\n",
      "Epoch:2/20... Training Step:427... Training loss:3.0874... 0.2016 sec/batch\n",
      "Epoch:2/20... Training Step:428... Training loss:3.1030... 0.1999 sec/batch\n",
      "Epoch:2/20... Training Step:429... Training loss:3.0968... 0.1909 sec/batch\n",
      "Epoch:2/20... Training Step:430... Training loss:3.0888... 0.1949 sec/batch\n",
      "Epoch:2/20... Training Step:431... Training loss:3.0717... 0.1919 sec/batch\n",
      "Epoch:2/20... Training Step:432... Training loss:3.1088... 0.2046 sec/batch\n",
      "Epoch:2/20... Training Step:433... Training loss:3.1018... 0.1925 sec/batch\n",
      "Epoch:2/20... Training Step:434... Training loss:3.0748... 0.1930 sec/batch\n",
      "Epoch:2/20... Training Step:435... Training loss:3.0912... 0.1932 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/20... Training Step:436... Training loss:3.0989... 0.2020 sec/batch\n",
      "Epoch:2/20... Training Step:437... Training loss:3.0555... 0.1932 sec/batch\n",
      "Epoch:2/20... Training Step:438... Training loss:3.0624... 0.1936 sec/batch\n",
      "Epoch:2/20... Training Step:439... Training loss:3.0783... 0.2021 sec/batch\n",
      "Epoch:2/20... Training Step:440... Training loss:3.1212... 0.2213 sec/batch\n",
      "Epoch:2/20... Training Step:441... Training loss:3.0744... 0.1999 sec/batch\n",
      "Epoch:2/20... Training Step:442... Training loss:3.0705... 0.1997 sec/batch\n",
      "Epoch:2/20... Training Step:443... Training loss:3.0738... 0.1921 sec/batch\n",
      "Epoch:2/20... Training Step:444... Training loss:3.0715... 0.1914 sec/batch\n",
      "Epoch:2/20... Training Step:445... Training loss:3.0686... 0.1920 sec/batch\n",
      "Epoch:2/20... Training Step:446... Training loss:3.0752... 0.2096 sec/batch\n",
      "Epoch:2/20... Training Step:447... Training loss:3.0926... 0.2053 sec/batch\n",
      "Epoch:2/20... Training Step:448... Training loss:3.0728... 0.1917 sec/batch\n",
      "Epoch:2/20... Training Step:449... Training loss:3.0638... 0.1910 sec/batch\n",
      "Epoch:2/20... Training Step:450... Training loss:3.0516... 0.1928 sec/batch\n",
      "Epoch:2/20... Training Step:451... Training loss:3.0545... 0.2025 sec/batch\n",
      "Epoch:2/20... Training Step:452... Training loss:3.0563... 0.1919 sec/batch\n",
      "Epoch:2/20... Training Step:453... Training loss:3.0492... 0.1947 sec/batch\n",
      "Epoch:2/20... Training Step:454... Training loss:3.0805... 0.1910 sec/batch\n",
      "Epoch:2/20... Training Step:455... Training loss:3.0762... 0.1941 sec/batch\n",
      "Epoch:2/20... Training Step:456... Training loss:3.1022... 0.2131 sec/batch\n",
      "Epoch:2/20... Training Step:457... Training loss:3.1035... 0.1918 sec/batch\n",
      "Epoch:2/20... Training Step:458... Training loss:3.0959... 0.2016 sec/batch\n",
      "Epoch:2/20... Training Step:459... Training loss:3.0599... 0.1991 sec/batch\n",
      "Epoch:2/20... Training Step:460... Training loss:3.0507... 0.1966 sec/batch\n",
      "Epoch:2/20... Training Step:461... Training loss:3.0604... 0.2069 sec/batch\n",
      "Epoch:2/20... Training Step:462... Training loss:3.0238... 0.2174 sec/batch\n",
      "Epoch:2/20... Training Step:463... Training loss:3.0314... 0.1905 sec/batch\n",
      "Epoch:2/20... Training Step:464... Training loss:3.0560... 0.1952 sec/batch\n",
      "Epoch:2/20... Training Step:465... Training loss:3.0459... 0.1922 sec/batch\n",
      "Epoch:2/20... Training Step:466... Training loss:3.0525... 0.1956 sec/batch\n",
      "Epoch:2/20... Training Step:467... Training loss:3.0416... 0.1930 sec/batch\n",
      "Epoch:2/20... Training Step:468... Training loss:3.0238... 0.2034 sec/batch\n",
      "Epoch:2/20... Training Step:469... Training loss:3.0150... 0.1907 sec/batch\n",
      "Epoch:2/20... Training Step:470... Training loss:3.0270... 0.2027 sec/batch\n",
      "Epoch:2/20... Training Step:471... Training loss:3.0343... 0.2009 sec/batch\n",
      "Epoch:2/20... Training Step:472... Training loss:3.0283... 0.1907 sec/batch\n",
      "Epoch:2/20... Training Step:473... Training loss:3.0289... 0.2154 sec/batch\n",
      "Epoch:2/20... Training Step:474... Training loss:3.0071... 0.1948 sec/batch\n",
      "Epoch:2/20... Training Step:475... Training loss:3.0321... 0.1999 sec/batch\n",
      "Epoch:2/20... Training Step:476... Training loss:3.0168... 0.1931 sec/batch\n",
      "Epoch:2/20... Training Step:477... Training loss:3.0397... 0.2068 sec/batch\n",
      "Epoch:2/20... Training Step:478... Training loss:3.0544... 0.2054 sec/batch\n",
      "Epoch:2/20... Training Step:479... Training loss:3.0214... 0.1998 sec/batch\n",
      "Epoch:2/20... Training Step:480... Training loss:3.0353... 0.2183 sec/batch\n",
      "Epoch:2/20... Training Step:481... Training loss:2.9972... 0.1933 sec/batch\n",
      "Epoch:2/20... Training Step:482... Training loss:3.0331... 0.2025 sec/batch\n",
      "Epoch:2/20... Training Step:483... Training loss:3.0463... 0.2006 sec/batch\n",
      "Epoch:2/20... Training Step:484... Training loss:3.0132... 0.2146 sec/batch\n",
      "Epoch:2/20... Training Step:485... Training loss:3.0354... 0.1923 sec/batch\n",
      "Epoch:2/20... Training Step:486... Training loss:3.0139... 0.2039 sec/batch\n",
      "Epoch:2/20... Training Step:487... Training loss:2.9932... 0.2112 sec/batch\n",
      "Epoch:2/20... Training Step:488... Training loss:3.0020... 0.1901 sec/batch\n",
      "Epoch:2/20... Training Step:489... Training loss:3.0137... 0.1975 sec/batch\n",
      "Epoch:2/20... Training Step:490... Training loss:3.0014... 0.1909 sec/batch\n",
      "Epoch:2/20... Training Step:491... Training loss:3.0183... 0.2151 sec/batch\n",
      "Epoch:2/20... Training Step:492... Training loss:2.9963... 0.1951 sec/batch\n",
      "Epoch:2/20... Training Step:493... Training loss:3.0169... 0.1942 sec/batch\n",
      "Epoch:2/20... Training Step:494... Training loss:2.9745... 0.1962 sec/batch\n",
      "Epoch:2/20... Training Step:495... Training loss:2.9721... 0.1966 sec/batch\n",
      "Epoch:2/20... Training Step:496... Training loss:3.0080... 0.2015 sec/batch\n",
      "Epoch:2/20... Training Step:497... Training loss:2.9757... 0.1931 sec/batch\n",
      "Epoch:2/20... Training Step:498... Training loss:2.9698... 0.2035 sec/batch\n",
      "Epoch:2/20... Training Step:499... Training loss:2.9519... 0.1923 sec/batch\n",
      "Epoch:2/20... Training Step:500... Training loss:2.9624... 0.1989 sec/batch\n",
      "Epoch:2/20... Training Step:501... Training loss:2.9772... 0.1940 sec/batch\n",
      "Epoch:2/20... Training Step:502... Training loss:2.9416... 0.1957 sec/batch\n",
      "Epoch:2/20... Training Step:503... Training loss:2.9697... 0.1918 sec/batch\n",
      "Epoch:2/20... Training Step:504... Training loss:3.0009... 0.1945 sec/batch\n",
      "Epoch:2/20... Training Step:505... Training loss:2.9844... 0.2119 sec/batch\n",
      "Epoch:2/20... Training Step:506... Training loss:2.9707... 0.2086 sec/batch\n",
      "Epoch:2/20... Training Step:507... Training loss:2.9735... 0.1906 sec/batch\n",
      "Epoch:2/20... Training Step:508... Training loss:2.9246... 0.2085 sec/batch\n",
      "Epoch:2/20... Training Step:509... Training loss:2.9459... 0.1921 sec/batch\n",
      "Epoch:2/20... Training Step:510... Training loss:2.9590... 0.2051 sec/batch\n",
      "Epoch:2/20... Training Step:511... Training loss:2.9690... 0.2173 sec/batch\n",
      "Epoch:2/20... Training Step:512... Training loss:2.9788... 0.2021 sec/batch\n",
      "Epoch:2/20... Training Step:513... Training loss:2.9522... 0.2075 sec/batch\n",
      "Epoch:2/20... Training Step:514... Training loss:2.9594... 0.2013 sec/batch\n",
      "Epoch:2/20... Training Step:515... Training loss:2.9776... 0.1984 sec/batch\n",
      "Epoch:2/20... Training Step:516... Training loss:2.9370... 0.2047 sec/batch\n",
      "Epoch:2/20... Training Step:517... Training loss:2.9621... 0.2135 sec/batch\n",
      "Epoch:2/20... Training Step:518... Training loss:2.9520... 0.1917 sec/batch\n",
      "Epoch:2/20... Training Step:519... Training loss:2.9394... 0.2076 sec/batch\n",
      "Epoch:2/20... Training Step:520... Training loss:2.9159... 0.1977 sec/batch\n",
      "Epoch:2/20... Training Step:521... Training loss:2.9174... 0.2042 sec/batch\n",
      "Epoch:2/20... Training Step:522... Training loss:2.9620... 0.1894 sec/batch\n",
      "Epoch:2/20... Training Step:523... Training loss:2.9382... 0.2045 sec/batch\n",
      "Epoch:2/20... Training Step:524... Training loss:2.9529... 0.1912 sec/batch\n",
      "Epoch:2/20... Training Step:525... Training loss:2.9415... 0.2054 sec/batch\n",
      "Epoch:2/20... Training Step:526... Training loss:2.9571... 0.2020 sec/batch\n",
      "Epoch:2/20... Training Step:527... Training loss:2.9649... 0.2091 sec/batch\n",
      "Epoch:2/20... Training Step:528... Training loss:2.9659... 0.2155 sec/batch\n",
      "Epoch:2/20... Training Step:529... Training loss:2.9308... 0.2044 sec/batch\n",
      "Epoch:2/20... Training Step:530... Training loss:2.9462... 0.1938 sec/batch\n",
      "Epoch:2/20... Training Step:531... Training loss:2.9656... 0.1997 sec/batch\n",
      "Epoch:2/20... Training Step:532... Training loss:3.0013... 0.2030 sec/batch\n",
      "Epoch:2/20... Training Step:533... Training loss:2.9826... 0.2011 sec/batch\n",
      "Epoch:2/20... Training Step:534... Training loss:2.9481... 0.2024 sec/batch\n",
      "Epoch:2/20... Training Step:535... Training loss:2.9713... 0.2122 sec/batch\n",
      "Epoch:2/20... Training Step:536... Training loss:2.9502... 0.2082 sec/batch\n",
      "Epoch:2/20... Training Step:537... Training loss:2.9430... 0.1918 sec/batch\n",
      "Epoch:2/20... Training Step:538... Training loss:2.9195... 0.1940 sec/batch\n",
      "Epoch:2/20... Training Step:539... Training loss:2.9107... 0.1918 sec/batch\n",
      "Epoch:2/20... Training Step:540... Training loss:2.9250... 0.2099 sec/batch\n",
      "Epoch:2/20... Training Step:541... Training loss:2.9313... 0.1916 sec/batch\n",
      "Epoch:2/20... Training Step:542... Training loss:2.9483... 0.1932 sec/batch\n",
      "Epoch:2/20... Training Step:543... Training loss:2.9232... 0.1936 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/20... Training Step:544... Training loss:2.9313... 0.1974 sec/batch\n",
      "Epoch:2/20... Training Step:545... Training loss:2.9442... 0.1911 sec/batch\n",
      "Epoch:2/20... Training Step:546... Training loss:2.9377... 0.2125 sec/batch\n",
      "Epoch:2/20... Training Step:547... Training loss:2.9533... 0.2036 sec/batch\n",
      "Epoch:2/20... Training Step:548... Training loss:2.9602... 0.2037 sec/batch\n",
      "Epoch:2/20... Training Step:549... Training loss:2.9511... 0.2034 sec/batch\n",
      "Epoch:2/20... Training Step:550... Training loss:2.9307... 0.1918 sec/batch\n",
      "Epoch:2/20... Training Step:551... Training loss:2.9361... 0.1937 sec/batch\n",
      "Epoch:2/20... Training Step:552... Training loss:2.8855... 0.1912 sec/batch\n",
      "Epoch:2/20... Training Step:553... Training loss:2.8753... 0.2175 sec/batch\n",
      "Epoch:2/20... Training Step:554... Training loss:2.8565... 0.1945 sec/batch\n",
      "Epoch:2/20... Training Step:555... Training loss:2.8639... 0.1931 sec/batch\n",
      "Epoch:2/20... Training Step:556... Training loss:2.8965... 0.2044 sec/batch\n",
      "Epoch:2/20... Training Step:557... Training loss:2.9232... 0.1903 sec/batch\n",
      "Epoch:2/20... Training Step:558... Training loss:2.9057... 0.1968 sec/batch\n",
      "Epoch:2/20... Training Step:559... Training loss:2.9095... 0.2016 sec/batch\n",
      "Epoch:2/20... Training Step:560... Training loss:2.9037... 0.2002 sec/batch\n",
      "Epoch:2/20... Training Step:561... Training loss:2.8789... 0.2015 sec/batch\n",
      "Epoch:2/20... Training Step:562... Training loss:2.8942... 0.1992 sec/batch\n",
      "Epoch:2/20... Training Step:563... Training loss:2.9236... 0.2089 sec/batch\n",
      "Epoch:2/20... Training Step:564... Training loss:2.8690... 0.1972 sec/batch\n",
      "Epoch:2/20... Training Step:565... Training loss:2.9026... 0.2003 sec/batch\n",
      "Epoch:2/20... Training Step:566... Training loss:2.9121... 0.2171 sec/batch\n",
      "Epoch:2/20... Training Step:567... Training loss:2.9107... 0.1929 sec/batch\n",
      "Epoch:2/20... Training Step:568... Training loss:2.9447... 0.1986 sec/batch\n",
      "Epoch:2/20... Training Step:569... Training loss:2.9114... 0.1975 sec/batch\n",
      "Epoch:2/20... Training Step:570... Training loss:2.9001... 0.1906 sec/batch\n",
      "Epoch:2/20... Training Step:571... Training loss:2.8896... 0.1929 sec/batch\n",
      "Epoch:2/20... Training Step:572... Training loss:2.9198... 0.1922 sec/batch\n",
      "Epoch:2/20... Training Step:573... Training loss:2.9125... 0.1961 sec/batch\n",
      "Epoch:2/20... Training Step:574... Training loss:2.9520... 0.1936 sec/batch\n",
      "Epoch:2/20... Training Step:575... Training loss:2.9229... 0.1911 sec/batch\n",
      "Epoch:2/20... Training Step:576... Training loss:2.9050... 0.1915 sec/batch\n",
      "Epoch:2/20... Training Step:577... Training loss:2.8984... 0.1929 sec/batch\n",
      "Epoch:2/20... Training Step:578... Training loss:2.9090... 0.1918 sec/batch\n",
      "Epoch:2/20... Training Step:579... Training loss:2.8844... 0.1935 sec/batch\n",
      "Epoch:2/20... Training Step:580... Training loss:2.8815... 0.2104 sec/batch\n",
      "Epoch:3/20... Training Step:581... Training loss:3.1944... 0.1919 sec/batch\n",
      "Epoch:3/20... Training Step:582... Training loss:2.9035... 0.1986 sec/batch\n",
      "Epoch:3/20... Training Step:583... Training loss:2.9118... 0.2110 sec/batch\n",
      "Epoch:3/20... Training Step:584... Training loss:2.9149... 0.1942 sec/batch\n",
      "Epoch:3/20... Training Step:585... Training loss:2.8954... 0.2036 sec/batch\n",
      "Epoch:3/20... Training Step:586... Training loss:2.8960... 0.2045 sec/batch\n",
      "Epoch:3/20... Training Step:587... Training loss:2.8822... 0.2041 sec/batch\n",
      "Epoch:3/20... Training Step:588... Training loss:2.8742... 0.1915 sec/batch\n",
      "Epoch:3/20... Training Step:589... Training loss:2.8793... 0.1929 sec/batch\n",
      "Epoch:3/20... Training Step:590... Training loss:2.8928... 0.1915 sec/batch\n",
      "Epoch:3/20... Training Step:591... Training loss:2.9095... 0.1998 sec/batch\n",
      "Epoch:3/20... Training Step:592... Training loss:2.9057... 0.2061 sec/batch\n",
      "Epoch:3/20... Training Step:593... Training loss:2.8925... 0.1985 sec/batch\n",
      "Epoch:3/20... Training Step:594... Training loss:2.8818... 0.1983 sec/batch\n",
      "Epoch:3/20... Training Step:595... Training loss:2.8479... 0.2033 sec/batch\n",
      "Epoch:3/20... Training Step:596... Training loss:2.8531... 0.1911 sec/batch\n",
      "Epoch:3/20... Training Step:597... Training loss:2.8685... 0.1936 sec/batch\n",
      "Epoch:3/20... Training Step:598... Training loss:2.8967... 0.1922 sec/batch\n",
      "Epoch:3/20... Training Step:599... Training loss:2.8782... 0.1920 sec/batch\n",
      "Epoch:3/20... Training Step:600... Training loss:2.8739... 0.1907 sec/batch\n",
      "Epoch:3/20... Training Step:601... Training loss:2.8606... 0.1925 sec/batch\n",
      "Epoch:3/20... Training Step:602... Training loss:2.8635... 0.1966 sec/batch\n",
      "Epoch:3/20... Training Step:603... Training loss:2.8476... 0.2003 sec/batch\n",
      "Epoch:3/20... Training Step:604... Training loss:2.8719... 0.1951 sec/batch\n",
      "Epoch:3/20... Training Step:605... Training loss:2.8413... 0.1966 sec/batch\n",
      "Epoch:3/20... Training Step:606... Training loss:2.8655... 0.1943 sec/batch\n",
      "Epoch:3/20... Training Step:607... Training loss:2.8559... 0.1926 sec/batch\n",
      "Epoch:3/20... Training Step:608... Training loss:2.8423... 0.1927 sec/batch\n",
      "Epoch:3/20... Training Step:609... Training loss:2.8699... 0.1919 sec/batch\n",
      "Epoch:3/20... Training Step:610... Training loss:2.8511... 0.1910 sec/batch\n",
      "Epoch:3/20... Training Step:611... Training loss:2.8294... 0.1966 sec/batch\n",
      "Epoch:3/20... Training Step:612... Training loss:2.8427... 0.1932 sec/batch\n",
      "Epoch:3/20... Training Step:613... Training loss:2.8189... 0.2079 sec/batch\n",
      "Epoch:3/20... Training Step:614... Training loss:2.8370... 0.2017 sec/batch\n",
      "Epoch:3/20... Training Step:615... Training loss:2.8565... 0.1995 sec/batch\n",
      "Epoch:3/20... Training Step:616... Training loss:2.8113... 0.2044 sec/batch\n",
      "Epoch:3/20... Training Step:617... Training loss:2.8293... 0.1926 sec/batch\n",
      "Epoch:3/20... Training Step:618... Training loss:2.8327... 0.1960 sec/batch\n",
      "Epoch:3/20... Training Step:619... Training loss:2.8165... 0.1955 sec/batch\n",
      "Epoch:3/20... Training Step:620... Training loss:2.7790... 0.1977 sec/batch\n",
      "Epoch:3/20... Training Step:621... Training loss:2.8280... 0.2066 sec/batch\n",
      "Epoch:3/20... Training Step:622... Training loss:2.8285... 0.1943 sec/batch\n",
      "Epoch:3/20... Training Step:623... Training loss:2.8457... 0.1924 sec/batch\n",
      "Epoch:3/20... Training Step:624... Training loss:2.8326... 0.2094 sec/batch\n",
      "Epoch:3/20... Training Step:625... Training loss:2.8352... 0.2128 sec/batch\n",
      "Epoch:3/20... Training Step:626... Training loss:2.8436... 0.2068 sec/batch\n",
      "Epoch:3/20... Training Step:627... Training loss:2.8380... 0.2079 sec/batch\n",
      "Epoch:3/20... Training Step:628... Training loss:2.8584... 0.1926 sec/batch\n",
      "Epoch:3/20... Training Step:629... Training loss:2.7994... 0.1915 sec/batch\n",
      "Epoch:3/20... Training Step:630... Training loss:2.8344... 0.1915 sec/batch\n",
      "Epoch:3/20... Training Step:631... Training loss:2.7924... 0.2145 sec/batch\n",
      "Epoch:3/20... Training Step:632... Training loss:2.8234... 0.1930 sec/batch\n",
      "Epoch:3/20... Training Step:633... Training loss:2.8044... 0.1917 sec/batch\n",
      "Epoch:3/20... Training Step:634... Training loss:2.7991... 0.1932 sec/batch\n",
      "Epoch:3/20... Training Step:635... Training loss:2.8058... 0.2062 sec/batch\n",
      "Epoch:3/20... Training Step:636... Training loss:2.8131... 0.1948 sec/batch\n",
      "Epoch:3/20... Training Step:637... Training loss:2.7739... 0.2003 sec/batch\n",
      "Epoch:3/20... Training Step:638... Training loss:2.7920... 0.1944 sec/batch\n",
      "Epoch:3/20... Training Step:639... Training loss:2.8344... 0.2050 sec/batch\n",
      "Epoch:3/20... Training Step:640... Training loss:2.8254... 0.1912 sec/batch\n",
      "Epoch:3/20... Training Step:641... Training loss:2.7859... 0.1929 sec/batch\n",
      "Epoch:3/20... Training Step:642... Training loss:2.8095... 0.1945 sec/batch\n",
      "Epoch:3/20... Training Step:643... Training loss:2.7846... 0.1908 sec/batch\n",
      "Epoch:3/20... Training Step:644... Training loss:2.8051... 0.1931 sec/batch\n",
      "Epoch:3/20... Training Step:645... Training loss:2.7722... 0.1910 sec/batch\n",
      "Epoch:3/20... Training Step:646... Training loss:2.7702... 0.2025 sec/batch\n",
      "Epoch:3/20... Training Step:647... Training loss:2.7491... 0.2010 sec/batch\n",
      "Epoch:3/20... Training Step:648... Training loss:2.7836... 0.2103 sec/batch\n",
      "Epoch:3/20... Training Step:649... Training loss:2.7877... 0.2035 sec/batch\n",
      "Epoch:3/20... Training Step:650... Training loss:2.7609... 0.2146 sec/batch\n",
      "Epoch:3/20... Training Step:651... Training loss:2.7388... 0.1939 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/20... Training Step:652... Training loss:2.7456... 0.2011 sec/batch\n",
      "Epoch:3/20... Training Step:653... Training loss:2.7829... 0.2049 sec/batch\n",
      "Epoch:3/20... Training Step:654... Training loss:2.7543... 0.2031 sec/batch\n",
      "Epoch:3/20... Training Step:655... Training loss:2.7315... 0.2080 sec/batch\n",
      "Epoch:3/20... Training Step:656... Training loss:2.7487... 0.2054 sec/batch\n",
      "Epoch:3/20... Training Step:657... Training loss:2.7242... 0.1921 sec/batch\n",
      "Epoch:3/20... Training Step:658... Training loss:2.7476... 0.1963 sec/batch\n",
      "Epoch:3/20... Training Step:659... Training loss:2.7661... 0.1924 sec/batch\n",
      "Epoch:3/20... Training Step:660... Training loss:2.7522... 0.1958 sec/batch\n",
      "Epoch:3/20... Training Step:661... Training loss:2.7389... 0.2060 sec/batch\n",
      "Epoch:3/20... Training Step:662... Training loss:2.7245... 0.2029 sec/batch\n",
      "Epoch:3/20... Training Step:663... Training loss:2.7560... 0.2109 sec/batch\n",
      "Epoch:3/20... Training Step:664... Training loss:2.7813... 0.1908 sec/batch\n",
      "Epoch:3/20... Training Step:665... Training loss:2.7717... 0.1973 sec/batch\n",
      "Epoch:3/20... Training Step:666... Training loss:2.7179... 0.1930 sec/batch\n",
      "Epoch:3/20... Training Step:667... Training loss:2.7608... 0.2004 sec/batch\n",
      "Epoch:3/20... Training Step:668... Training loss:2.7657... 0.1927 sec/batch\n",
      "Epoch:3/20... Training Step:669... Training loss:2.7698... 0.1919 sec/batch\n",
      "Epoch:3/20... Training Step:670... Training loss:2.7467... 0.1920 sec/batch\n",
      "Epoch:3/20... Training Step:671... Training loss:2.7461... 0.2000 sec/batch\n",
      "Epoch:3/20... Training Step:672... Training loss:2.7617... 0.1910 sec/batch\n",
      "Epoch:3/20... Training Step:673... Training loss:2.8019... 0.1937 sec/batch\n",
      "Epoch:3/20... Training Step:674... Training loss:2.7444... 0.1977 sec/batch\n",
      "Epoch:3/20... Training Step:675... Training loss:2.7314... 0.2000 sec/batch\n",
      "Epoch:3/20... Training Step:676... Training loss:2.7465... 0.2059 sec/batch\n",
      "Epoch:3/20... Training Step:677... Training loss:2.7393... 0.1974 sec/batch\n",
      "Epoch:3/20... Training Step:678... Training loss:2.7639... 0.1929 sec/batch\n",
      "Epoch:3/20... Training Step:679... Training loss:2.7153... 0.1943 sec/batch\n",
      "Epoch:3/20... Training Step:680... Training loss:2.7554... 0.1934 sec/batch\n",
      "Epoch:3/20... Training Step:681... Training loss:2.7347... 0.2060 sec/batch\n",
      "Epoch:3/20... Training Step:682... Training loss:2.7371... 0.1981 sec/batch\n",
      "Epoch:3/20... Training Step:683... Training loss:2.7477... 0.2042 sec/batch\n",
      "Epoch:3/20... Training Step:684... Training loss:2.7266... 0.2025 sec/batch\n",
      "Epoch:3/20... Training Step:685... Training loss:2.7917... 0.1998 sec/batch\n",
      "Epoch:3/20... Training Step:686... Training loss:2.7941... 0.2127 sec/batch\n",
      "Epoch:3/20... Training Step:687... Training loss:2.7777... 0.1928 sec/batch\n",
      "Epoch:3/20... Training Step:688... Training loss:2.7387... 0.1931 sec/batch\n",
      "Epoch:3/20... Training Step:689... Training loss:2.7374... 0.1924 sec/batch\n",
      "Epoch:3/20... Training Step:690... Training loss:2.7426... 0.2080 sec/batch\n",
      "Epoch:3/20... Training Step:691... Training loss:2.7362... 0.1934 sec/batch\n",
      "Epoch:3/20... Training Step:692... Training loss:2.7033... 0.1992 sec/batch\n",
      "Epoch:3/20... Training Step:693... Training loss:2.7035... 0.2387 sec/batch\n",
      "Epoch:3/20... Training Step:694... Training loss:2.7396... 0.1928 sec/batch\n",
      "Epoch:3/20... Training Step:695... Training loss:2.7193... 0.2002 sec/batch\n",
      "Epoch:3/20... Training Step:696... Training loss:2.7374... 0.1943 sec/batch\n",
      "Epoch:3/20... Training Step:697... Training loss:2.7472... 0.1956 sec/batch\n",
      "Epoch:3/20... Training Step:698... Training loss:2.7373... 0.2145 sec/batch\n",
      "Epoch:3/20... Training Step:699... Training loss:2.7437... 0.1913 sec/batch\n",
      "Epoch:3/20... Training Step:700... Training loss:2.7470... 0.1939 sec/batch\n",
      "Epoch:3/20... Training Step:701... Training loss:2.7370... 0.1920 sec/batch\n",
      "Epoch:3/20... Training Step:702... Training loss:2.7406... 0.1931 sec/batch\n",
      "Epoch:3/20... Training Step:703... Training loss:2.7333... 0.2027 sec/batch\n",
      "Epoch:3/20... Training Step:704... Training loss:2.7567... 0.2060 sec/batch\n",
      "Epoch:3/20... Training Step:705... Training loss:2.7308... 0.1999 sec/batch\n",
      "Epoch:3/20... Training Step:706... Training loss:2.7329... 0.2194 sec/batch\n",
      "Epoch:3/20... Training Step:707... Training loss:2.7596... 0.1936 sec/batch\n",
      "Epoch:3/20... Training Step:708... Training loss:2.7723... 0.1995 sec/batch\n",
      "Epoch:3/20... Training Step:709... Training loss:2.7675... 0.1935 sec/batch\n",
      "Epoch:3/20... Training Step:710... Training loss:2.8082... 0.1955 sec/batch\n",
      "Epoch:3/20... Training Step:711... Training loss:2.7731... 0.1909 sec/batch\n",
      "Epoch:3/20... Training Step:712... Training loss:2.7417... 0.1930 sec/batch\n",
      "Epoch:3/20... Training Step:713... Training loss:2.7314... 0.1960 sec/batch\n",
      "Epoch:3/20... Training Step:714... Training loss:2.7392... 0.2001 sec/batch\n",
      "Epoch:3/20... Training Step:715... Training loss:2.7010... 0.1916 sec/batch\n",
      "Epoch:3/20... Training Step:716... Training loss:2.7117... 0.1949 sec/batch\n",
      "Epoch:3/20... Training Step:717... Training loss:2.7008... 0.2015 sec/batch\n",
      "Epoch:3/20... Training Step:718... Training loss:2.7220... 0.2035 sec/batch\n",
      "Epoch:3/20... Training Step:719... Training loss:2.7130... 0.1916 sec/batch\n",
      "Epoch:3/20... Training Step:720... Training loss:2.6972... 0.1912 sec/batch\n",
      "Epoch:3/20... Training Step:721... Training loss:2.6929... 0.1913 sec/batch\n",
      "Epoch:3/20... Training Step:722... Training loss:2.7479... 0.1928 sec/batch\n",
      "Epoch:3/20... Training Step:723... Training loss:2.7225... 0.1909 sec/batch\n",
      "Epoch:3/20... Training Step:724... Training loss:2.7021... 0.1957 sec/batch\n",
      "Epoch:3/20... Training Step:725... Training loss:2.7086... 0.1911 sec/batch\n",
      "Epoch:3/20... Training Step:726... Training loss:2.7128... 0.2201 sec/batch\n",
      "Epoch:3/20... Training Step:727... Training loss:2.6828... 0.1915 sec/batch\n",
      "Epoch:3/20... Training Step:728... Training loss:2.6857... 0.1953 sec/batch\n",
      "Epoch:3/20... Training Step:729... Training loss:2.7063... 0.1914 sec/batch\n",
      "Epoch:3/20... Training Step:730... Training loss:2.7525... 0.2002 sec/batch\n",
      "Epoch:3/20... Training Step:731... Training loss:2.6933... 0.1927 sec/batch\n",
      "Epoch:3/20... Training Step:732... Training loss:2.6988... 0.2118 sec/batch\n",
      "Epoch:3/20... Training Step:733... Training loss:2.6994... 0.1980 sec/batch\n",
      "Epoch:3/20... Training Step:734... Training loss:2.6976... 0.1920 sec/batch\n",
      "Epoch:3/20... Training Step:735... Training loss:2.7029... 0.1915 sec/batch\n",
      "Epoch:3/20... Training Step:736... Training loss:2.7171... 0.1936 sec/batch\n",
      "Epoch:3/20... Training Step:737... Training loss:2.7174... 0.1958 sec/batch\n",
      "Epoch:3/20... Training Step:738... Training loss:2.6925... 0.1930 sec/batch\n",
      "Epoch:3/20... Training Step:739... Training loss:2.6757... 0.2008 sec/batch\n",
      "Epoch:3/20... Training Step:740... Training loss:2.6840... 0.1925 sec/batch\n",
      "Epoch:3/20... Training Step:741... Training loss:2.6845... 0.2009 sec/batch\n",
      "Epoch:3/20... Training Step:742... Training loss:2.6761... 0.1917 sec/batch\n",
      "Epoch:3/20... Training Step:743... Training loss:2.6837... 0.1961 sec/batch\n",
      "Epoch:3/20... Training Step:744... Training loss:2.7150... 0.1966 sec/batch\n",
      "Epoch:3/20... Training Step:745... Training loss:2.7286... 0.1978 sec/batch\n",
      "Epoch:3/20... Training Step:746... Training loss:2.7486... 0.1967 sec/batch\n",
      "Epoch:3/20... Training Step:747... Training loss:2.7518... 0.2115 sec/batch\n",
      "Epoch:3/20... Training Step:748... Training loss:2.7517... 0.1940 sec/batch\n",
      "Epoch:3/20... Training Step:749... Training loss:2.7005... 0.1961 sec/batch\n",
      "Epoch:3/20... Training Step:750... Training loss:2.6795... 0.2019 sec/batch\n",
      "Epoch:3/20... Training Step:751... Training loss:2.6878... 0.1991 sec/batch\n",
      "Epoch:3/20... Training Step:752... Training loss:2.6586... 0.1990 sec/batch\n",
      "Epoch:3/20... Training Step:753... Training loss:2.6618... 0.1922 sec/batch\n",
      "Epoch:3/20... Training Step:754... Training loss:2.6943... 0.2010 sec/batch\n",
      "Epoch:3/20... Training Step:755... Training loss:2.6935... 0.1942 sec/batch\n",
      "Epoch:3/20... Training Step:756... Training loss:2.6915... 0.1942 sec/batch\n",
      "Epoch:3/20... Training Step:757... Training loss:2.6812... 0.1908 sec/batch\n",
      "Epoch:3/20... Training Step:758... Training loss:2.6684... 0.2153 sec/batch\n",
      "Epoch:3/20... Training Step:759... Training loss:2.6747... 0.2058 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/20... Training Step:760... Training loss:2.6899... 0.1998 sec/batch\n",
      "Epoch:3/20... Training Step:761... Training loss:2.6714... 0.2083 sec/batch\n",
      "Epoch:3/20... Training Step:762... Training loss:2.6934... 0.2025 sec/batch\n",
      "Epoch:3/20... Training Step:763... Training loss:2.6705... 0.1917 sec/batch\n",
      "Epoch:3/20... Training Step:764... Training loss:2.6756... 0.1933 sec/batch\n",
      "Epoch:3/20... Training Step:765... Training loss:2.6936... 0.2035 sec/batch\n",
      "Epoch:3/20... Training Step:766... Training loss:2.6711... 0.2015 sec/batch\n",
      "Epoch:3/20... Training Step:767... Training loss:2.7038... 0.1991 sec/batch\n",
      "Epoch:3/20... Training Step:768... Training loss:2.7121... 0.2096 sec/batch\n",
      "Epoch:3/20... Training Step:769... Training loss:2.6797... 0.1925 sec/batch\n",
      "Epoch:3/20... Training Step:770... Training loss:2.6884... 0.2128 sec/batch\n",
      "Epoch:3/20... Training Step:771... Training loss:2.6559... 0.1998 sec/batch\n",
      "Epoch:3/20... Training Step:772... Training loss:2.7010... 0.1912 sec/batch\n",
      "Epoch:3/20... Training Step:773... Training loss:2.7097... 0.1942 sec/batch\n",
      "Epoch:3/20... Training Step:774... Training loss:2.6687... 0.1970 sec/batch\n",
      "Epoch:3/20... Training Step:775... Training loss:2.6967... 0.1964 sec/batch\n",
      "Epoch:3/20... Training Step:776... Training loss:2.6847... 0.1960 sec/batch\n",
      "Epoch:3/20... Training Step:777... Training loss:2.6531... 0.2047 sec/batch\n",
      "Epoch:3/20... Training Step:778... Training loss:2.6717... 0.2111 sec/batch\n",
      "Epoch:3/20... Training Step:779... Training loss:2.6865... 0.1915 sec/batch\n",
      "Epoch:3/20... Training Step:780... Training loss:2.6629... 0.2058 sec/batch\n",
      "Epoch:3/20... Training Step:781... Training loss:2.6925... 0.2015 sec/batch\n",
      "Epoch:3/20... Training Step:782... Training loss:2.6698... 0.1926 sec/batch\n",
      "Epoch:3/20... Training Step:783... Training loss:2.6937... 0.1950 sec/batch\n",
      "Epoch:3/20... Training Step:784... Training loss:2.6644... 0.1948 sec/batch\n",
      "Epoch:3/20... Training Step:785... Training loss:2.6371... 0.1936 sec/batch\n",
      "Epoch:3/20... Training Step:786... Training loss:2.6931... 0.1920 sec/batch\n",
      "Epoch:3/20... Training Step:787... Training loss:2.6649... 0.2033 sec/batch\n",
      "Epoch:3/20... Training Step:788... Training loss:2.6542... 0.2131 sec/batch\n",
      "Epoch:3/20... Training Step:789... Training loss:2.6388... 0.1922 sec/batch\n",
      "Epoch:3/20... Training Step:790... Training loss:2.6452... 0.1926 sec/batch\n",
      "Epoch:3/20... Training Step:791... Training loss:2.6508... 0.2072 sec/batch\n",
      "Epoch:3/20... Training Step:792... Training loss:2.6196... 0.1931 sec/batch\n",
      "Epoch:3/20... Training Step:793... Training loss:2.6572... 0.2013 sec/batch\n",
      "Epoch:3/20... Training Step:794... Training loss:2.6896... 0.1925 sec/batch\n",
      "Epoch:3/20... Training Step:795... Training loss:2.6424... 0.1946 sec/batch\n",
      "Epoch:3/20... Training Step:796... Training loss:2.6178... 0.1929 sec/batch\n",
      "Epoch:3/20... Training Step:797... Training loss:2.6329... 0.1947 sec/batch\n",
      "Epoch:3/20... Training Step:798... Training loss:2.6098... 0.2031 sec/batch\n",
      "Epoch:3/20... Training Step:799... Training loss:2.6253... 0.1914 sec/batch\n",
      "Epoch:3/20... Training Step:800... Training loss:2.6532... 0.1938 sec/batch\n",
      "Epoch:3/20... Training Step:801... Training loss:2.6667... 0.1957 sec/batch\n",
      "Epoch:3/20... Training Step:802... Training loss:2.6752... 0.1950 sec/batch\n",
      "Epoch:3/20... Training Step:803... Training loss:2.6405... 0.1974 sec/batch\n",
      "Epoch:3/20... Training Step:804... Training loss:2.6476... 0.1907 sec/batch\n",
      "Epoch:3/20... Training Step:805... Training loss:2.6714... 0.2067 sec/batch\n",
      "Epoch:3/20... Training Step:806... Training loss:2.6306... 0.1908 sec/batch\n",
      "Epoch:3/20... Training Step:807... Training loss:2.6604... 0.2203 sec/batch\n",
      "Epoch:3/20... Training Step:808... Training loss:2.6544... 0.1981 sec/batch\n",
      "Epoch:3/20... Training Step:809... Training loss:2.6584... 0.2002 sec/batch\n",
      "Epoch:3/20... Training Step:810... Training loss:2.6135... 0.1908 sec/batch\n",
      "Epoch:3/20... Training Step:811... Training loss:2.6115... 0.1974 sec/batch\n",
      "Epoch:3/20... Training Step:812... Training loss:2.6556... 0.1913 sec/batch\n",
      "Epoch:3/20... Training Step:813... Training loss:2.6323... 0.2055 sec/batch\n",
      "Epoch:3/20... Training Step:814... Training loss:2.6538... 0.2067 sec/batch\n",
      "Epoch:3/20... Training Step:815... Training loss:2.6496... 0.1925 sec/batch\n",
      "Epoch:3/20... Training Step:816... Training loss:2.6579... 0.2005 sec/batch\n",
      "Epoch:3/20... Training Step:817... Training loss:2.6515... 0.1938 sec/batch\n",
      "Epoch:3/20... Training Step:818... Training loss:2.6624... 0.1929 sec/batch\n",
      "Epoch:3/20... Training Step:819... Training loss:2.6297... 0.1995 sec/batch\n",
      "Epoch:3/20... Training Step:820... Training loss:2.6412... 0.1983 sec/batch\n",
      "Epoch:3/20... Training Step:821... Training loss:2.6404... 0.1955 sec/batch\n",
      "Epoch:3/20... Training Step:822... Training loss:2.6851... 0.1918 sec/batch\n",
      "Epoch:3/20... Training Step:823... Training loss:2.6856... 0.2021 sec/batch\n",
      "Epoch:3/20... Training Step:824... Training loss:2.6389... 0.2079 sec/batch\n",
      "Epoch:3/20... Training Step:825... Training loss:2.6618... 0.2027 sec/batch\n",
      "Epoch:3/20... Training Step:826... Training loss:2.6461... 0.1923 sec/batch\n",
      "Epoch:3/20... Training Step:827... Training loss:2.6457... 0.1952 sec/batch\n",
      "Epoch:3/20... Training Step:828... Training loss:2.6234... 0.2031 sec/batch\n",
      "Epoch:3/20... Training Step:829... Training loss:2.6149... 0.1967 sec/batch\n",
      "Epoch:3/20... Training Step:830... Training loss:2.6320... 0.2027 sec/batch\n",
      "Epoch:3/20... Training Step:831... Training loss:2.6497... 0.1968 sec/batch\n",
      "Epoch:3/20... Training Step:832... Training loss:2.6726... 0.2103 sec/batch\n",
      "Epoch:3/20... Training Step:833... Training loss:2.6177... 0.1923 sec/batch\n",
      "Epoch:3/20... Training Step:834... Training loss:2.6456... 0.1930 sec/batch\n",
      "Epoch:3/20... Training Step:835... Training loss:2.6651... 0.1944 sec/batch\n",
      "Epoch:3/20... Training Step:836... Training loss:2.6527... 0.2047 sec/batch\n",
      "Epoch:3/20... Training Step:837... Training loss:2.6283... 0.1915 sec/batch\n",
      "Epoch:3/20... Training Step:838... Training loss:2.6790... 0.1935 sec/batch\n",
      "Epoch:3/20... Training Step:839... Training loss:2.6556... 0.2026 sec/batch\n",
      "Epoch:3/20... Training Step:840... Training loss:2.6599... 0.1916 sec/batch\n",
      "Epoch:3/20... Training Step:841... Training loss:2.6584... 0.1952 sec/batch\n",
      "Epoch:3/20... Training Step:842... Training loss:2.6188... 0.1895 sec/batch\n",
      "Epoch:3/20... Training Step:843... Training loss:2.6061... 0.1982 sec/batch\n",
      "Epoch:3/20... Training Step:844... Training loss:2.5855... 0.1922 sec/batch\n",
      "Epoch:3/20... Training Step:845... Training loss:2.5905... 0.2087 sec/batch\n",
      "Epoch:3/20... Training Step:846... Training loss:2.6307... 0.2089 sec/batch\n",
      "Epoch:3/20... Training Step:847... Training loss:2.6551... 0.1920 sec/batch\n",
      "Epoch:3/20... Training Step:848... Training loss:2.6306... 0.1927 sec/batch\n",
      "Epoch:3/20... Training Step:849... Training loss:2.6268... 0.2050 sec/batch\n",
      "Epoch:3/20... Training Step:850... Training loss:2.6254... 0.1905 sec/batch\n",
      "Epoch:3/20... Training Step:851... Training loss:2.6071... 0.1921 sec/batch\n",
      "Epoch:3/20... Training Step:852... Training loss:2.6408... 0.2088 sec/batch\n",
      "Epoch:3/20... Training Step:853... Training loss:2.6649... 0.1911 sec/batch\n",
      "Epoch:3/20... Training Step:854... Training loss:2.6011... 0.1961 sec/batch\n",
      "Epoch:3/20... Training Step:855... Training loss:2.6453... 0.2060 sec/batch\n",
      "Epoch:3/20... Training Step:856... Training loss:2.6372... 0.1952 sec/batch\n",
      "Epoch:3/20... Training Step:857... Training loss:2.6455... 0.2036 sec/batch\n",
      "Epoch:3/20... Training Step:858... Training loss:2.6734... 0.2014 sec/batch\n",
      "Epoch:3/20... Training Step:859... Training loss:2.6381... 0.1927 sec/batch\n",
      "Epoch:3/20... Training Step:860... Training loss:2.6392... 0.2105 sec/batch\n",
      "Epoch:3/20... Training Step:861... Training loss:2.6276... 0.2012 sec/batch\n",
      "Epoch:3/20... Training Step:862... Training loss:2.6552... 0.1908 sec/batch\n",
      "Epoch:3/20... Training Step:863... Training loss:2.6494... 0.2113 sec/batch\n",
      "Epoch:3/20... Training Step:864... Training loss:2.6864... 0.1966 sec/batch\n",
      "Epoch:3/20... Training Step:865... Training loss:2.6644... 0.2002 sec/batch\n",
      "Epoch:3/20... Training Step:866... Training loss:2.6432... 0.2118 sec/batch\n",
      "Epoch:3/20... Training Step:867... Training loss:2.6264... 0.1956 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/20... Training Step:868... Training loss:2.6337... 0.1928 sec/batch\n",
      "Epoch:3/20... Training Step:869... Training loss:2.6237... 0.2057 sec/batch\n",
      "Epoch:3/20... Training Step:870... Training loss:2.6201... 0.1903 sec/batch\n",
      "Epoch:4/20... Training Step:871... Training loss:2.8760... 0.1950 sec/batch\n",
      "Epoch:4/20... Training Step:872... Training loss:2.6456... 0.1918 sec/batch\n",
      "Epoch:4/20... Training Step:873... Training loss:2.6497... 0.1993 sec/batch\n",
      "Epoch:4/20... Training Step:874... Training loss:2.6648... 0.1927 sec/batch\n",
      "Epoch:4/20... Training Step:875... Training loss:2.6359... 0.2039 sec/batch\n",
      "Epoch:4/20... Training Step:876... Training loss:2.6430... 0.1931 sec/batch\n",
      "Epoch:4/20... Training Step:877... Training loss:2.6156... 0.1991 sec/batch\n",
      "Epoch:4/20... Training Step:878... Training loss:2.6307... 0.2165 sec/batch\n",
      "Epoch:4/20... Training Step:879... Training loss:2.6377... 0.2095 sec/batch\n",
      "Epoch:4/20... Training Step:880... Training loss:2.6214... 0.2089 sec/batch\n",
      "Epoch:4/20... Training Step:881... Training loss:2.6534... 0.1915 sec/batch\n",
      "Epoch:4/20... Training Step:882... Training loss:2.6460... 0.2029 sec/batch\n",
      "Epoch:4/20... Training Step:883... Training loss:2.6578... 0.2093 sec/batch\n",
      "Epoch:4/20... Training Step:884... Training loss:2.6355... 0.1929 sec/batch\n",
      "Epoch:4/20... Training Step:885... Training loss:2.6079... 0.1930 sec/batch\n",
      "Epoch:4/20... Training Step:886... Training loss:2.6298... 0.1965 sec/batch\n",
      "Epoch:4/20... Training Step:887... Training loss:2.6266... 0.1921 sec/batch\n",
      "Epoch:4/20... Training Step:888... Training loss:2.6574... 0.1948 sec/batch\n",
      "Epoch:4/20... Training Step:889... Training loss:2.6334... 0.2062 sec/batch\n",
      "Epoch:4/20... Training Step:890... Training loss:2.6368... 0.1935 sec/batch\n",
      "Epoch:4/20... Training Step:891... Training loss:2.6322... 0.2063 sec/batch\n",
      "Epoch:4/20... Training Step:892... Training loss:2.6245... 0.1933 sec/batch\n",
      "Epoch:4/20... Training Step:893... Training loss:2.6268... 0.1967 sec/batch\n",
      "Epoch:4/20... Training Step:894... Training loss:2.6436... 0.1951 sec/batch\n",
      "Epoch:4/20... Training Step:895... Training loss:2.6068... 0.1925 sec/batch\n",
      "Epoch:4/20... Training Step:896... Training loss:2.6398... 0.1916 sec/batch\n",
      "Epoch:4/20... Training Step:897... Training loss:2.6241... 0.2191 sec/batch\n",
      "Epoch:4/20... Training Step:898... Training loss:2.6157... 0.1932 sec/batch\n",
      "Epoch:4/20... Training Step:899... Training loss:2.6477... 0.1951 sec/batch\n",
      "Epoch:4/20... Training Step:900... Training loss:2.6268... 0.2059 sec/batch\n",
      "Epoch:4/20... Training Step:901... Training loss:2.6124... 0.1937 sec/batch\n",
      "Epoch:4/20... Training Step:902... Training loss:2.6241... 0.1926 sec/batch\n",
      "Epoch:4/20... Training Step:903... Training loss:2.6019... 0.2029 sec/batch\n",
      "Epoch:4/20... Training Step:904... Training loss:2.6043... 0.1898 sec/batch\n",
      "Epoch:4/20... Training Step:905... Training loss:2.6327... 0.1968 sec/batch\n",
      "Epoch:4/20... Training Step:906... Training loss:2.5884... 0.1958 sec/batch\n",
      "Epoch:4/20... Training Step:907... Training loss:2.6056... 0.1980 sec/batch\n",
      "Epoch:4/20... Training Step:908... Training loss:2.6159... 0.2083 sec/batch\n",
      "Epoch:4/20... Training Step:909... Training loss:2.5865... 0.1927 sec/batch\n",
      "Epoch:4/20... Training Step:910... Training loss:2.5570... 0.1983 sec/batch\n",
      "Epoch:4/20... Training Step:911... Training loss:2.6161... 0.1914 sec/batch\n",
      "Epoch:4/20... Training Step:912... Training loss:2.6052... 0.2083 sec/batch\n",
      "Epoch:4/20... Training Step:913... Training loss:2.6270... 0.2034 sec/batch\n",
      "Epoch:4/20... Training Step:914... Training loss:2.6121... 0.2052 sec/batch\n",
      "Epoch:4/20... Training Step:915... Training loss:2.6238... 0.2130 sec/batch\n",
      "Epoch:4/20... Training Step:916... Training loss:2.6364... 0.1929 sec/batch\n",
      "Epoch:4/20... Training Step:917... Training loss:2.6355... 0.1932 sec/batch\n",
      "Epoch:4/20... Training Step:918... Training loss:2.6397... 0.2021 sec/batch\n",
      "Epoch:4/20... Training Step:919... Training loss:2.5870... 0.2002 sec/batch\n",
      "Epoch:4/20... Training Step:920... Training loss:2.6372... 0.1938 sec/batch\n",
      "Epoch:4/20... Training Step:921... Training loss:2.5817... 0.1935 sec/batch\n",
      "Epoch:4/20... Training Step:922... Training loss:2.6127... 0.2079 sec/batch\n",
      "Epoch:4/20... Training Step:923... Training loss:2.5958... 0.2022 sec/batch\n",
      "Epoch:4/20... Training Step:924... Training loss:2.5750... 0.1957 sec/batch\n",
      "Epoch:4/20... Training Step:925... Training loss:2.6022... 0.1928 sec/batch\n",
      "Epoch:4/20... Training Step:926... Training loss:2.5988... 0.1931 sec/batch\n",
      "Epoch:4/20... Training Step:927... Training loss:2.5737... 0.2103 sec/batch\n",
      "Epoch:4/20... Training Step:928... Training loss:2.5858... 0.2034 sec/batch\n",
      "Epoch:4/20... Training Step:929... Training loss:2.6237... 0.1920 sec/batch\n",
      "Epoch:4/20... Training Step:930... Training loss:2.6133... 0.1920 sec/batch\n",
      "Epoch:4/20... Training Step:931... Training loss:2.5851... 0.1918 sec/batch\n",
      "Epoch:4/20... Training Step:932... Training loss:2.6094... 0.1943 sec/batch\n",
      "Epoch:4/20... Training Step:933... Training loss:2.5755... 0.1931 sec/batch\n",
      "Epoch:4/20... Training Step:934... Training loss:2.6052... 0.2101 sec/batch\n",
      "Epoch:4/20... Training Step:935... Training loss:2.5786... 0.1904 sec/batch\n",
      "Epoch:4/20... Training Step:936... Training loss:2.5634... 0.1933 sec/batch\n",
      "Epoch:4/20... Training Step:937... Training loss:2.5566... 0.2079 sec/batch\n",
      "Epoch:4/20... Training Step:938... Training loss:2.5899... 0.1918 sec/batch\n",
      "Epoch:4/20... Training Step:939... Training loss:2.5941... 0.1935 sec/batch\n",
      "Epoch:4/20... Training Step:940... Training loss:2.5592... 0.2058 sec/batch\n",
      "Epoch:4/20... Training Step:941... Training loss:2.5550... 0.2005 sec/batch\n",
      "Epoch:4/20... Training Step:942... Training loss:2.5566... 0.1989 sec/batch\n",
      "Epoch:4/20... Training Step:943... Training loss:2.5857... 0.2069 sec/batch\n",
      "Epoch:4/20... Training Step:944... Training loss:2.5559... 0.1920 sec/batch\n",
      "Epoch:4/20... Training Step:945... Training loss:2.5220... 0.1922 sec/batch\n",
      "Epoch:4/20... Training Step:946... Training loss:2.5537... 0.2206 sec/batch\n",
      "Epoch:4/20... Training Step:947... Training loss:2.5328... 0.2052 sec/batch\n",
      "Epoch:4/20... Training Step:948... Training loss:2.5577... 0.2019 sec/batch\n",
      "Epoch:4/20... Training Step:949... Training loss:2.5813... 0.2036 sec/batch\n",
      "Epoch:4/20... Training Step:950... Training loss:2.5640... 0.1925 sec/batch\n",
      "Epoch:4/20... Training Step:951... Training loss:2.5475... 0.1959 sec/batch\n",
      "Epoch:4/20... Training Step:952... Training loss:2.5563... 0.1957 sec/batch\n",
      "Epoch:4/20... Training Step:953... Training loss:2.5816... 0.2140 sec/batch\n",
      "Epoch:4/20... Training Step:954... Training loss:2.6108... 0.2104 sec/batch\n",
      "Epoch:4/20... Training Step:955... Training loss:2.5905... 0.1918 sec/batch\n",
      "Epoch:4/20... Training Step:956... Training loss:2.5404... 0.1995 sec/batch\n",
      "Epoch:4/20... Training Step:957... Training loss:2.5693... 0.1987 sec/batch\n",
      "Epoch:4/20... Training Step:958... Training loss:2.5852... 0.1953 sec/batch\n",
      "Epoch:4/20... Training Step:959... Training loss:2.5805... 0.1899 sec/batch\n",
      "Epoch:4/20... Training Step:960... Training loss:2.5736... 0.1922 sec/batch\n",
      "Epoch:4/20... Training Step:961... Training loss:2.5736... 0.2088 sec/batch\n",
      "Epoch:4/20... Training Step:962... Training loss:2.5950... 0.2002 sec/batch\n",
      "Epoch:4/20... Training Step:963... Training loss:2.6269... 0.2074 sec/batch\n",
      "Epoch:4/20... Training Step:964... Training loss:2.5768... 0.1970 sec/batch\n",
      "Epoch:4/20... Training Step:965... Training loss:2.5452... 0.2107 sec/batch\n",
      "Epoch:4/20... Training Step:966... Training loss:2.5650... 0.2010 sec/batch\n",
      "Epoch:4/20... Training Step:967... Training loss:2.5610... 0.1940 sec/batch\n",
      "Epoch:4/20... Training Step:968... Training loss:2.5928... 0.2065 sec/batch\n",
      "Epoch:4/20... Training Step:969... Training loss:2.5547... 0.2014 sec/batch\n",
      "Epoch:4/20... Training Step:970... Training loss:2.5911... 0.1992 sec/batch\n",
      "Epoch:4/20... Training Step:971... Training loss:2.5767... 0.1931 sec/batch\n",
      "Epoch:4/20... Training Step:972... Training loss:2.5643... 0.2160 sec/batch\n",
      "Epoch:4/20... Training Step:973... Training loss:2.5848... 0.2151 sec/batch\n",
      "Epoch:4/20... Training Step:974... Training loss:2.5637... 0.2025 sec/batch\n",
      "Epoch:4/20... Training Step:975... Training loss:2.6242... 0.2058 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4/20... Training Step:976... Training loss:2.6111... 0.2235 sec/batch\n",
      "Epoch:4/20... Training Step:977... Training loss:2.6031... 0.2086 sec/batch\n",
      "Epoch:4/20... Training Step:978... Training loss:2.5789... 0.1979 sec/batch\n",
      "Epoch:4/20... Training Step:979... Training loss:2.5846... 0.1972 sec/batch\n",
      "Epoch:4/20... Training Step:980... Training loss:2.5805... 0.1920 sec/batch\n",
      "Epoch:4/20... Training Step:981... Training loss:2.5778... 0.2030 sec/batch\n",
      "Epoch:4/20... Training Step:982... Training loss:2.5434... 0.2006 sec/batch\n",
      "Epoch:4/20... Training Step:983... Training loss:2.5304... 0.2061 sec/batch\n",
      "Epoch:4/20... Training Step:984... Training loss:2.5785... 0.1941 sec/batch\n",
      "Epoch:4/20... Training Step:985... Training loss:2.5587... 0.1941 sec/batch\n",
      "Epoch:4/20... Training Step:986... Training loss:2.5876... 0.1934 sec/batch\n",
      "Epoch:4/20... Training Step:987... Training loss:2.5885... 0.1948 sec/batch\n",
      "Epoch:4/20... Training Step:988... Training loss:2.5809... 0.1975 sec/batch\n",
      "Epoch:4/20... Training Step:989... Training loss:2.5772... 0.2013 sec/batch\n",
      "Epoch:4/20... Training Step:990... Training loss:2.5897... 0.1981 sec/batch\n",
      "Epoch:4/20... Training Step:991... Training loss:2.5833... 0.2024 sec/batch\n",
      "Epoch:4/20... Training Step:992... Training loss:2.5926... 0.2081 sec/batch\n",
      "Epoch:4/20... Training Step:993... Training loss:2.5715... 0.2032 sec/batch\n",
      "Epoch:4/20... Training Step:994... Training loss:2.5934... 0.1984 sec/batch\n",
      "Epoch:4/20... Training Step:995... Training loss:2.5773... 0.1926 sec/batch\n",
      "Epoch:4/20... Training Step:996... Training loss:2.5875... 0.1982 sec/batch\n",
      "Epoch:4/20... Training Step:997... Training loss:2.5965... 0.2164 sec/batch\n",
      "Epoch:4/20... Training Step:998... Training loss:2.6185... 0.2027 sec/batch\n",
      "Epoch:4/20... Training Step:999... Training loss:2.6135... 0.2053 sec/batch\n",
      "Epoch:4/20... Training Step:1000... Training loss:2.6626... 0.1919 sec/batch\n",
      "Epoch:4/20... Training Step:1001... Training loss:2.6179... 0.1977 sec/batch\n",
      "Epoch:4/20... Training Step:1002... Training loss:2.5882... 0.2033 sec/batch\n",
      "Epoch:4/20... Training Step:1003... Training loss:2.5935... 0.2559 sec/batch\n",
      "Epoch:4/20... Training Step:1004... Training loss:2.5997... 0.1939 sec/batch\n",
      "Epoch:4/20... Training Step:1005... Training loss:2.5688... 0.2024 sec/batch\n",
      "Epoch:4/20... Training Step:1006... Training loss:2.5641... 0.1970 sec/batch\n",
      "Epoch:4/20... Training Step:1007... Training loss:2.5729... 0.2092 sec/batch\n",
      "Epoch:4/20... Training Step:1008... Training loss:2.5824... 0.2012 sec/batch\n",
      "Epoch:4/20... Training Step:1009... Training loss:2.5740... 0.2129 sec/batch\n",
      "Epoch:4/20... Training Step:1010... Training loss:2.5606... 0.1920 sec/batch\n",
      "Epoch:4/20... Training Step:1011... Training loss:2.5530... 0.1929 sec/batch\n",
      "Epoch:4/20... Training Step:1012... Training loss:2.6124... 0.2073 sec/batch\n",
      "Epoch:4/20... Training Step:1013... Training loss:2.5880... 0.1929 sec/batch\n",
      "Epoch:4/20... Training Step:1014... Training loss:2.5534... 0.2056 sec/batch\n",
      "Epoch:4/20... Training Step:1015... Training loss:2.5707... 0.1992 sec/batch\n",
      "Epoch:4/20... Training Step:1016... Training loss:2.5745... 0.2005 sec/batch\n",
      "Epoch:4/20... Training Step:1017... Training loss:2.5392... 0.1994 sec/batch\n",
      "Epoch:4/20... Training Step:1018... Training loss:2.5523... 0.1918 sec/batch\n",
      "Epoch:4/20... Training Step:1019... Training loss:2.5677... 0.2046 sec/batch\n",
      "Epoch:4/20... Training Step:1020... Training loss:2.6107... 0.1899 sec/batch\n",
      "Epoch:4/20... Training Step:1021... Training loss:2.5497... 0.2010 sec/batch\n",
      "Epoch:4/20... Training Step:1022... Training loss:2.5521... 0.2030 sec/batch\n",
      "Epoch:4/20... Training Step:1023... Training loss:2.5760... 0.1978 sec/batch\n",
      "Epoch:4/20... Training Step:1024... Training loss:2.5589... 0.2065 sec/batch\n",
      "Epoch:4/20... Training Step:1025... Training loss:2.5615... 0.1929 sec/batch\n",
      "Epoch:4/20... Training Step:1026... Training loss:2.5738... 0.1943 sec/batch\n",
      "Epoch:4/20... Training Step:1027... Training loss:2.5784... 0.2044 sec/batch\n",
      "Epoch:4/20... Training Step:1028... Training loss:2.5466... 0.1903 sec/batch\n",
      "Epoch:4/20... Training Step:1029... Training loss:2.5368... 0.1929 sec/batch\n",
      "Epoch:4/20... Training Step:1030... Training loss:2.5420... 0.1949 sec/batch\n",
      "Epoch:4/20... Training Step:1031... Training loss:2.5534... 0.2101 sec/batch\n",
      "Epoch:4/20... Training Step:1032... Training loss:2.5378... 0.1986 sec/batch\n",
      "Epoch:4/20... Training Step:1033... Training loss:2.5439... 0.2067 sec/batch\n",
      "Epoch:4/20... Training Step:1034... Training loss:2.5784... 0.2105 sec/batch\n",
      "Epoch:4/20... Training Step:1035... Training loss:2.5889... 0.1898 sec/batch\n",
      "Epoch:4/20... Training Step:1036... Training loss:2.6093... 0.1922 sec/batch\n",
      "Epoch:4/20... Training Step:1037... Training loss:2.6140... 0.1908 sec/batch\n",
      "Epoch:4/20... Training Step:1038... Training loss:2.6266... 0.1991 sec/batch\n",
      "Epoch:4/20... Training Step:1039... Training loss:2.5556... 0.1933 sec/batch\n",
      "Epoch:4/20... Training Step:1040... Training loss:2.5401... 0.1979 sec/batch\n",
      "Epoch:4/20... Training Step:1041... Training loss:2.5486... 0.1971 sec/batch\n",
      "Epoch:4/20... Training Step:1042... Training loss:2.5409... 0.1979 sec/batch\n",
      "Epoch:4/20... Training Step:1043... Training loss:2.5445... 0.1906 sec/batch\n",
      "Epoch:4/20... Training Step:1044... Training loss:2.5672... 0.2000 sec/batch\n",
      "Epoch:4/20... Training Step:1045... Training loss:2.5717... 0.1920 sec/batch\n",
      "Epoch:4/20... Training Step:1046... Training loss:2.5631... 0.1982 sec/batch\n",
      "Epoch:4/20... Training Step:1047... Training loss:2.5588... 0.2028 sec/batch\n",
      "Epoch:4/20... Training Step:1048... Training loss:2.5431... 0.1913 sec/batch\n",
      "Epoch:4/20... Training Step:1049... Training loss:2.5482... 0.2076 sec/batch\n",
      "Epoch:4/20... Training Step:1050... Training loss:2.5567... 0.1909 sec/batch\n",
      "Epoch:4/20... Training Step:1051... Training loss:2.5347... 0.1968 sec/batch\n",
      "Epoch:4/20... Training Step:1052... Training loss:2.5594... 0.1916 sec/batch\n",
      "Epoch:4/20... Training Step:1053... Training loss:2.5534... 0.2037 sec/batch\n",
      "Epoch:4/20... Training Step:1054... Training loss:2.5413... 0.2116 sec/batch\n",
      "Epoch:4/20... Training Step:1055... Training loss:2.5670... 0.2123 sec/batch\n",
      "Epoch:4/20... Training Step:1056... Training loss:2.5391... 0.1999 sec/batch\n",
      "Epoch:4/20... Training Step:1057... Training loss:2.5755... 0.2025 sec/batch\n",
      "Epoch:4/20... Training Step:1058... Training loss:2.5743... 0.1937 sec/batch\n",
      "Epoch:4/20... Training Step:1059... Training loss:2.5574... 0.2043 sec/batch\n",
      "Epoch:4/20... Training Step:1060... Training loss:2.5661... 0.1935 sec/batch\n",
      "Epoch:4/20... Training Step:1061... Training loss:2.5430... 0.2118 sec/batch\n",
      "Epoch:4/20... Training Step:1062... Training loss:2.5768... 0.2158 sec/batch\n",
      "Epoch:4/20... Training Step:1063... Training loss:2.5981... 0.1990 sec/batch\n",
      "Epoch:4/20... Training Step:1064... Training loss:2.5493... 0.1980 sec/batch\n",
      "Epoch:4/20... Training Step:1065... Training loss:2.5692... 0.1939 sec/batch\n",
      "Epoch:4/20... Training Step:1066... Training loss:2.5646... 0.1931 sec/batch\n",
      "Epoch:4/20... Training Step:1067... Training loss:2.5301... 0.2118 sec/batch\n",
      "Epoch:4/20... Training Step:1068... Training loss:2.5551... 0.1961 sec/batch\n",
      "Epoch:4/20... Training Step:1069... Training loss:2.5586... 0.2111 sec/batch\n",
      "Epoch:4/20... Training Step:1070... Training loss:2.5498... 0.1911 sec/batch\n",
      "Epoch:4/20... Training Step:1071... Training loss:2.5698... 0.1964 sec/batch\n",
      "Epoch:4/20... Training Step:1072... Training loss:2.5605... 0.1949 sec/batch\n",
      "Epoch:4/20... Training Step:1073... Training loss:2.5722... 0.1935 sec/batch\n",
      "Epoch:4/20... Training Step:1074... Training loss:2.5434... 0.1908 sec/batch\n",
      "Epoch:4/20... Training Step:1075... Training loss:2.5160... 0.1946 sec/batch\n",
      "Epoch:4/20... Training Step:1076... Training loss:2.5618... 0.1939 sec/batch\n",
      "Epoch:4/20... Training Step:1077... Training loss:2.5356... 0.1920 sec/batch\n",
      "Epoch:4/20... Training Step:1078... Training loss:2.5310... 0.1924 sec/batch\n",
      "Epoch:4/20... Training Step:1079... Training loss:2.5128... 0.1985 sec/batch\n",
      "Epoch:4/20... Training Step:1080... Training loss:2.5168... 0.1935 sec/batch\n",
      "Epoch:4/20... Training Step:1081... Training loss:2.5275... 0.2078 sec/batch\n",
      "Epoch:4/20... Training Step:1082... Training loss:2.4933... 0.1952 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4/20... Training Step:1083... Training loss:2.5326... 0.1984 sec/batch\n",
      "Epoch:4/20... Training Step:1084... Training loss:2.5737... 0.1904 sec/batch\n",
      "Epoch:4/20... Training Step:1085... Training loss:2.5145... 0.1973 sec/batch\n",
      "Epoch:4/20... Training Step:1086... Training loss:2.4988... 0.2148 sec/batch\n",
      "Epoch:4/20... Training Step:1087... Training loss:2.5250... 0.2014 sec/batch\n",
      "Epoch:4/20... Training Step:1088... Training loss:2.5012... 0.2070 sec/batch\n",
      "Epoch:4/20... Training Step:1089... Training loss:2.5137... 0.1983 sec/batch\n",
      "Epoch:4/20... Training Step:1090... Training loss:2.5443... 0.2063 sec/batch\n",
      "Epoch:4/20... Training Step:1091... Training loss:2.5493... 0.1960 sec/batch\n",
      "Epoch:4/20... Training Step:1092... Training loss:2.5654... 0.1932 sec/batch\n",
      "Epoch:4/20... Training Step:1093... Training loss:2.5405... 0.2032 sec/batch\n",
      "Epoch:4/20... Training Step:1094... Training loss:2.5238... 0.1921 sec/batch\n",
      "Epoch:4/20... Training Step:1095... Training loss:2.5500... 0.2122 sec/batch\n",
      "Epoch:4/20... Training Step:1096... Training loss:2.5177... 0.2009 sec/batch\n",
      "Epoch:4/20... Training Step:1097... Training loss:2.5337... 0.1987 sec/batch\n",
      "Epoch:4/20... Training Step:1098... Training loss:2.5421... 0.2036 sec/batch\n",
      "Epoch:4/20... Training Step:1099... Training loss:2.5378... 0.2063 sec/batch\n",
      "Epoch:4/20... Training Step:1100... Training loss:2.5038... 0.1926 sec/batch\n",
      "Epoch:4/20... Training Step:1101... Training loss:2.4980... 0.2055 sec/batch\n",
      "Epoch:4/20... Training Step:1102... Training loss:2.5446... 0.2005 sec/batch\n",
      "Epoch:4/20... Training Step:1103... Training loss:2.5213... 0.2009 sec/batch\n",
      "Epoch:4/20... Training Step:1104... Training loss:2.5336... 0.2064 sec/batch\n",
      "Epoch:4/20... Training Step:1105... Training loss:2.5370... 0.1921 sec/batch\n",
      "Epoch:4/20... Training Step:1106... Training loss:2.5513... 0.1952 sec/batch\n",
      "Epoch:4/20... Training Step:1107... Training loss:2.5375... 0.1954 sec/batch\n",
      "Epoch:4/20... Training Step:1108... Training loss:2.5475... 0.2082 sec/batch\n",
      "Epoch:4/20... Training Step:1109... Training loss:2.5113... 0.1954 sec/batch\n",
      "Epoch:4/20... Training Step:1110... Training loss:2.5279... 0.1963 sec/batch\n",
      "Epoch:4/20... Training Step:1111... Training loss:2.5316... 0.1943 sec/batch\n",
      "Epoch:4/20... Training Step:1112... Training loss:2.5724... 0.2042 sec/batch\n",
      "Epoch:4/20... Training Step:1113... Training loss:2.5795... 0.1938 sec/batch\n",
      "Epoch:4/20... Training Step:1114... Training loss:2.5345... 0.1932 sec/batch\n",
      "Epoch:4/20... Training Step:1115... Training loss:2.5603... 0.1919 sec/batch\n",
      "Epoch:4/20... Training Step:1116... Training loss:2.5435... 0.1905 sec/batch\n",
      "Epoch:4/20... Training Step:1117... Training loss:2.5190... 0.2116 sec/batch\n",
      "Epoch:4/20... Training Step:1118... Training loss:2.5286... 0.1938 sec/batch\n",
      "Epoch:4/20... Training Step:1119... Training loss:2.5091... 0.1920 sec/batch\n",
      "Epoch:4/20... Training Step:1120... Training loss:2.5229... 0.1919 sec/batch\n",
      "Epoch:4/20... Training Step:1121... Training loss:2.5274... 0.2006 sec/batch\n",
      "Epoch:4/20... Training Step:1122... Training loss:2.5649... 0.1918 sec/batch\n",
      "Epoch:4/20... Training Step:1123... Training loss:2.5086... 0.1987 sec/batch\n",
      "Epoch:4/20... Training Step:1124... Training loss:2.5246... 0.1929 sec/batch\n",
      "Epoch:4/20... Training Step:1125... Training loss:2.5575... 0.1935 sec/batch\n",
      "Epoch:4/20... Training Step:1126... Training loss:2.5370... 0.1920 sec/batch\n",
      "Epoch:4/20... Training Step:1127... Training loss:2.5185... 0.1930 sec/batch\n",
      "Epoch:4/20... Training Step:1128... Training loss:2.5704... 0.1927 sec/batch\n",
      "Epoch:4/20... Training Step:1129... Training loss:2.5458... 0.2039 sec/batch\n",
      "Epoch:4/20... Training Step:1130... Training loss:2.5500... 0.2081 sec/batch\n",
      "Epoch:4/20... Training Step:1131... Training loss:2.5636... 0.2049 sec/batch\n",
      "Epoch:4/20... Training Step:1132... Training loss:2.5210... 0.2091 sec/batch\n",
      "Epoch:4/20... Training Step:1133... Training loss:2.5011... 0.2103 sec/batch\n",
      "Epoch:4/20... Training Step:1134... Training loss:2.4855... 0.2019 sec/batch\n",
      "Epoch:4/20... Training Step:1135... Training loss:2.4948... 0.2012 sec/batch\n",
      "Epoch:4/20... Training Step:1136... Training loss:2.5245... 0.2060 sec/batch\n",
      "Epoch:4/20... Training Step:1137... Training loss:2.5616... 0.1989 sec/batch\n",
      "Epoch:4/20... Training Step:1138... Training loss:2.5363... 0.1970 sec/batch\n",
      "Epoch:4/20... Training Step:1139... Training loss:2.5159... 0.2010 sec/batch\n",
      "Epoch:4/20... Training Step:1140... Training loss:2.5132... 0.2071 sec/batch\n",
      "Epoch:4/20... Training Step:1141... Training loss:2.4978... 0.2043 sec/batch\n",
      "Epoch:4/20... Training Step:1142... Training loss:2.5403... 0.1924 sec/batch\n",
      "Epoch:4/20... Training Step:1143... Training loss:2.5567... 0.1921 sec/batch\n",
      "Epoch:4/20... Training Step:1144... Training loss:2.4937... 0.2129 sec/batch\n",
      "Epoch:4/20... Training Step:1145... Training loss:2.5434... 0.2036 sec/batch\n",
      "Epoch:4/20... Training Step:1146... Training loss:2.5360... 0.1912 sec/batch\n",
      "Epoch:4/20... Training Step:1147... Training loss:2.5451... 0.2005 sec/batch\n",
      "Epoch:4/20... Training Step:1148... Training loss:2.5552... 0.2013 sec/batch\n",
      "Epoch:4/20... Training Step:1149... Training loss:2.5260... 0.2028 sec/batch\n",
      "Epoch:4/20... Training Step:1150... Training loss:2.5344... 0.1922 sec/batch\n",
      "Epoch:4/20... Training Step:1151... Training loss:2.5328... 0.2094 sec/batch\n",
      "Epoch:4/20... Training Step:1152... Training loss:2.5451... 0.1926 sec/batch\n",
      "Epoch:4/20... Training Step:1153... Training loss:2.5463... 0.1931 sec/batch\n",
      "Epoch:4/20... Training Step:1154... Training loss:2.5819... 0.1941 sec/batch\n",
      "Epoch:4/20... Training Step:1155... Training loss:2.5687... 0.1941 sec/batch\n",
      "Epoch:4/20... Training Step:1156... Training loss:2.5519... 0.2154 sec/batch\n",
      "Epoch:4/20... Training Step:1157... Training loss:2.5258... 0.1937 sec/batch\n",
      "Epoch:4/20... Training Step:1158... Training loss:2.5285... 0.1958 sec/batch\n",
      "Epoch:4/20... Training Step:1159... Training loss:2.5199... 0.1913 sec/batch\n",
      "Epoch:4/20... Training Step:1160... Training loss:2.5144... 0.1936 sec/batch\n",
      "Epoch:5/20... Training Step:1161... Training loss:2.7412... 0.1992 sec/batch\n",
      "Epoch:5/20... Training Step:1162... Training loss:2.5384... 0.2014 sec/batch\n",
      "Epoch:5/20... Training Step:1163... Training loss:2.5351... 0.2052 sec/batch\n",
      "Epoch:5/20... Training Step:1164... Training loss:2.5585... 0.2015 sec/batch\n",
      "Epoch:5/20... Training Step:1165... Training loss:2.5301... 0.1919 sec/batch\n",
      "Epoch:5/20... Training Step:1166... Training loss:2.5347... 0.2084 sec/batch\n",
      "Epoch:5/20... Training Step:1167... Training loss:2.5207... 0.1927 sec/batch\n",
      "Epoch:5/20... Training Step:1168... Training loss:2.5296... 0.2111 sec/batch\n",
      "Epoch:5/20... Training Step:1169... Training loss:2.5301... 0.1928 sec/batch\n",
      "Epoch:5/20... Training Step:1170... Training loss:2.5216... 0.1952 sec/batch\n",
      "Epoch:5/20... Training Step:1171... Training loss:2.5540... 0.1905 sec/batch\n",
      "Epoch:5/20... Training Step:1172... Training loss:2.5505... 0.2123 sec/batch\n",
      "Epoch:5/20... Training Step:1173... Training loss:2.5501... 0.1960 sec/batch\n",
      "Epoch:5/20... Training Step:1174... Training loss:2.5448... 0.2034 sec/batch\n",
      "Epoch:5/20... Training Step:1175... Training loss:2.5195... 0.1954 sec/batch\n",
      "Epoch:5/20... Training Step:1176... Training loss:2.5321... 0.2117 sec/batch\n",
      "Epoch:5/20... Training Step:1177... Training loss:2.5331... 0.2160 sec/batch\n",
      "Epoch:5/20... Training Step:1178... Training loss:2.5570... 0.2010 sec/batch\n",
      "Epoch:5/20... Training Step:1179... Training loss:2.5430... 0.2027 sec/batch\n",
      "Epoch:5/20... Training Step:1180... Training loss:2.5438... 0.1976 sec/batch\n",
      "Epoch:5/20... Training Step:1181... Training loss:2.5427... 0.2141 sec/batch\n",
      "Epoch:5/20... Training Step:1182... Training loss:2.5329... 0.2091 sec/batch\n",
      "Epoch:5/20... Training Step:1183... Training loss:2.5299... 0.2050 sec/batch\n",
      "Epoch:5/20... Training Step:1184... Training loss:2.5611... 0.1940 sec/batch\n",
      "Epoch:5/20... Training Step:1185... Training loss:2.5092... 0.1942 sec/batch\n",
      "Epoch:5/20... Training Step:1186... Training loss:2.5375... 0.2096 sec/batch\n",
      "Epoch:5/20... Training Step:1187... Training loss:2.5425... 0.1927 sec/batch\n",
      "Epoch:5/20... Training Step:1188... Training loss:2.5232... 0.1911 sec/batch\n",
      "Epoch:5/20... Training Step:1189... Training loss:2.5588... 0.2119 sec/batch\n",
      "Epoch:5/20... Training Step:1190... Training loss:2.5421... 0.1914 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5/20... Training Step:1191... Training loss:2.5199... 0.2212 sec/batch\n",
      "Epoch:5/20... Training Step:1192... Training loss:2.5282... 0.2121 sec/batch\n",
      "Epoch:5/20... Training Step:1193... Training loss:2.5101... 0.2072 sec/batch\n",
      "Epoch:5/20... Training Step:1194... Training loss:2.5205... 0.2029 sec/batch\n",
      "Epoch:5/20... Training Step:1195... Training loss:2.5291... 0.1969 sec/batch\n",
      "Epoch:5/20... Training Step:1196... Training loss:2.4942... 0.1951 sec/batch\n",
      "Epoch:5/20... Training Step:1197... Training loss:2.5131... 0.1916 sec/batch\n",
      "Epoch:5/20... Training Step:1198... Training loss:2.5328... 0.1939 sec/batch\n",
      "Epoch:5/20... Training Step:1199... Training loss:2.5078... 0.1991 sec/batch\n",
      "Epoch:5/20... Training Step:1200... Training loss:2.4665... 0.2035 sec/batch\n",
      "Epoch:5/20... Training Step:1201... Training loss:2.5015... 0.1932 sec/batch\n",
      "Epoch:5/20... Training Step:1202... Training loss:2.5186... 0.1924 sec/batch\n",
      "Epoch:5/20... Training Step:1203... Training loss:2.5430... 0.2040 sec/batch\n",
      "Epoch:5/20... Training Step:1204... Training loss:2.5200... 0.1965 sec/batch\n",
      "Epoch:5/20... Training Step:1205... Training loss:2.5441... 0.1958 sec/batch\n",
      "Epoch:5/20... Training Step:1206... Training loss:2.5407... 0.1980 sec/batch\n",
      "Epoch:5/20... Training Step:1207... Training loss:2.5480... 0.2097 sec/batch\n",
      "Epoch:5/20... Training Step:1208... Training loss:2.5355... 0.2083 sec/batch\n",
      "Epoch:5/20... Training Step:1209... Training loss:2.4892... 0.1919 sec/batch\n",
      "Epoch:5/20... Training Step:1210... Training loss:2.5460... 0.1930 sec/batch\n",
      "Epoch:5/20... Training Step:1211... Training loss:2.5001... 0.1996 sec/batch\n",
      "Epoch:5/20... Training Step:1212... Training loss:2.5184... 0.1968 sec/batch\n",
      "Epoch:5/20... Training Step:1213... Training loss:2.5120... 0.2000 sec/batch\n",
      "Epoch:5/20... Training Step:1214... Training loss:2.4822... 0.2026 sec/batch\n",
      "Epoch:5/20... Training Step:1215... Training loss:2.5030... 0.2025 sec/batch\n",
      "Epoch:5/20... Training Step:1216... Training loss:2.5128... 0.1914 sec/batch\n",
      "Epoch:5/20... Training Step:1217... Training loss:2.4929... 0.1962 sec/batch\n",
      "Epoch:5/20... Training Step:1218... Training loss:2.4971... 0.2019 sec/batch\n",
      "Epoch:5/20... Training Step:1219... Training loss:2.5294... 0.1972 sec/batch\n",
      "Epoch:5/20... Training Step:1220... Training loss:2.5247... 0.2003 sec/batch\n",
      "Epoch:5/20... Training Step:1221... Training loss:2.5051... 0.1911 sec/batch\n",
      "Epoch:5/20... Training Step:1222... Training loss:2.5184... 0.1973 sec/batch\n",
      "Epoch:5/20... Training Step:1223... Training loss:2.4902... 0.2060 sec/batch\n",
      "Epoch:5/20... Training Step:1224... Training loss:2.5229... 0.1923 sec/batch\n",
      "Epoch:5/20... Training Step:1225... Training loss:2.5108... 0.1948 sec/batch\n",
      "Epoch:5/20... Training Step:1226... Training loss:2.4644... 0.2072 sec/batch\n",
      "Epoch:5/20... Training Step:1227... Training loss:2.4734... 0.2061 sec/batch\n",
      "Epoch:5/20... Training Step:1228... Training loss:2.4957... 0.1947 sec/batch\n",
      "Epoch:5/20... Training Step:1229... Training loss:2.5156... 0.1932 sec/batch\n",
      "Epoch:5/20... Training Step:1230... Training loss:2.4653... 0.2047 sec/batch\n",
      "Epoch:5/20... Training Step:1231... Training loss:2.4695... 0.1990 sec/batch\n",
      "Epoch:5/20... Training Step:1232... Training loss:2.4900... 0.2059 sec/batch\n",
      "Epoch:5/20... Training Step:1233... Training loss:2.5036... 0.1920 sec/batch\n",
      "Epoch:5/20... Training Step:1234... Training loss:2.4676... 0.1965 sec/batch\n",
      "Epoch:5/20... Training Step:1235... Training loss:2.4331... 0.1988 sec/batch\n",
      "Epoch:5/20... Training Step:1236... Training loss:2.4687... 0.2061 sec/batch\n",
      "Epoch:5/20... Training Step:1237... Training loss:2.4546... 0.1991 sec/batch\n",
      "Epoch:5/20... Training Step:1238... Training loss:2.4683... 0.2072 sec/batch\n",
      "Epoch:5/20... Training Step:1239... Training loss:2.4916... 0.2016 sec/batch\n",
      "Epoch:5/20... Training Step:1240... Training loss:2.4753... 0.1924 sec/batch\n",
      "Epoch:5/20... Training Step:1241... Training loss:2.4679... 0.2043 sec/batch\n",
      "Epoch:5/20... Training Step:1242... Training loss:2.4728... 0.2129 sec/batch\n",
      "Epoch:5/20... Training Step:1243... Training loss:2.4921... 0.1908 sec/batch\n",
      "Epoch:5/20... Training Step:1244... Training loss:2.5283... 0.1977 sec/batch\n",
      "Epoch:5/20... Training Step:1245... Training loss:2.5041... 0.1959 sec/batch\n",
      "Epoch:5/20... Training Step:1246... Training loss:2.4533... 0.1927 sec/batch\n",
      "Epoch:5/20... Training Step:1247... Training loss:2.4914... 0.2032 sec/batch\n",
      "Epoch:5/20... Training Step:1248... Training loss:2.4999... 0.1912 sec/batch\n",
      "Epoch:5/20... Training Step:1249... Training loss:2.4964... 0.1936 sec/batch\n",
      "Epoch:5/20... Training Step:1250... Training loss:2.4931... 0.1918 sec/batch\n",
      "Epoch:5/20... Training Step:1251... Training loss:2.4888... 0.1942 sec/batch\n",
      "Epoch:5/20... Training Step:1252... Training loss:2.5068... 0.2012 sec/batch\n",
      "Epoch:5/20... Training Step:1253... Training loss:2.5357... 0.1909 sec/batch\n",
      "Epoch:5/20... Training Step:1254... Training loss:2.4958... 0.1943 sec/batch\n",
      "Epoch:5/20... Training Step:1255... Training loss:2.4607... 0.1914 sec/batch\n",
      "Epoch:5/20... Training Step:1256... Training loss:2.4853... 0.1936 sec/batch\n",
      "Epoch:5/20... Training Step:1257... Training loss:2.4849... 0.2045 sec/batch\n",
      "Epoch:5/20... Training Step:1258... Training loss:2.5049... 0.2043 sec/batch\n",
      "Epoch:5/20... Training Step:1259... Training loss:2.4705... 0.2015 sec/batch\n",
      "Epoch:5/20... Training Step:1260... Training loss:2.5050... 0.2028 sec/batch\n",
      "Epoch:5/20... Training Step:1261... Training loss:2.4900... 0.1918 sec/batch\n",
      "Epoch:5/20... Training Step:1262... Training loss:2.4766... 0.2056 sec/batch\n",
      "Epoch:5/20... Training Step:1263... Training loss:2.5059... 0.1960 sec/batch\n",
      "Epoch:5/20... Training Step:1264... Training loss:2.4786... 0.1972 sec/batch\n",
      "Epoch:5/20... Training Step:1265... Training loss:2.5455... 0.2036 sec/batch\n",
      "Epoch:5/20... Training Step:1266... Training loss:2.5306... 0.1958 sec/batch\n",
      "Epoch:5/20... Training Step:1267... Training loss:2.5190... 0.1986 sec/batch\n",
      "Epoch:5/20... Training Step:1268... Training loss:2.4974... 0.2062 sec/batch\n",
      "Epoch:5/20... Training Step:1269... Training loss:2.5132... 0.1905 sec/batch\n",
      "Epoch:5/20... Training Step:1270... Training loss:2.5061... 0.2010 sec/batch\n",
      "Epoch:5/20... Training Step:1271... Training loss:2.4957... 0.2121 sec/batch\n",
      "Epoch:5/20... Training Step:1272... Training loss:2.4692... 0.1927 sec/batch\n",
      "Epoch:5/20... Training Step:1273... Training loss:2.4527... 0.1939 sec/batch\n",
      "Epoch:5/20... Training Step:1274... Training loss:2.4932... 0.2023 sec/batch\n",
      "Epoch:5/20... Training Step:1275... Training loss:2.4939... 0.1922 sec/batch\n",
      "Epoch:5/20... Training Step:1276... Training loss:2.5139... 0.1903 sec/batch\n",
      "Epoch:5/20... Training Step:1277... Training loss:2.5146... 0.1967 sec/batch\n",
      "Epoch:5/20... Training Step:1278... Training loss:2.4965... 0.2057 sec/batch\n",
      "Epoch:5/20... Training Step:1279... Training loss:2.4964... 0.1911 sec/batch\n",
      "Epoch:5/20... Training Step:1280... Training loss:2.5097... 0.1929 sec/batch\n",
      "Epoch:5/20... Training Step:1281... Training loss:2.5059... 0.1964 sec/batch\n",
      "Epoch:5/20... Training Step:1282... Training loss:2.5174... 0.1988 sec/batch\n",
      "Epoch:5/20... Training Step:1283... Training loss:2.4934... 0.1909 sec/batch\n",
      "Epoch:5/20... Training Step:1284... Training loss:2.5173... 0.1954 sec/batch\n",
      "Epoch:5/20... Training Step:1285... Training loss:2.4954... 0.1911 sec/batch\n",
      "Epoch:5/20... Training Step:1286... Training loss:2.5057... 0.1959 sec/batch\n",
      "Epoch:5/20... Training Step:1287... Training loss:2.5241... 0.2058 sec/batch\n",
      "Epoch:5/20... Training Step:1288... Training loss:2.5430... 0.2108 sec/batch\n",
      "Epoch:5/20... Training Step:1289... Training loss:2.5366... 0.1978 sec/batch\n",
      "Epoch:5/20... Training Step:1290... Training loss:2.5717... 0.1934 sec/batch\n",
      "Epoch:5/20... Training Step:1291... Training loss:2.5251... 0.2045 sec/batch\n",
      "Epoch:5/20... Training Step:1292... Training loss:2.5064... 0.1962 sec/batch\n",
      "Epoch:5/20... Training Step:1293... Training loss:2.5130... 0.2064 sec/batch\n",
      "Epoch:5/20... Training Step:1294... Training loss:2.5298... 0.1953 sec/batch\n",
      "Epoch:5/20... Training Step:1295... Training loss:2.4960... 0.2033 sec/batch\n",
      "Epoch:5/20... Training Step:1296... Training loss:2.4918... 0.1919 sec/batch\n",
      "Epoch:5/20... Training Step:1297... Training loss:2.4887... 0.2234 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5/20... Training Step:1298... Training loss:2.4977... 0.2038 sec/batch\n",
      "Epoch:5/20... Training Step:1299... Training loss:2.4917... 0.2509 sec/batch\n",
      "Epoch:5/20... Training Step:1300... Training loss:2.4810... 0.2044 sec/batch\n",
      "Epoch:5/20... Training Step:1301... Training loss:2.4766... 0.1934 sec/batch\n",
      "Epoch:5/20... Training Step:1302... Training loss:2.5425... 0.2033 sec/batch\n",
      "Epoch:5/20... Training Step:1303... Training loss:2.5107... 0.1947 sec/batch\n",
      "Epoch:5/20... Training Step:1304... Training loss:2.4781... 0.2114 sec/batch\n",
      "Epoch:5/20... Training Step:1305... Training loss:2.4893... 0.2049 sec/batch\n",
      "Epoch:5/20... Training Step:1306... Training loss:2.5105... 0.1913 sec/batch\n",
      "Epoch:5/20... Training Step:1307... Training loss:2.4624... 0.2020 sec/batch\n",
      "Epoch:5/20... Training Step:1308... Training loss:2.4754... 0.1912 sec/batch\n",
      "Epoch:5/20... Training Step:1309... Training loss:2.5026... 0.1944 sec/batch\n",
      "Epoch:5/20... Training Step:1310... Training loss:2.5251... 0.1913 sec/batch\n",
      "Epoch:5/20... Training Step:1311... Training loss:2.4715... 0.1927 sec/batch\n",
      "Epoch:5/20... Training Step:1312... Training loss:2.4877... 0.2028 sec/batch\n",
      "Epoch:5/20... Training Step:1313... Training loss:2.4993... 0.2057 sec/batch\n",
      "Epoch:5/20... Training Step:1314... Training loss:2.4836... 0.1909 sec/batch\n",
      "Epoch:5/20... Training Step:1315... Training loss:2.4925... 0.1950 sec/batch\n",
      "Epoch:5/20... Training Step:1316... Training loss:2.5077... 0.2018 sec/batch\n",
      "Epoch:5/20... Training Step:1317... Training loss:2.4970... 0.1925 sec/batch\n",
      "Epoch:5/20... Training Step:1318... Training loss:2.4758... 0.1930 sec/batch\n",
      "Epoch:5/20... Training Step:1319... Training loss:2.4617... 0.2122 sec/batch\n",
      "Epoch:5/20... Training Step:1320... Training loss:2.4721... 0.1951 sec/batch\n",
      "Epoch:5/20... Training Step:1321... Training loss:2.4739... 0.1992 sec/batch\n",
      "Epoch:5/20... Training Step:1322... Training loss:2.4748... 0.1920 sec/batch\n",
      "Epoch:5/20... Training Step:1323... Training loss:2.4684... 0.1938 sec/batch\n",
      "Epoch:5/20... Training Step:1324... Training loss:2.5060... 0.2134 sec/batch\n",
      "Epoch:5/20... Training Step:1325... Training loss:2.5086... 0.1938 sec/batch\n",
      "Epoch:5/20... Training Step:1326... Training loss:2.5418... 0.1960 sec/batch\n",
      "Epoch:5/20... Training Step:1327... Training loss:2.5382... 0.1921 sec/batch\n",
      "Epoch:5/20... Training Step:1328... Training loss:2.5420... 0.1922 sec/batch\n",
      "Epoch:5/20... Training Step:1329... Training loss:2.4870... 0.1933 sec/batch\n",
      "Epoch:5/20... Training Step:1330... Training loss:2.4699... 0.2183 sec/batch\n",
      "Epoch:5/20... Training Step:1331... Training loss:2.4844... 0.2016 sec/batch\n",
      "Epoch:5/20... Training Step:1332... Training loss:2.4713... 0.1956 sec/batch\n",
      "Epoch:5/20... Training Step:1333... Training loss:2.4605... 0.1953 sec/batch\n",
      "Epoch:5/20... Training Step:1334... Training loss:2.4882... 0.1907 sec/batch\n",
      "Epoch:5/20... Training Step:1335... Training loss:2.4952... 0.1922 sec/batch\n",
      "Epoch:5/20... Training Step:1336... Training loss:2.4856... 0.1976 sec/batch\n",
      "Epoch:5/20... Training Step:1337... Training loss:2.4828... 0.2126 sec/batch\n",
      "Epoch:5/20... Training Step:1338... Training loss:2.4589... 0.1955 sec/batch\n",
      "Epoch:5/20... Training Step:1339... Training loss:2.4638... 0.1940 sec/batch\n",
      "Epoch:5/20... Training Step:1340... Training loss:2.4795... 0.2074 sec/batch\n",
      "Epoch:5/20... Training Step:1341... Training loss:2.4733... 0.1918 sec/batch\n",
      "Epoch:5/20... Training Step:1342... Training loss:2.4974... 0.2073 sec/batch\n",
      "Epoch:5/20... Training Step:1343... Training loss:2.4849... 0.2024 sec/batch\n",
      "Epoch:5/20... Training Step:1344... Training loss:2.4684... 0.1903 sec/batch\n",
      "Epoch:5/20... Training Step:1345... Training loss:2.4945... 0.2032 sec/batch\n",
      "Epoch:5/20... Training Step:1346... Training loss:2.4632... 0.2030 sec/batch\n",
      "Epoch:5/20... Training Step:1347... Training loss:2.5042... 0.2026 sec/batch\n",
      "Epoch:5/20... Training Step:1348... Training loss:2.5022... 0.2032 sec/batch\n",
      "Epoch:5/20... Training Step:1349... Training loss:2.4808... 0.1924 sec/batch\n",
      "Epoch:5/20... Training Step:1350... Training loss:2.4913... 0.1910 sec/batch\n",
      "Epoch:5/20... Training Step:1351... Training loss:2.4673... 0.1922 sec/batch\n",
      "Epoch:5/20... Training Step:1352... Training loss:2.5002... 0.1953 sec/batch\n",
      "Epoch:5/20... Training Step:1353... Training loss:2.5227... 0.1968 sec/batch\n",
      "Epoch:5/20... Training Step:1354... Training loss:2.4750... 0.1941 sec/batch\n",
      "Epoch:5/20... Training Step:1355... Training loss:2.4988... 0.1913 sec/batch\n",
      "Epoch:5/20... Training Step:1356... Training loss:2.4948... 0.1935 sec/batch\n",
      "Epoch:5/20... Training Step:1357... Training loss:2.4557... 0.1945 sec/batch\n",
      "Epoch:5/20... Training Step:1358... Training loss:2.4961... 0.1998 sec/batch\n",
      "Epoch:5/20... Training Step:1359... Training loss:2.4846... 0.1968 sec/batch\n",
      "Epoch:5/20... Training Step:1360... Training loss:2.4784... 0.1948 sec/batch\n",
      "Epoch:5/20... Training Step:1361... Training loss:2.4888... 0.1962 sec/batch\n",
      "Epoch:5/20... Training Step:1362... Training loss:2.4803... 0.2018 sec/batch\n",
      "Epoch:5/20... Training Step:1363... Training loss:2.4962... 0.1998 sec/batch\n",
      "Epoch:5/20... Training Step:1364... Training loss:2.4716... 0.1975 sec/batch\n",
      "Epoch:5/20... Training Step:1365... Training loss:2.4424... 0.2044 sec/batch\n",
      "Epoch:5/20... Training Step:1366... Training loss:2.4888... 0.2058 sec/batch\n",
      "Epoch:5/20... Training Step:1367... Training loss:2.4683... 0.2146 sec/batch\n",
      "Epoch:5/20... Training Step:1368... Training loss:2.4568... 0.1935 sec/batch\n",
      "Epoch:5/20... Training Step:1369... Training loss:2.4448... 0.1971 sec/batch\n",
      "Epoch:5/20... Training Step:1370... Training loss:2.4494... 0.2023 sec/batch\n",
      "Epoch:5/20... Training Step:1371... Training loss:2.4488... 0.1994 sec/batch\n",
      "Epoch:5/20... Training Step:1372... Training loss:2.4320... 0.2019 sec/batch\n",
      "Epoch:5/20... Training Step:1373... Training loss:2.4580... 0.1951 sec/batch\n",
      "Epoch:5/20... Training Step:1374... Training loss:2.4972... 0.2055 sec/batch\n",
      "Epoch:5/20... Training Step:1375... Training loss:2.4438... 0.2000 sec/batch\n",
      "Epoch:5/20... Training Step:1376... Training loss:2.4257... 0.1929 sec/batch\n",
      "Epoch:5/20... Training Step:1377... Training loss:2.4505... 0.2174 sec/batch\n",
      "Epoch:5/20... Training Step:1378... Training loss:2.4280... 0.1920 sec/batch\n",
      "Epoch:5/20... Training Step:1379... Training loss:2.4565... 0.1980 sec/batch\n",
      "Epoch:5/20... Training Step:1380... Training loss:2.4738... 0.1968 sec/batch\n",
      "Epoch:5/20... Training Step:1381... Training loss:2.4828... 0.1999 sec/batch\n",
      "Epoch:5/20... Training Step:1382... Training loss:2.4945... 0.2141 sec/batch\n",
      "Epoch:5/20... Training Step:1383... Training loss:2.4636... 0.2113 sec/batch\n",
      "Epoch:5/20... Training Step:1384... Training loss:2.4426... 0.1913 sec/batch\n",
      "Epoch:5/20... Training Step:1385... Training loss:2.4765... 0.1919 sec/batch\n",
      "Epoch:5/20... Training Step:1386... Training loss:2.4486... 0.2012 sec/batch\n",
      "Epoch:5/20... Training Step:1387... Training loss:2.4693... 0.1951 sec/batch\n",
      "Epoch:5/20... Training Step:1388... Training loss:2.4653... 0.2021 sec/batch\n",
      "Epoch:5/20... Training Step:1389... Training loss:2.4722... 0.1919 sec/batch\n",
      "Epoch:5/20... Training Step:1390... Training loss:2.4390... 0.1992 sec/batch\n",
      "Epoch:5/20... Training Step:1391... Training loss:2.4315... 0.2074 sec/batch\n",
      "Epoch:5/20... Training Step:1392... Training loss:2.4790... 0.1921 sec/batch\n",
      "Epoch:5/20... Training Step:1393... Training loss:2.4602... 0.1935 sec/batch\n",
      "Epoch:5/20... Training Step:1394... Training loss:2.4601... 0.1927 sec/batch\n",
      "Epoch:5/20... Training Step:1395... Training loss:2.4646... 0.1977 sec/batch\n",
      "Epoch:5/20... Training Step:1396... Training loss:2.4727... 0.2026 sec/batch\n",
      "Epoch:5/20... Training Step:1397... Training loss:2.4605... 0.2079 sec/batch\n",
      "Epoch:5/20... Training Step:1398... Training loss:2.4755... 0.1925 sec/batch\n",
      "Epoch:5/20... Training Step:1399... Training loss:2.4348... 0.2020 sec/batch\n",
      "Epoch:5/20... Training Step:1400... Training loss:2.4599... 0.1917 sec/batch\n",
      "Epoch:5/20... Training Step:1401... Training loss:2.4519... 0.1933 sec/batch\n",
      "Epoch:5/20... Training Step:1402... Training loss:2.5060... 0.1897 sec/batch\n",
      "Epoch:5/20... Training Step:1403... Training loss:2.5083... 0.1988 sec/batch\n",
      "Epoch:5/20... Training Step:1404... Training loss:2.4627... 0.1918 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5/20... Training Step:1405... Training loss:2.4792... 0.1959 sec/batch\n",
      "Epoch:5/20... Training Step:1406... Training loss:2.4806... 0.1937 sec/batch\n",
      "Epoch:5/20... Training Step:1407... Training loss:2.4524... 0.1922 sec/batch\n",
      "Epoch:5/20... Training Step:1408... Training loss:2.4561... 0.1933 sec/batch\n",
      "Epoch:5/20... Training Step:1409... Training loss:2.4341... 0.1953 sec/batch\n",
      "Epoch:5/20... Training Step:1410... Training loss:2.4669... 0.1922 sec/batch\n",
      "Epoch:5/20... Training Step:1411... Training loss:2.4738... 0.1925 sec/batch\n",
      "Epoch:5/20... Training Step:1412... Training loss:2.4942... 0.1955 sec/batch\n",
      "Epoch:5/20... Training Step:1413... Training loss:2.4425... 0.1961 sec/batch\n",
      "Epoch:5/20... Training Step:1414... Training loss:2.4694... 0.2105 sec/batch\n",
      "Epoch:5/20... Training Step:1415... Training loss:2.4959... 0.2039 sec/batch\n",
      "Epoch:5/20... Training Step:1416... Training loss:2.4726... 0.1989 sec/batch\n",
      "Epoch:5/20... Training Step:1417... Training loss:2.4461... 0.2005 sec/batch\n",
      "Epoch:5/20... Training Step:1418... Training loss:2.4942... 0.2190 sec/batch\n",
      "Epoch:5/20... Training Step:1419... Training loss:2.4809... 0.2024 sec/batch\n",
      "Epoch:5/20... Training Step:1420... Training loss:2.4823... 0.1927 sec/batch\n",
      "Epoch:5/20... Training Step:1421... Training loss:2.4938... 0.1988 sec/batch\n",
      "Epoch:5/20... Training Step:1422... Training loss:2.4515... 0.1914 sec/batch\n",
      "Epoch:5/20... Training Step:1423... Training loss:2.4353... 0.1921 sec/batch\n",
      "Epoch:5/20... Training Step:1424... Training loss:2.4096... 0.1946 sec/batch\n",
      "Epoch:5/20... Training Step:1425... Training loss:2.4277... 0.2052 sec/batch\n",
      "Epoch:5/20... Training Step:1426... Training loss:2.4443... 0.1944 sec/batch\n",
      "Epoch:5/20... Training Step:1427... Training loss:2.4777... 0.1918 sec/batch\n",
      "Epoch:5/20... Training Step:1428... Training loss:2.4555... 0.1950 sec/batch\n",
      "Epoch:5/20... Training Step:1429... Training loss:2.4479... 0.2305 sec/batch\n",
      "Epoch:5/20... Training Step:1430... Training loss:2.4480... 0.2188 sec/batch\n",
      "Epoch:5/20... Training Step:1431... Training loss:2.4307... 0.2129 sec/batch\n",
      "Epoch:5/20... Training Step:1432... Training loss:2.4711... 0.2023 sec/batch\n",
      "Epoch:5/20... Training Step:1433... Training loss:2.4869... 0.2023 sec/batch\n",
      "Epoch:5/20... Training Step:1434... Training loss:2.4353... 0.2106 sec/batch\n",
      "Epoch:5/20... Training Step:1435... Training loss:2.4684... 0.1990 sec/batch\n",
      "Epoch:5/20... Training Step:1436... Training loss:2.4632... 0.2081 sec/batch\n",
      "Epoch:5/20... Training Step:1437... Training loss:2.4733... 0.2020 sec/batch\n",
      "Epoch:5/20... Training Step:1438... Training loss:2.4839... 0.1949 sec/batch\n",
      "Epoch:5/20... Training Step:1439... Training loss:2.4627... 0.1935 sec/batch\n",
      "Epoch:5/20... Training Step:1440... Training loss:2.4687... 0.1998 sec/batch\n",
      "Epoch:5/20... Training Step:1441... Training loss:2.4678... 0.1931 sec/batch\n",
      "Epoch:5/20... Training Step:1442... Training loss:2.4808... 0.2037 sec/batch\n",
      "Epoch:5/20... Training Step:1443... Training loss:2.4817... 0.1977 sec/batch\n",
      "Epoch:5/20... Training Step:1444... Training loss:2.5070... 0.1966 sec/batch\n",
      "Epoch:5/20... Training Step:1445... Training loss:2.5008... 0.2026 sec/batch\n",
      "Epoch:5/20... Training Step:1446... Training loss:2.4857... 0.2016 sec/batch\n",
      "Epoch:5/20... Training Step:1447... Training loss:2.4533... 0.1977 sec/batch\n",
      "Epoch:5/20... Training Step:1448... Training loss:2.4608... 0.1961 sec/batch\n",
      "Epoch:5/20... Training Step:1449... Training loss:2.4571... 0.2012 sec/batch\n",
      "Epoch:5/20... Training Step:1450... Training loss:2.4409... 0.2154 sec/batch\n",
      "Epoch:6/20... Training Step:1451... Training loss:2.6458... 0.1984 sec/batch\n",
      "Epoch:6/20... Training Step:1452... Training loss:2.4699... 0.2127 sec/batch\n",
      "Epoch:6/20... Training Step:1453... Training loss:2.4691... 0.2038 sec/batch\n",
      "Epoch:6/20... Training Step:1454... Training loss:2.4800... 0.2148 sec/batch\n",
      "Epoch:6/20... Training Step:1455... Training loss:2.4674... 0.1938 sec/batch\n",
      "Epoch:6/20... Training Step:1456... Training loss:2.4676... 0.1937 sec/batch\n",
      "Epoch:6/20... Training Step:1457... Training loss:2.4546... 0.2129 sec/batch\n",
      "Epoch:6/20... Training Step:1458... Training loss:2.4613... 0.1933 sec/batch\n",
      "Epoch:6/20... Training Step:1459... Training loss:2.4468... 0.1928 sec/batch\n",
      "Epoch:6/20... Training Step:1460... Training loss:2.4454... 0.1924 sec/batch\n",
      "Epoch:6/20... Training Step:1461... Training loss:2.4975... 0.1934 sec/batch\n",
      "Epoch:6/20... Training Step:1462... Training loss:2.4737... 0.1943 sec/batch\n",
      "Epoch:6/20... Training Step:1463... Training loss:2.4792... 0.1944 sec/batch\n",
      "Epoch:6/20... Training Step:1464... Training loss:2.4705... 0.2180 sec/batch\n",
      "Epoch:6/20... Training Step:1465... Training loss:2.4488... 0.2000 sec/batch\n",
      "Epoch:6/20... Training Step:1466... Training loss:2.4695... 0.2103 sec/batch\n",
      "Epoch:6/20... Training Step:1467... Training loss:2.4735... 0.1993 sec/batch\n",
      "Epoch:6/20... Training Step:1468... Training loss:2.4921... 0.2002 sec/batch\n",
      "Epoch:6/20... Training Step:1469... Training loss:2.4761... 0.1926 sec/batch\n",
      "Epoch:6/20... Training Step:1470... Training loss:2.4852... 0.1924 sec/batch\n",
      "Epoch:6/20... Training Step:1471... Training loss:2.4752... 0.2080 sec/batch\n",
      "Epoch:6/20... Training Step:1472... Training loss:2.4555... 0.2044 sec/batch\n",
      "Epoch:6/20... Training Step:1473... Training loss:2.4579... 0.1994 sec/batch\n",
      "Epoch:6/20... Training Step:1474... Training loss:2.5020... 0.1937 sec/batch\n",
      "Epoch:6/20... Training Step:1475... Training loss:2.4370... 0.1969 sec/batch\n",
      "Epoch:6/20... Training Step:1476... Training loss:2.4717... 0.2102 sec/batch\n",
      "Epoch:6/20... Training Step:1477... Training loss:2.4732... 0.1974 sec/batch\n",
      "Epoch:6/20... Training Step:1478... Training loss:2.4702... 0.1956 sec/batch\n",
      "Epoch:6/20... Training Step:1479... Training loss:2.4936... 0.1945 sec/batch\n",
      "Epoch:6/20... Training Step:1480... Training loss:2.4653... 0.1946 sec/batch\n",
      "Epoch:6/20... Training Step:1481... Training loss:2.4655... 0.1907 sec/batch\n",
      "Epoch:6/20... Training Step:1482... Training loss:2.4562... 0.2076 sec/batch\n",
      "Epoch:6/20... Training Step:1483... Training loss:2.4475... 0.2281 sec/batch\n",
      "Epoch:6/20... Training Step:1484... Training loss:2.4654... 0.1998 sec/batch\n",
      "Epoch:6/20... Training Step:1485... Training loss:2.4602... 0.1913 sec/batch\n",
      "Epoch:6/20... Training Step:1486... Training loss:2.4417... 0.2110 sec/batch\n",
      "Epoch:6/20... Training Step:1487... Training loss:2.4481... 0.2115 sec/batch\n",
      "Epoch:6/20... Training Step:1488... Training loss:2.4692... 0.2023 sec/batch\n",
      "Epoch:6/20... Training Step:1489... Training loss:2.4366... 0.2016 sec/batch\n",
      "Epoch:6/20... Training Step:1490... Training loss:2.4001... 0.1912 sec/batch\n",
      "Epoch:6/20... Training Step:1491... Training loss:2.4392... 0.2003 sec/batch\n",
      "Epoch:6/20... Training Step:1492... Training loss:2.4593... 0.1919 sec/batch\n",
      "Epoch:6/20... Training Step:1493... Training loss:2.4726... 0.2064 sec/batch\n",
      "Epoch:6/20... Training Step:1494... Training loss:2.4580... 0.2128 sec/batch\n",
      "Epoch:6/20... Training Step:1495... Training loss:2.4828... 0.2028 sec/batch\n",
      "Epoch:6/20... Training Step:1496... Training loss:2.4829... 0.2105 sec/batch\n",
      "Epoch:6/20... Training Step:1497... Training loss:2.4851... 0.1918 sec/batch\n",
      "Epoch:6/20... Training Step:1498... Training loss:2.4627... 0.2056 sec/batch\n",
      "Epoch:6/20... Training Step:1499... Training loss:2.4353... 0.1992 sec/batch\n",
      "Epoch:6/20... Training Step:1500... Training loss:2.4870... 0.1996 sec/batch\n",
      "Epoch:6/20... Training Step:1501... Training loss:2.4409... 0.1972 sec/batch\n",
      "Epoch:6/20... Training Step:1502... Training loss:2.4567... 0.1956 sec/batch\n",
      "Epoch:6/20... Training Step:1503... Training loss:2.4474... 0.2050 sec/batch\n",
      "Epoch:6/20... Training Step:1504... Training loss:2.4025... 0.2059 sec/batch\n",
      "Epoch:6/20... Training Step:1505... Training loss:2.4383... 0.1961 sec/batch\n",
      "Epoch:6/20... Training Step:1506... Training loss:2.4489... 0.1946 sec/batch\n",
      "Epoch:6/20... Training Step:1507... Training loss:2.4319... 0.1920 sec/batch\n",
      "Epoch:6/20... Training Step:1508... Training loss:2.4426... 0.2117 sec/batch\n",
      "Epoch:6/20... Training Step:1509... Training loss:2.4684... 0.1908 sec/batch\n",
      "Epoch:6/20... Training Step:1510... Training loss:2.4635... 0.1971 sec/batch\n",
      "Epoch:6/20... Training Step:1511... Training loss:2.4441... 0.1989 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6/20... Training Step:1512... Training loss:2.4501... 0.2000 sec/batch\n",
      "Epoch:6/20... Training Step:1513... Training loss:2.4199... 0.2012 sec/batch\n",
      "Epoch:6/20... Training Step:1514... Training loss:2.4567... 0.2035 sec/batch\n",
      "Epoch:6/20... Training Step:1515... Training loss:2.4367... 0.2039 sec/batch\n",
      "Epoch:6/20... Training Step:1516... Training loss:2.4075... 0.2171 sec/batch\n",
      "Epoch:6/20... Training Step:1517... Training loss:2.4174... 0.1913 sec/batch\n",
      "Epoch:6/20... Training Step:1518... Training loss:2.4343... 0.2150 sec/batch\n",
      "Epoch:6/20... Training Step:1519... Training loss:2.4512... 0.1950 sec/batch\n",
      "Epoch:6/20... Training Step:1520... Training loss:2.4026... 0.2019 sec/batch\n",
      "Epoch:6/20... Training Step:1521... Training loss:2.4017... 0.2015 sec/batch\n",
      "Epoch:6/20... Training Step:1522... Training loss:2.4200... 0.2064 sec/batch\n",
      "Epoch:6/20... Training Step:1523... Training loss:2.4344... 0.2012 sec/batch\n",
      "Epoch:6/20... Training Step:1524... Training loss:2.4042... 0.2148 sec/batch\n",
      "Epoch:6/20... Training Step:1525... Training loss:2.3755... 0.1956 sec/batch\n",
      "Epoch:6/20... Training Step:1526... Training loss:2.3993... 0.1932 sec/batch\n",
      "Epoch:6/20... Training Step:1527... Training loss:2.3940... 0.1931 sec/batch\n",
      "Epoch:6/20... Training Step:1528... Training loss:2.4138... 0.2031 sec/batch\n",
      "Epoch:6/20... Training Step:1529... Training loss:2.4325... 0.2209 sec/batch\n",
      "Epoch:6/20... Training Step:1530... Training loss:2.4084... 0.1942 sec/batch\n",
      "Epoch:6/20... Training Step:1531... Training loss:2.3975... 0.1946 sec/batch\n",
      "Epoch:6/20... Training Step:1532... Training loss:2.3996... 0.1915 sec/batch\n",
      "Epoch:6/20... Training Step:1533... Training loss:2.4347... 0.1962 sec/batch\n",
      "Epoch:6/20... Training Step:1534... Training loss:2.4572... 0.1949 sec/batch\n",
      "Epoch:6/20... Training Step:1535... Training loss:2.4331... 0.2043 sec/batch\n",
      "Epoch:6/20... Training Step:1536... Training loss:2.4047... 0.2099 sec/batch\n",
      "Epoch:6/20... Training Step:1537... Training loss:2.4251... 0.1917 sec/batch\n",
      "Epoch:6/20... Training Step:1538... Training loss:2.4419... 0.2036 sec/batch\n",
      "Epoch:6/20... Training Step:1539... Training loss:2.4381... 0.2046 sec/batch\n",
      "Epoch:6/20... Training Step:1540... Training loss:2.4376... 0.2042 sec/batch\n",
      "Epoch:6/20... Training Step:1541... Training loss:2.4354... 0.1925 sec/batch\n",
      "Epoch:6/20... Training Step:1542... Training loss:2.4497... 0.1927 sec/batch\n",
      "Epoch:6/20... Training Step:1543... Training loss:2.4767... 0.2019 sec/batch\n",
      "Epoch:6/20... Training Step:1544... Training loss:2.4368... 0.2025 sec/batch\n",
      "Epoch:6/20... Training Step:1545... Training loss:2.4004... 0.1968 sec/batch\n",
      "Epoch:6/20... Training Step:1546... Training loss:2.4217... 0.1942 sec/batch\n",
      "Epoch:6/20... Training Step:1547... Training loss:2.4328... 0.1937 sec/batch\n",
      "Epoch:6/20... Training Step:1548... Training loss:2.4563... 0.2148 sec/batch\n",
      "Epoch:6/20... Training Step:1549... Training loss:2.4063... 0.2073 sec/batch\n",
      "Epoch:6/20... Training Step:1550... Training loss:2.4436... 0.1984 sec/batch\n",
      "Epoch:6/20... Training Step:1551... Training loss:2.4333... 0.1945 sec/batch\n",
      "Epoch:6/20... Training Step:1552... Training loss:2.4201... 0.1931 sec/batch\n",
      "Epoch:6/20... Training Step:1553... Training loss:2.4469... 0.1918 sec/batch\n",
      "Epoch:6/20... Training Step:1554... Training loss:2.4264... 0.1917 sec/batch\n",
      "Epoch:6/20... Training Step:1555... Training loss:2.4846... 0.1917 sec/batch\n",
      "Epoch:6/20... Training Step:1556... Training loss:2.4605... 0.1929 sec/batch\n",
      "Epoch:6/20... Training Step:1557... Training loss:2.4608... 0.1914 sec/batch\n",
      "Epoch:6/20... Training Step:1558... Training loss:2.4322... 0.1918 sec/batch\n",
      "Epoch:6/20... Training Step:1559... Training loss:2.4528... 0.1923 sec/batch\n",
      "Epoch:6/20... Training Step:1560... Training loss:2.4446... 0.1959 sec/batch\n",
      "Epoch:6/20... Training Step:1561... Training loss:2.4358... 0.1945 sec/batch\n",
      "Epoch:6/20... Training Step:1562... Training loss:2.3960... 0.1948 sec/batch\n",
      "Epoch:6/20... Training Step:1563... Training loss:2.3889... 0.2021 sec/batch\n",
      "Epoch:6/20... Training Step:1564... Training loss:2.4318... 0.1994 sec/batch\n",
      "Epoch:6/20... Training Step:1565... Training loss:2.4299... 0.2124 sec/batch\n",
      "Epoch:6/20... Training Step:1566... Training loss:2.4424... 0.2093 sec/batch\n",
      "Epoch:6/20... Training Step:1567... Training loss:2.4498... 0.1978 sec/batch\n",
      "Epoch:6/20... Training Step:1568... Training loss:2.4226... 0.1952 sec/batch\n",
      "Epoch:6/20... Training Step:1569... Training loss:2.4417... 0.1915 sec/batch\n",
      "Epoch:6/20... Training Step:1570... Training loss:2.4523... 0.1973 sec/batch\n",
      "Epoch:6/20... Training Step:1571... Training loss:2.4501... 0.1986 sec/batch\n",
      "Epoch:6/20... Training Step:1572... Training loss:2.4516... 0.2035 sec/batch\n",
      "Epoch:6/20... Training Step:1573... Training loss:2.4382... 0.2087 sec/batch\n",
      "Epoch:6/20... Training Step:1574... Training loss:2.4658... 0.2049 sec/batch\n",
      "Epoch:6/20... Training Step:1575... Training loss:2.4362... 0.1955 sec/batch\n",
      "Epoch:6/20... Training Step:1576... Training loss:2.4451... 0.1932 sec/batch\n",
      "Epoch:6/20... Training Step:1577... Training loss:2.4642... 0.1928 sec/batch\n",
      "Epoch:6/20... Training Step:1578... Training loss:2.4703... 0.2058 sec/batch\n",
      "Epoch:6/20... Training Step:1579... Training loss:2.4734... 0.1922 sec/batch\n",
      "Epoch:6/20... Training Step:1580... Training loss:2.5096... 0.2134 sec/batch\n",
      "Epoch:6/20... Training Step:1581... Training loss:2.4600... 0.1920 sec/batch\n",
      "Epoch:6/20... Training Step:1582... Training loss:2.4549... 0.1936 sec/batch\n",
      "Epoch:6/20... Training Step:1583... Training loss:2.4646... 0.1962 sec/batch\n",
      "Epoch:6/20... Training Step:1584... Training loss:2.4722... 0.1933 sec/batch\n",
      "Epoch:6/20... Training Step:1585... Training loss:2.4402... 0.1942 sec/batch\n",
      "Epoch:6/20... Training Step:1586... Training loss:2.4344... 0.1944 sec/batch\n",
      "Epoch:6/20... Training Step:1587... Training loss:2.4319... 0.1948 sec/batch\n",
      "Epoch:6/20... Training Step:1588... Training loss:2.4422... 0.1947 sec/batch\n",
      "Epoch:6/20... Training Step:1589... Training loss:2.4376... 0.1941 sec/batch\n",
      "Epoch:6/20... Training Step:1590... Training loss:2.4350... 0.2018 sec/batch\n",
      "Epoch:6/20... Training Step:1591... Training loss:2.4233... 0.1911 sec/batch\n",
      "Epoch:6/20... Training Step:1592... Training loss:2.4804... 0.1961 sec/batch\n",
      "Epoch:6/20... Training Step:1593... Training loss:2.4523... 0.2174 sec/batch\n",
      "Epoch:6/20... Training Step:1594... Training loss:2.4171... 0.2057 sec/batch\n",
      "Epoch:6/20... Training Step:1595... Training loss:2.4394... 0.1928 sec/batch\n",
      "Epoch:6/20... Training Step:1596... Training loss:2.4476... 0.2489 sec/batch\n",
      "Epoch:6/20... Training Step:1597... Training loss:2.4038... 0.2518 sec/batch\n",
      "Epoch:6/20... Training Step:1598... Training loss:2.4141... 0.1918 sec/batch\n",
      "Epoch:6/20... Training Step:1599... Training loss:2.4375... 0.1995 sec/batch\n",
      "Epoch:6/20... Training Step:1600... Training loss:2.4553... 0.1918 sec/batch\n",
      "Epoch:6/20... Training Step:1601... Training loss:2.4082... 0.1905 sec/batch\n",
      "Epoch:6/20... Training Step:1602... Training loss:2.4156... 0.1912 sec/batch\n",
      "Epoch:6/20... Training Step:1603... Training loss:2.4432... 0.1915 sec/batch\n",
      "Epoch:6/20... Training Step:1604... Training loss:2.4240... 0.2037 sec/batch\n",
      "Epoch:6/20... Training Step:1605... Training loss:2.4301... 0.2016 sec/batch\n",
      "Epoch:6/20... Training Step:1606... Training loss:2.4503... 0.1937 sec/batch\n",
      "Epoch:6/20... Training Step:1607... Training loss:2.4387... 0.1945 sec/batch\n",
      "Epoch:6/20... Training Step:1608... Training loss:2.4229... 0.1923 sec/batch\n",
      "Epoch:6/20... Training Step:1609... Training loss:2.4102... 0.1911 sec/batch\n",
      "Epoch:6/20... Training Step:1610... Training loss:2.4103... 0.1922 sec/batch\n",
      "Epoch:6/20... Training Step:1611... Training loss:2.4236... 0.2073 sec/batch\n",
      "Epoch:6/20... Training Step:1612... Training loss:2.4151... 0.1910 sec/batch\n",
      "Epoch:6/20... Training Step:1613... Training loss:2.4080... 0.1924 sec/batch\n",
      "Epoch:6/20... Training Step:1614... Training loss:2.4544... 0.1988 sec/batch\n",
      "Epoch:6/20... Training Step:1615... Training loss:2.4518... 0.2224 sec/batch\n",
      "Epoch:6/20... Training Step:1616... Training loss:2.4818... 0.2034 sec/batch\n",
      "Epoch:6/20... Training Step:1617... Training loss:2.4801... 0.2022 sec/batch\n",
      "Epoch:6/20... Training Step:1618... Training loss:2.4840... 0.1909 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6/20... Training Step:1619... Training loss:2.4250... 0.2139 sec/batch\n",
      "Epoch:6/20... Training Step:1620... Training loss:2.4156... 0.2101 sec/batch\n",
      "Epoch:6/20... Training Step:1621... Training loss:2.4275... 0.1918 sec/batch\n",
      "Epoch:6/20... Training Step:1622... Training loss:2.4084... 0.1928 sec/batch\n",
      "Epoch:6/20... Training Step:1623... Training loss:2.4128... 0.2028 sec/batch\n",
      "Epoch:6/20... Training Step:1624... Training loss:2.4262... 0.2124 sec/batch\n",
      "Epoch:6/20... Training Step:1625... Training loss:2.4374... 0.1914 sec/batch\n",
      "Epoch:6/20... Training Step:1626... Training loss:2.4239... 0.1943 sec/batch\n",
      "Epoch:6/20... Training Step:1627... Training loss:2.4152... 0.2039 sec/batch\n",
      "Epoch:6/20... Training Step:1628... Training loss:2.4081... 0.2052 sec/batch\n",
      "Epoch:6/20... Training Step:1629... Training loss:2.4144... 0.1940 sec/batch\n",
      "Epoch:6/20... Training Step:1630... Training loss:2.4241... 0.1991 sec/batch\n",
      "Epoch:6/20... Training Step:1631... Training loss:2.4070... 0.2010 sec/batch\n",
      "Epoch:6/20... Training Step:1632... Training loss:2.4402... 0.2031 sec/batch\n",
      "Epoch:6/20... Training Step:1633... Training loss:2.4262... 0.1974 sec/batch\n",
      "Epoch:6/20... Training Step:1634... Training loss:2.4163... 0.1955 sec/batch\n",
      "Epoch:6/20... Training Step:1635... Training loss:2.4309... 0.1924 sec/batch\n",
      "Epoch:6/20... Training Step:1636... Training loss:2.4088... 0.1998 sec/batch\n",
      "Epoch:6/20... Training Step:1637... Training loss:2.4437... 0.1998 sec/batch\n",
      "Epoch:6/20... Training Step:1638... Training loss:2.4434... 0.2067 sec/batch\n",
      "Epoch:6/20... Training Step:1639... Training loss:2.4321... 0.1982 sec/batch\n",
      "Epoch:6/20... Training Step:1640... Training loss:2.4302... 0.2099 sec/batch\n",
      "Epoch:6/20... Training Step:1641... Training loss:2.4181... 0.1926 sec/batch\n",
      "Epoch:6/20... Training Step:1642... Training loss:2.4413... 0.2026 sec/batch\n",
      "Epoch:6/20... Training Step:1643... Training loss:2.4586... 0.1981 sec/batch\n",
      "Epoch:6/20... Training Step:1644... Training loss:2.4219... 0.1937 sec/batch\n",
      "Epoch:6/20... Training Step:1645... Training loss:2.4406... 0.1909 sec/batch\n",
      "Epoch:6/20... Training Step:1646... Training loss:2.4387... 0.2185 sec/batch\n",
      "Epoch:6/20... Training Step:1647... Training loss:2.4062... 0.2007 sec/batch\n",
      "Epoch:6/20... Training Step:1648... Training loss:2.4392... 0.2148 sec/batch\n",
      "Epoch:6/20... Training Step:1649... Training loss:2.4265... 0.1923 sec/batch\n",
      "Epoch:6/20... Training Step:1650... Training loss:2.4188... 0.2101 sec/batch\n",
      "Epoch:6/20... Training Step:1651... Training loss:2.4363... 0.1967 sec/batch\n",
      "Epoch:6/20... Training Step:1652... Training loss:2.4309... 0.1950 sec/batch\n",
      "Epoch:6/20... Training Step:1653... Training loss:2.4490... 0.1925 sec/batch\n",
      "Epoch:6/20... Training Step:1654... Training loss:2.4105... 0.2042 sec/batch\n",
      "Epoch:6/20... Training Step:1655... Training loss:2.3865... 0.2108 sec/batch\n",
      "Epoch:6/20... Training Step:1656... Training loss:2.4220... 0.2058 sec/batch\n",
      "Epoch:6/20... Training Step:1657... Training loss:2.4056... 0.2018 sec/batch\n",
      "Epoch:6/20... Training Step:1658... Training loss:2.3980... 0.1936 sec/batch\n",
      "Epoch:6/20... Training Step:1659... Training loss:2.3875... 0.1926 sec/batch\n",
      "Epoch:6/20... Training Step:1660... Training loss:2.3851... 0.1925 sec/batch\n",
      "Epoch:6/20... Training Step:1661... Training loss:2.3916... 0.2027 sec/batch\n",
      "Epoch:6/20... Training Step:1662... Training loss:2.3692... 0.1943 sec/batch\n",
      "Epoch:6/20... Training Step:1663... Training loss:2.3980... 0.2093 sec/batch\n",
      "Epoch:6/20... Training Step:1664... Training loss:2.4438... 0.2147 sec/batch\n",
      "Epoch:6/20... Training Step:1665... Training loss:2.3901... 0.1969 sec/batch\n",
      "Epoch:6/20... Training Step:1666... Training loss:2.3601... 0.1966 sec/batch\n",
      "Epoch:6/20... Training Step:1667... Training loss:2.3840... 0.1928 sec/batch\n",
      "Epoch:6/20... Training Step:1668... Training loss:2.3702... 0.2083 sec/batch\n",
      "Epoch:6/20... Training Step:1669... Training loss:2.3958... 0.1926 sec/batch\n",
      "Epoch:6/20... Training Step:1670... Training loss:2.4061... 0.1936 sec/batch\n",
      "Epoch:6/20... Training Step:1671... Training loss:2.4237... 0.2063 sec/batch\n",
      "Epoch:6/20... Training Step:1672... Training loss:2.4379... 0.2093 sec/batch\n",
      "Epoch:6/20... Training Step:1673... Training loss:2.4094... 0.1930 sec/batch\n",
      "Epoch:6/20... Training Step:1674... Training loss:2.3878... 0.1978 sec/batch\n",
      "Epoch:6/20... Training Step:1675... Training loss:2.4206... 0.2101 sec/batch\n",
      "Epoch:6/20... Training Step:1676... Training loss:2.3868... 0.1950 sec/batch\n",
      "Epoch:6/20... Training Step:1677... Training loss:2.4083... 0.1932 sec/batch\n",
      "Epoch:6/20... Training Step:1678... Training loss:2.4075... 0.1904 sec/batch\n",
      "Epoch:6/20... Training Step:1679... Training loss:2.4114... 0.1972 sec/batch\n",
      "Epoch:6/20... Training Step:1680... Training loss:2.3919... 0.1964 sec/batch\n",
      "Epoch:6/20... Training Step:1681... Training loss:2.3840... 0.2041 sec/batch\n",
      "Epoch:6/20... Training Step:1682... Training loss:2.4166... 0.2016 sec/batch\n",
      "Epoch:6/20... Training Step:1683... Training loss:2.4013... 0.1996 sec/batch\n",
      "Epoch:6/20... Training Step:1684... Training loss:2.4070... 0.2145 sec/batch\n",
      "Epoch:6/20... Training Step:1685... Training loss:2.4201... 0.2095 sec/batch\n",
      "Epoch:6/20... Training Step:1686... Training loss:2.4201... 0.2091 sec/batch\n",
      "Epoch:6/20... Training Step:1687... Training loss:2.3959... 0.1927 sec/batch\n",
      "Epoch:6/20... Training Step:1688... Training loss:2.4062... 0.1920 sec/batch\n",
      "Epoch:6/20... Training Step:1689... Training loss:2.3813... 0.2185 sec/batch\n",
      "Epoch:6/20... Training Step:1690... Training loss:2.3914... 0.1925 sec/batch\n",
      "Epoch:6/20... Training Step:1691... Training loss:2.4009... 0.1927 sec/batch\n",
      "Epoch:6/20... Training Step:1692... Training loss:2.4399... 0.1911 sec/batch\n",
      "Epoch:6/20... Training Step:1693... Training loss:2.4412... 0.1932 sec/batch\n",
      "Epoch:6/20... Training Step:1694... Training loss:2.3954... 0.1921 sec/batch\n",
      "Epoch:6/20... Training Step:1695... Training loss:2.4213... 0.2069 sec/batch\n",
      "Epoch:6/20... Training Step:1696... Training loss:2.4189... 0.1947 sec/batch\n",
      "Epoch:6/20... Training Step:1697... Training loss:2.4001... 0.2043 sec/batch\n",
      "Epoch:6/20... Training Step:1698... Training loss:2.4021... 0.2048 sec/batch\n",
      "Epoch:6/20... Training Step:1699... Training loss:2.3885... 0.2080 sec/batch\n",
      "Epoch:6/20... Training Step:1700... Training loss:2.4002... 0.2048 sec/batch\n",
      "Epoch:6/20... Training Step:1701... Training loss:2.4103... 0.2054 sec/batch\n",
      "Epoch:6/20... Training Step:1702... Training loss:2.4328... 0.2022 sec/batch\n",
      "Epoch:6/20... Training Step:1703... Training loss:2.3890... 0.1979 sec/batch\n",
      "Epoch:6/20... Training Step:1704... Training loss:2.4062... 0.2029 sec/batch\n",
      "Epoch:6/20... Training Step:1705... Training loss:2.4305... 0.1927 sec/batch\n",
      "Epoch:6/20... Training Step:1706... Training loss:2.4122... 0.1991 sec/batch\n",
      "Epoch:6/20... Training Step:1707... Training loss:2.3956... 0.1932 sec/batch\n",
      "Epoch:6/20... Training Step:1708... Training loss:2.4255... 0.2033 sec/batch\n",
      "Epoch:6/20... Training Step:1709... Training loss:2.4256... 0.2135 sec/batch\n",
      "Epoch:6/20... Training Step:1710... Training loss:2.4207... 0.1919 sec/batch\n",
      "Epoch:6/20... Training Step:1711... Training loss:2.4252... 0.1926 sec/batch\n",
      "Epoch:6/20... Training Step:1712... Training loss:2.4035... 0.1941 sec/batch\n",
      "Epoch:6/20... Training Step:1713... Training loss:2.3847... 0.2042 sec/batch\n",
      "Epoch:6/20... Training Step:1714... Training loss:2.3599... 0.1905 sec/batch\n",
      "Epoch:6/20... Training Step:1715... Training loss:2.3597... 0.1913 sec/batch\n",
      "Epoch:6/20... Training Step:1716... Training loss:2.3894... 0.2075 sec/batch\n",
      "Epoch:6/20... Training Step:1717... Training loss:2.4252... 0.1919 sec/batch\n",
      "Epoch:6/20... Training Step:1718... Training loss:2.3903... 0.1938 sec/batch\n",
      "Epoch:6/20... Training Step:1719... Training loss:2.3833... 0.2018 sec/batch\n",
      "Epoch:6/20... Training Step:1720... Training loss:2.3968... 0.2085 sec/batch\n",
      "Epoch:6/20... Training Step:1721... Training loss:2.3704... 0.1935 sec/batch\n",
      "Epoch:6/20... Training Step:1722... Training loss:2.4083... 0.1938 sec/batch\n",
      "Epoch:6/20... Training Step:1723... Training loss:2.4274... 0.2026 sec/batch\n",
      "Epoch:6/20... Training Step:1724... Training loss:2.3734... 0.2112 sec/batch\n",
      "Epoch:6/20... Training Step:1725... Training loss:2.4034... 0.2108 sec/batch\n",
      "Epoch:6/20... Training Step:1726... Training loss:2.4078... 0.1904 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6/20... Training Step:1727... Training loss:2.4111... 0.2066 sec/batch\n",
      "Epoch:6/20... Training Step:1728... Training loss:2.4231... 0.1972 sec/batch\n",
      "Epoch:6/20... Training Step:1729... Training loss:2.4034... 0.1977 sec/batch\n",
      "Epoch:6/20... Training Step:1730... Training loss:2.4154... 0.1955 sec/batch\n",
      "Epoch:6/20... Training Step:1731... Training loss:2.4056... 0.2007 sec/batch\n",
      "Epoch:6/20... Training Step:1732... Training loss:2.4197... 0.1919 sec/batch\n",
      "Epoch:6/20... Training Step:1733... Training loss:2.4144... 0.1982 sec/batch\n",
      "Epoch:6/20... Training Step:1734... Training loss:2.4452... 0.2094 sec/batch\n",
      "Epoch:6/20... Training Step:1735... Training loss:2.4407... 0.1914 sec/batch\n",
      "Epoch:6/20... Training Step:1736... Training loss:2.4228... 0.1994 sec/batch\n",
      "Epoch:6/20... Training Step:1737... Training loss:2.4064... 0.1960 sec/batch\n",
      "Epoch:6/20... Training Step:1738... Training loss:2.4079... 0.2116 sec/batch\n",
      "Epoch:6/20... Training Step:1739... Training loss:2.3887... 0.1967 sec/batch\n",
      "Epoch:6/20... Training Step:1740... Training loss:2.3896... 0.2088 sec/batch\n",
      "Epoch:7/20... Training Step:1741... Training loss:2.5689... 0.1919 sec/batch\n",
      "Epoch:7/20... Training Step:1742... Training loss:2.4036... 0.2022 sec/batch\n",
      "Epoch:7/20... Training Step:1743... Training loss:2.4055... 0.1916 sec/batch\n",
      "Epoch:7/20... Training Step:1744... Training loss:2.4250... 0.2064 sec/batch\n",
      "Epoch:7/20... Training Step:1745... Training loss:2.4046... 0.2085 sec/batch\n",
      "Epoch:7/20... Training Step:1746... Training loss:2.4072... 0.1913 sec/batch\n",
      "Epoch:7/20... Training Step:1747... Training loss:2.4009... 0.2028 sec/batch\n",
      "Epoch:7/20... Training Step:1748... Training loss:2.4095... 0.1913 sec/batch\n",
      "Epoch:7/20... Training Step:1749... Training loss:2.3969... 0.1935 sec/batch\n",
      "Epoch:7/20... Training Step:1750... Training loss:2.3971... 0.2131 sec/batch\n",
      "Epoch:7/20... Training Step:1751... Training loss:2.4426... 0.2041 sec/batch\n",
      "Epoch:7/20... Training Step:1752... Training loss:2.4158... 0.1990 sec/batch\n",
      "Epoch:7/20... Training Step:1753... Training loss:2.4174... 0.2067 sec/batch\n",
      "Epoch:7/20... Training Step:1754... Training loss:2.4232... 0.1933 sec/batch\n",
      "Epoch:7/20... Training Step:1755... Training loss:2.4000... 0.1940 sec/batch\n",
      "Epoch:7/20... Training Step:1756... Training loss:2.4203... 0.1918 sec/batch\n",
      "Epoch:7/20... Training Step:1757... Training loss:2.4159... 0.1933 sec/batch\n",
      "Epoch:7/20... Training Step:1758... Training loss:2.4382... 0.2049 sec/batch\n",
      "Epoch:7/20... Training Step:1759... Training loss:2.4150... 0.1946 sec/batch\n",
      "Epoch:7/20... Training Step:1760... Training loss:2.4200... 0.1926 sec/batch\n",
      "Epoch:7/20... Training Step:1761... Training loss:2.4198... 0.1988 sec/batch\n",
      "Epoch:7/20... Training Step:1762... Training loss:2.4139... 0.2104 sec/batch\n",
      "Epoch:7/20... Training Step:1763... Training loss:2.3918... 0.1924 sec/batch\n",
      "Epoch:7/20... Training Step:1764... Training loss:2.4404... 0.1964 sec/batch\n",
      "Epoch:7/20... Training Step:1765... Training loss:2.3822... 0.2045 sec/batch\n",
      "Epoch:7/20... Training Step:1766... Training loss:2.4091... 0.1922 sec/batch\n",
      "Epoch:7/20... Training Step:1767... Training loss:2.4203... 0.2061 sec/batch\n",
      "Epoch:7/20... Training Step:1768... Training loss:2.4122... 0.1904 sec/batch\n",
      "Epoch:7/20... Training Step:1769... Training loss:2.4264... 0.1915 sec/batch\n",
      "Epoch:7/20... Training Step:1770... Training loss:2.4224... 0.1925 sec/batch\n",
      "Epoch:7/20... Training Step:1771... Training loss:2.4001... 0.1940 sec/batch\n",
      "Epoch:7/20... Training Step:1772... Training loss:2.3962... 0.1929 sec/batch\n",
      "Epoch:7/20... Training Step:1773... Training loss:2.3941... 0.2102 sec/batch\n",
      "Epoch:7/20... Training Step:1774... Training loss:2.4006... 0.2147 sec/batch\n",
      "Epoch:7/20... Training Step:1775... Training loss:2.4156... 0.2025 sec/batch\n",
      "Epoch:7/20... Training Step:1776... Training loss:2.3830... 0.1944 sec/batch\n",
      "Epoch:7/20... Training Step:1777... Training loss:2.4003... 0.2084 sec/batch\n",
      "Epoch:7/20... Training Step:1778... Training loss:2.4073... 0.1925 sec/batch\n",
      "Epoch:7/20... Training Step:1779... Training loss:2.3898... 0.2071 sec/batch\n",
      "Epoch:7/20... Training Step:1780... Training loss:2.3456... 0.2050 sec/batch\n",
      "Epoch:7/20... Training Step:1781... Training loss:2.3875... 0.2161 sec/batch\n",
      "Epoch:7/20... Training Step:1782... Training loss:2.4006... 0.1925 sec/batch\n",
      "Epoch:7/20... Training Step:1783... Training loss:2.4172... 0.1979 sec/batch\n",
      "Epoch:7/20... Training Step:1784... Training loss:2.4032... 0.2105 sec/batch\n",
      "Epoch:7/20... Training Step:1785... Training loss:2.4222... 0.2103 sec/batch\n",
      "Epoch:7/20... Training Step:1786... Training loss:2.4172... 0.2041 sec/batch\n",
      "Epoch:7/20... Training Step:1787... Training loss:2.4212... 0.1966 sec/batch\n",
      "Epoch:7/20... Training Step:1788... Training loss:2.4051... 0.1995 sec/batch\n",
      "Epoch:7/20... Training Step:1789... Training loss:2.3696... 0.1937 sec/batch\n",
      "Epoch:7/20... Training Step:1790... Training loss:2.4266... 0.1977 sec/batch\n",
      "Epoch:7/20... Training Step:1791... Training loss:2.3760... 0.1941 sec/batch\n",
      "Epoch:7/20... Training Step:1792... Training loss:2.3990... 0.1927 sec/batch\n",
      "Epoch:7/20... Training Step:1793... Training loss:2.3943... 0.1982 sec/batch\n",
      "Epoch:7/20... Training Step:1794... Training loss:2.3579... 0.2064 sec/batch\n",
      "Epoch:7/20... Training Step:1795... Training loss:2.3852... 0.1951 sec/batch\n",
      "Epoch:7/20... Training Step:1796... Training loss:2.3889... 0.2060 sec/batch\n",
      "Epoch:7/20... Training Step:1797... Training loss:2.3685... 0.1921 sec/batch\n",
      "Epoch:7/20... Training Step:1798... Training loss:2.3765... 0.1953 sec/batch\n",
      "Epoch:7/20... Training Step:1799... Training loss:2.4090... 0.1942 sec/batch\n",
      "Epoch:7/20... Training Step:1800... Training loss:2.4061... 0.1928 sec/batch\n",
      "Epoch:7/20... Training Step:1801... Training loss:2.3793... 0.1965 sec/batch\n",
      "Epoch:7/20... Training Step:1802... Training loss:2.3993... 0.1942 sec/batch\n",
      "Epoch:7/20... Training Step:1803... Training loss:2.3667... 0.1950 sec/batch\n",
      "Epoch:7/20... Training Step:1804... Training loss:2.3989... 0.2061 sec/batch\n",
      "Epoch:7/20... Training Step:1805... Training loss:2.3681... 0.1992 sec/batch\n",
      "Epoch:7/20... Training Step:1806... Training loss:2.3411... 0.2129 sec/batch\n",
      "Epoch:7/20... Training Step:1807... Training loss:2.3552... 0.2032 sec/batch\n",
      "Epoch:7/20... Training Step:1808... Training loss:2.3812... 0.1911 sec/batch\n",
      "Epoch:7/20... Training Step:1809... Training loss:2.3862... 0.1999 sec/batch\n",
      "Epoch:7/20... Training Step:1810... Training loss:2.3454... 0.1923 sec/batch\n",
      "Epoch:7/20... Training Step:1811... Training loss:2.3439... 0.2016 sec/batch\n",
      "Epoch:7/20... Training Step:1812... Training loss:2.3569... 0.2052 sec/batch\n",
      "Epoch:7/20... Training Step:1813... Training loss:2.3822... 0.2135 sec/batch\n",
      "Epoch:7/20... Training Step:1814... Training loss:2.3528... 0.1925 sec/batch\n",
      "Epoch:7/20... Training Step:1815... Training loss:2.3203... 0.2091 sec/batch\n",
      "Epoch:7/20... Training Step:1816... Training loss:2.3515... 0.1969 sec/batch\n",
      "Epoch:7/20... Training Step:1817... Training loss:2.3329... 0.1969 sec/batch\n",
      "Epoch:7/20... Training Step:1818... Training loss:2.3522... 0.2146 sec/batch\n",
      "Epoch:7/20... Training Step:1819... Training loss:2.3758... 0.2053 sec/batch\n",
      "Epoch:7/20... Training Step:1820... Training loss:2.3604... 0.2014 sec/batch\n",
      "Epoch:7/20... Training Step:1821... Training loss:2.3423... 0.2026 sec/batch\n",
      "Epoch:7/20... Training Step:1822... Training loss:2.3420... 0.1985 sec/batch\n",
      "Epoch:7/20... Training Step:1823... Training loss:2.3729... 0.2144 sec/batch\n",
      "Epoch:7/20... Training Step:1824... Training loss:2.4062... 0.2126 sec/batch\n",
      "Epoch:7/20... Training Step:1825... Training loss:2.3922... 0.1951 sec/batch\n",
      "Epoch:7/20... Training Step:1826... Training loss:2.3430... 0.1952 sec/batch\n",
      "Epoch:7/20... Training Step:1827... Training loss:2.3663... 0.2044 sec/batch\n",
      "Epoch:7/20... Training Step:1828... Training loss:2.3823... 0.2064 sec/batch\n",
      "Epoch:7/20... Training Step:1829... Training loss:2.3779... 0.1927 sec/batch\n",
      "Epoch:7/20... Training Step:1830... Training loss:2.3801... 0.2090 sec/batch\n",
      "Epoch:7/20... Training Step:1831... Training loss:2.3814... 0.1961 sec/batch\n",
      "Epoch:7/20... Training Step:1832... Training loss:2.3879... 0.2105 sec/batch\n",
      "Epoch:7/20... Training Step:1833... Training loss:2.4132... 0.1955 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7/20... Training Step:1834... Training loss:2.3725... 0.1954 sec/batch\n",
      "Epoch:7/20... Training Step:1835... Training loss:2.3469... 0.1911 sec/batch\n",
      "Epoch:7/20... Training Step:1836... Training loss:2.3709... 0.2075 sec/batch\n",
      "Epoch:7/20... Training Step:1837... Training loss:2.3797... 0.2117 sec/batch\n",
      "Epoch:7/20... Training Step:1838... Training loss:2.3932... 0.1924 sec/batch\n",
      "Epoch:7/20... Training Step:1839... Training loss:2.3421... 0.1927 sec/batch\n",
      "Epoch:7/20... Training Step:1840... Training loss:2.3787... 0.2084 sec/batch\n",
      "Epoch:7/20... Training Step:1841... Training loss:2.3816... 0.1925 sec/batch\n",
      "Epoch:7/20... Training Step:1842... Training loss:2.3768... 0.2059 sec/batch\n",
      "Epoch:7/20... Training Step:1843... Training loss:2.3922... 0.2070 sec/batch\n",
      "Epoch:7/20... Training Step:1844... Training loss:2.3625... 0.1902 sec/batch\n",
      "Epoch:7/20... Training Step:1845... Training loss:2.4235... 0.1933 sec/batch\n",
      "Epoch:7/20... Training Step:1846... Training loss:2.4042... 0.2034 sec/batch\n",
      "Epoch:7/20... Training Step:1847... Training loss:2.4111... 0.1938 sec/batch\n",
      "Epoch:7/20... Training Step:1848... Training loss:2.3729... 0.2107 sec/batch\n",
      "Epoch:7/20... Training Step:1849... Training loss:2.3946... 0.1970 sec/batch\n",
      "Epoch:7/20... Training Step:1850... Training loss:2.3973... 0.2122 sec/batch\n",
      "Epoch:7/20... Training Step:1851... Training loss:2.3796... 0.1908 sec/batch\n",
      "Epoch:7/20... Training Step:1852... Training loss:2.3361... 0.2191 sec/batch\n",
      "Epoch:7/20... Training Step:1853... Training loss:2.3310... 0.1929 sec/batch\n",
      "Epoch:7/20... Training Step:1854... Training loss:2.3872... 0.1958 sec/batch\n",
      "Epoch:7/20... Training Step:1855... Training loss:2.3703... 0.1986 sec/batch\n",
      "Epoch:7/20... Training Step:1856... Training loss:2.3926... 0.1980 sec/batch\n",
      "Epoch:7/20... Training Step:1857... Training loss:2.3927... 0.1928 sec/batch\n",
      "Epoch:7/20... Training Step:1858... Training loss:2.3693... 0.1943 sec/batch\n",
      "Epoch:7/20... Training Step:1859... Training loss:2.3862... 0.1929 sec/batch\n",
      "Epoch:7/20... Training Step:1860... Training loss:2.3896... 0.2040 sec/batch\n",
      "Epoch:7/20... Training Step:1861... Training loss:2.3913... 0.1986 sec/batch\n",
      "Epoch:7/20... Training Step:1862... Training loss:2.4068... 0.2007 sec/batch\n",
      "Epoch:7/20... Training Step:1863... Training loss:2.3710... 0.1979 sec/batch\n",
      "Epoch:7/20... Training Step:1864... Training loss:2.4029... 0.2030 sec/batch\n",
      "Epoch:7/20... Training Step:1865... Training loss:2.3849... 0.1907 sec/batch\n",
      "Epoch:7/20... Training Step:1866... Training loss:2.3912... 0.1986 sec/batch\n",
      "Epoch:7/20... Training Step:1867... Training loss:2.4041... 0.2016 sec/batch\n",
      "Epoch:7/20... Training Step:1868... Training loss:2.4130... 0.2055 sec/batch\n",
      "Epoch:7/20... Training Step:1869... Training loss:2.4233... 0.2105 sec/batch\n",
      "Epoch:7/20... Training Step:1870... Training loss:2.4550... 0.1922 sec/batch\n",
      "Epoch:7/20... Training Step:1871... Training loss:2.3983... 0.1905 sec/batch\n",
      "Epoch:7/20... Training Step:1872... Training loss:2.4039... 0.1905 sec/batch\n",
      "Epoch:7/20... Training Step:1873... Training loss:2.4141... 0.1929 sec/batch\n",
      "Epoch:7/20... Training Step:1874... Training loss:2.4255... 0.1969 sec/batch\n",
      "Epoch:7/20... Training Step:1875... Training loss:2.3925... 0.1943 sec/batch\n",
      "Epoch:7/20... Training Step:1876... Training loss:2.3827... 0.2123 sec/batch\n",
      "Epoch:7/20... Training Step:1877... Training loss:2.3829... 0.1920 sec/batch\n",
      "Epoch:7/20... Training Step:1878... Training loss:2.3836... 0.1958 sec/batch\n",
      "Epoch:7/20... Training Step:1879... Training loss:2.3750... 0.1944 sec/batch\n",
      "Epoch:7/20... Training Step:1880... Training loss:2.3736... 0.2073 sec/batch\n",
      "Epoch:7/20... Training Step:1881... Training loss:2.3666... 0.2124 sec/batch\n",
      "Epoch:7/20... Training Step:1882... Training loss:2.4392... 0.2025 sec/batch\n",
      "Epoch:7/20... Training Step:1883... Training loss:2.3886... 0.2074 sec/batch\n",
      "Epoch:7/20... Training Step:1884... Training loss:2.3666... 0.2115 sec/batch\n",
      "Epoch:7/20... Training Step:1885... Training loss:2.3773... 0.1986 sec/batch\n",
      "Epoch:7/20... Training Step:1886... Training loss:2.3840... 0.1939 sec/batch\n",
      "Epoch:7/20... Training Step:1887... Training loss:2.3544... 0.2089 sec/batch\n",
      "Epoch:7/20... Training Step:1888... Training loss:2.3675... 0.2122 sec/batch\n",
      "Epoch:7/20... Training Step:1889... Training loss:2.3824... 0.2064 sec/batch\n",
      "Epoch:7/20... Training Step:1890... Training loss:2.3963... 0.2015 sec/batch\n",
      "Epoch:7/20... Training Step:1891... Training loss:2.3584... 0.1984 sec/batch\n",
      "Epoch:7/20... Training Step:1892... Training loss:2.3775... 0.1956 sec/batch\n",
      "Epoch:7/20... Training Step:1893... Training loss:2.3859... 0.1947 sec/batch\n",
      "Epoch:7/20... Training Step:1894... Training loss:2.3637... 0.2170 sec/batch\n",
      "Epoch:7/20... Training Step:1895... Training loss:2.3681... 0.1919 sec/batch\n",
      "Epoch:7/20... Training Step:1896... Training loss:2.3958... 0.1962 sec/batch\n",
      "Epoch:7/20... Training Step:1897... Training loss:2.3883... 0.1952 sec/batch\n",
      "Epoch:7/20... Training Step:1898... Training loss:2.3646... 0.1929 sec/batch\n",
      "Epoch:7/20... Training Step:1899... Training loss:2.3518... 0.1932 sec/batch\n",
      "Epoch:7/20... Training Step:1900... Training loss:2.3552... 0.1932 sec/batch\n",
      "Epoch:7/20... Training Step:1901... Training loss:2.3574... 0.1988 sec/batch\n",
      "Epoch:7/20... Training Step:1902... Training loss:2.3560... 0.2004 sec/batch\n",
      "Epoch:7/20... Training Step:1903... Training loss:2.3572... 0.2086 sec/batch\n",
      "Epoch:7/20... Training Step:1904... Training loss:2.3982... 0.2067 sec/batch\n",
      "Epoch:7/20... Training Step:1905... Training loss:2.3950... 0.1995 sec/batch\n",
      "Epoch:7/20... Training Step:1906... Training loss:2.4211... 0.1997 sec/batch\n",
      "Epoch:7/20... Training Step:1907... Training loss:2.4182... 0.2012 sec/batch\n",
      "Epoch:7/20... Training Step:1908... Training loss:2.4217... 0.1947 sec/batch\n",
      "Epoch:7/20... Training Step:1909... Training loss:2.3570... 0.2025 sec/batch\n",
      "Epoch:7/20... Training Step:1910... Training loss:2.3573... 0.1946 sec/batch\n",
      "Epoch:7/20... Training Step:1911... Training loss:2.3671... 0.2015 sec/batch\n",
      "Epoch:7/20... Training Step:1912... Training loss:2.3491... 0.2034 sec/batch\n",
      "Epoch:7/20... Training Step:1913... Training loss:2.3585... 0.2043 sec/batch\n",
      "Epoch:7/20... Training Step:1914... Training loss:2.3790... 0.2040 sec/batch\n",
      "Epoch:7/20... Training Step:1915... Training loss:2.3754... 0.1917 sec/batch\n",
      "Epoch:7/20... Training Step:1916... Training loss:2.3745... 0.2119 sec/batch\n",
      "Epoch:7/20... Training Step:1917... Training loss:2.3618... 0.1934 sec/batch\n",
      "Epoch:7/20... Training Step:1918... Training loss:2.3464... 0.2075 sec/batch\n",
      "Epoch:7/20... Training Step:1919... Training loss:2.3499... 0.2032 sec/batch\n",
      "Epoch:7/20... Training Step:1920... Training loss:2.3670... 0.1935 sec/batch\n",
      "Epoch:7/20... Training Step:1921... Training loss:2.3629... 0.1922 sec/batch\n",
      "Epoch:7/20... Training Step:1922... Training loss:2.3831... 0.2110 sec/batch\n",
      "Epoch:7/20... Training Step:1923... Training loss:2.3750... 0.1917 sec/batch\n",
      "Epoch:7/20... Training Step:1924... Training loss:2.3609... 0.1926 sec/batch\n",
      "Epoch:7/20... Training Step:1925... Training loss:2.3741... 0.2084 sec/batch\n",
      "Epoch:7/20... Training Step:1926... Training loss:2.3536... 0.1902 sec/batch\n",
      "Epoch:7/20... Training Step:1927... Training loss:2.3870... 0.1932 sec/batch\n",
      "Epoch:7/20... Training Step:1928... Training loss:2.3902... 0.2159 sec/batch\n",
      "Epoch:7/20... Training Step:1929... Training loss:2.3746... 0.1908 sec/batch\n",
      "Epoch:7/20... Training Step:1930... Training loss:2.3811... 0.2165 sec/batch\n",
      "Epoch:7/20... Training Step:1931... Training loss:2.3597... 0.1915 sec/batch\n",
      "Epoch:7/20... Training Step:1932... Training loss:2.3888... 0.1915 sec/batch\n",
      "Epoch:7/20... Training Step:1933... Training loss:2.4033... 0.2042 sec/batch\n",
      "Epoch:7/20... Training Step:1934... Training loss:2.3690... 0.2046 sec/batch\n",
      "Epoch:7/20... Training Step:1935... Training loss:2.3920... 0.2064 sec/batch\n",
      "Epoch:7/20... Training Step:1936... Training loss:2.3810... 0.1932 sec/batch\n",
      "Epoch:7/20... Training Step:1937... Training loss:2.3479... 0.2169 sec/batch\n",
      "Epoch:7/20... Training Step:1938... Training loss:2.3766... 0.1991 sec/batch\n",
      "Epoch:7/20... Training Step:1939... Training loss:2.3697... 0.1959 sec/batch\n",
      "Epoch:7/20... Training Step:1940... Training loss:2.3664... 0.1960 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7/20... Training Step:1941... Training loss:2.3866... 0.2048 sec/batch\n",
      "Epoch:7/20... Training Step:1942... Training loss:2.3702... 0.1922 sec/batch\n",
      "Epoch:7/20... Training Step:1943... Training loss:2.3931... 0.1925 sec/batch\n",
      "Epoch:7/20... Training Step:1944... Training loss:2.3533... 0.2024 sec/batch\n",
      "Epoch:7/20... Training Step:1945... Training loss:2.3241... 0.2014 sec/batch\n",
      "Epoch:7/20... Training Step:1946... Training loss:2.3725... 0.2035 sec/batch\n",
      "Epoch:7/20... Training Step:1947... Training loss:2.3542... 0.2061 sec/batch\n",
      "Epoch:7/20... Training Step:1948... Training loss:2.3513... 0.2068 sec/batch\n",
      "Epoch:7/20... Training Step:1949... Training loss:2.3245... 0.1929 sec/batch\n",
      "Epoch:7/20... Training Step:1950... Training loss:2.3312... 0.1984 sec/batch\n",
      "Epoch:7/20... Training Step:1951... Training loss:2.3337... 0.2031 sec/batch\n",
      "Epoch:7/20... Training Step:1952... Training loss:2.3225... 0.2068 sec/batch\n",
      "Epoch:7/20... Training Step:1953... Training loss:2.3432... 0.1981 sec/batch\n",
      "Epoch:7/20... Training Step:1954... Training loss:2.3865... 0.2123 sec/batch\n",
      "Epoch:7/20... Training Step:1955... Training loss:2.3234... 0.2070 sec/batch\n",
      "Epoch:7/20... Training Step:1956... Training loss:2.3088... 0.2102 sec/batch\n",
      "Epoch:7/20... Training Step:1957... Training loss:2.3272... 0.1923 sec/batch\n",
      "Epoch:7/20... Training Step:1958... Training loss:2.3187... 0.1924 sec/batch\n",
      "Epoch:7/20... Training Step:1959... Training loss:2.3456... 0.1929 sec/batch\n",
      "Epoch:7/20... Training Step:1960... Training loss:2.3601... 0.2089 sec/batch\n",
      "Epoch:7/20... Training Step:1961... Training loss:2.3841... 0.1960 sec/batch\n",
      "Epoch:7/20... Training Step:1962... Training loss:2.3845... 0.2001 sec/batch\n",
      "Epoch:7/20... Training Step:1963... Training loss:2.3581... 0.1940 sec/batch\n",
      "Epoch:7/20... Training Step:1964... Training loss:2.3263... 0.1937 sec/batch\n",
      "Epoch:7/20... Training Step:1965... Training loss:2.3732... 0.2017 sec/batch\n",
      "Epoch:7/20... Training Step:1966... Training loss:2.3373... 0.2019 sec/batch\n",
      "Epoch:7/20... Training Step:1967... Training loss:2.3525... 0.2020 sec/batch\n",
      "Epoch:7/20... Training Step:1968... Training loss:2.3533... 0.2027 sec/batch\n",
      "Epoch:7/20... Training Step:1969... Training loss:2.3563... 0.2103 sec/batch\n",
      "Epoch:7/20... Training Step:1970... Training loss:2.3335... 0.1942 sec/batch\n",
      "Epoch:7/20... Training Step:1971... Training loss:2.3303... 0.2130 sec/batch\n",
      "Epoch:7/20... Training Step:1972... Training loss:2.3642... 0.1988 sec/batch\n",
      "Epoch:7/20... Training Step:1973... Training loss:2.3411... 0.2161 sec/batch\n",
      "Epoch:7/20... Training Step:1974... Training loss:2.3515... 0.2027 sec/batch\n",
      "Epoch:7/20... Training Step:1975... Training loss:2.3636... 0.2013 sec/batch\n",
      "Epoch:7/20... Training Step:1976... Training loss:2.3648... 0.2058 sec/batch\n",
      "Epoch:7/20... Training Step:1977... Training loss:2.3462... 0.2077 sec/batch\n",
      "Epoch:7/20... Training Step:1978... Training loss:2.3543... 0.1911 sec/batch\n",
      "Epoch:7/20... Training Step:1979... Training loss:2.3155... 0.2115 sec/batch\n",
      "Epoch:7/20... Training Step:1980... Training loss:2.3340... 0.1916 sec/batch\n",
      "Epoch:7/20... Training Step:1981... Training loss:2.3442... 0.1935 sec/batch\n",
      "Epoch:7/20... Training Step:1982... Training loss:2.3794... 0.2092 sec/batch\n",
      "Epoch:7/20... Training Step:1983... Training loss:2.3910... 0.1914 sec/batch\n",
      "Epoch:7/20... Training Step:1984... Training loss:2.3503... 0.1940 sec/batch\n",
      "Epoch:7/20... Training Step:1985... Training loss:2.3680... 0.2037 sec/batch\n",
      "Epoch:7/20... Training Step:1986... Training loss:2.3650... 0.2036 sec/batch\n",
      "Epoch:7/20... Training Step:1987... Training loss:2.3478... 0.1945 sec/batch\n",
      "Epoch:7/20... Training Step:1988... Training loss:2.3467... 0.1970 sec/batch\n",
      "Epoch:7/20... Training Step:1989... Training loss:2.3290... 0.2107 sec/batch\n",
      "Epoch:7/20... Training Step:1990... Training loss:2.3551... 0.1910 sec/batch\n",
      "Epoch:7/20... Training Step:1991... Training loss:2.3552... 0.1926 sec/batch\n",
      "Epoch:7/20... Training Step:1992... Training loss:2.3779... 0.1930 sec/batch\n",
      "Epoch:7/20... Training Step:1993... Training loss:2.3244... 0.2080 sec/batch\n",
      "Epoch:7/20... Training Step:1994... Training loss:2.3526... 0.2045 sec/batch\n",
      "Epoch:7/20... Training Step:1995... Training loss:2.3833... 0.2113 sec/batch\n",
      "Epoch:7/20... Training Step:1996... Training loss:2.3631... 0.2136 sec/batch\n",
      "Epoch:7/20... Training Step:1997... Training loss:2.3388... 0.1916 sec/batch\n",
      "Epoch:7/20... Training Step:1998... Training loss:2.3753... 0.2085 sec/batch\n",
      "Epoch:7/20... Training Step:1999... Training loss:2.3686... 0.2152 sec/batch\n",
      "Epoch:7/20... Training Step:2000... Training loss:2.3720... 0.2040 sec/batch\n",
      "Epoch:7/20... Training Step:2001... Training loss:2.3704... 0.1929 sec/batch\n",
      "Epoch:7/20... Training Step:2002... Training loss:2.3497... 0.1939 sec/batch\n",
      "Epoch:7/20... Training Step:2003... Training loss:2.3236... 0.1968 sec/batch\n",
      "Epoch:7/20... Training Step:2004... Training loss:2.3021... 0.2085 sec/batch\n",
      "Epoch:7/20... Training Step:2005... Training loss:2.3164... 0.1928 sec/batch\n",
      "Epoch:7/20... Training Step:2006... Training loss:2.3331... 0.1982 sec/batch\n",
      "Epoch:7/20... Training Step:2007... Training loss:2.3748... 0.2074 sec/batch\n",
      "Epoch:7/20... Training Step:2008... Training loss:2.3405... 0.1932 sec/batch\n",
      "Epoch:7/20... Training Step:2009... Training loss:2.3296... 0.2157 sec/batch\n",
      "Epoch:7/20... Training Step:2010... Training loss:2.3366... 0.1912 sec/batch\n",
      "Epoch:7/20... Training Step:2011... Training loss:2.3346... 0.1923 sec/batch\n",
      "Epoch:7/20... Training Step:2012... Training loss:2.3696... 0.2076 sec/batch\n",
      "Epoch:7/20... Training Step:2013... Training loss:2.3744... 0.1929 sec/batch\n",
      "Epoch:7/20... Training Step:2014... Training loss:2.3196... 0.2131 sec/batch\n",
      "Epoch:7/20... Training Step:2015... Training loss:2.3655... 0.1923 sec/batch\n",
      "Epoch:7/20... Training Step:2016... Training loss:2.3662... 0.1967 sec/batch\n",
      "Epoch:7/20... Training Step:2017... Training loss:2.3586... 0.2070 sec/batch\n",
      "Epoch:7/20... Training Step:2018... Training loss:2.3742... 0.1925 sec/batch\n",
      "Epoch:7/20... Training Step:2019... Training loss:2.3615... 0.2163 sec/batch\n",
      "Epoch:7/20... Training Step:2020... Training loss:2.3722... 0.1924 sec/batch\n",
      "Epoch:7/20... Training Step:2021... Training loss:2.3539... 0.1924 sec/batch\n",
      "Epoch:7/20... Training Step:2022... Training loss:2.3573... 0.1927 sec/batch\n",
      "Epoch:7/20... Training Step:2023... Training loss:2.3706... 0.1984 sec/batch\n",
      "Epoch:7/20... Training Step:2024... Training loss:2.3828... 0.2172 sec/batch\n",
      "Epoch:7/20... Training Step:2025... Training loss:2.3989... 0.2097 sec/batch\n",
      "Epoch:7/20... Training Step:2026... Training loss:2.3770... 0.2037 sec/batch\n",
      "Epoch:7/20... Training Step:2027... Training loss:2.3561... 0.1919 sec/batch\n",
      "Epoch:7/20... Training Step:2028... Training loss:2.3483... 0.2048 sec/batch\n",
      "Epoch:7/20... Training Step:2029... Training loss:2.3438... 0.2152 sec/batch\n",
      "Epoch:7/20... Training Step:2030... Training loss:2.3431... 0.2039 sec/batch\n",
      "Epoch:8/20... Training Step:2031... Training loss:2.5005... 0.1928 sec/batch\n",
      "Epoch:8/20... Training Step:2032... Training loss:2.3506... 0.2448 sec/batch\n",
      "Epoch:8/20... Training Step:2033... Training loss:2.3552... 0.2093 sec/batch\n",
      "Epoch:8/20... Training Step:2034... Training loss:2.3718... 0.1981 sec/batch\n",
      "Epoch:8/20... Training Step:2035... Training loss:2.3486... 0.2254 sec/batch\n",
      "Epoch:8/20... Training Step:2036... Training loss:2.3589... 0.1971 sec/batch\n",
      "Epoch:8/20... Training Step:2037... Training loss:2.3450... 0.2054 sec/batch\n",
      "Epoch:8/20... Training Step:2038... Training loss:2.3502... 0.1905 sec/batch\n",
      "Epoch:8/20... Training Step:2039... Training loss:2.3491... 0.1942 sec/batch\n",
      "Epoch:8/20... Training Step:2040... Training loss:2.3352... 0.1947 sec/batch\n",
      "Epoch:8/20... Training Step:2041... Training loss:2.3782... 0.2006 sec/batch\n",
      "Epoch:8/20... Training Step:2042... Training loss:2.3727... 0.2022 sec/batch\n",
      "Epoch:8/20... Training Step:2043... Training loss:2.3720... 0.2023 sec/batch\n",
      "Epoch:8/20... Training Step:2044... Training loss:2.3644... 0.2030 sec/batch\n",
      "Epoch:8/20... Training Step:2045... Training loss:2.3422... 0.2123 sec/batch\n",
      "Epoch:8/20... Training Step:2046... Training loss:2.3614... 0.2219 sec/batch\n",
      "Epoch:8/20... Training Step:2047... Training loss:2.3632... 0.2080 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8/20... Training Step:2048... Training loss:2.3819... 0.2057 sec/batch\n",
      "Epoch:8/20... Training Step:2049... Training loss:2.3683... 0.1971 sec/batch\n",
      "Epoch:8/20... Training Step:2050... Training loss:2.3711... 0.2096 sec/batch\n",
      "Epoch:8/20... Training Step:2051... Training loss:2.3721... 0.2012 sec/batch\n",
      "Epoch:8/20... Training Step:2052... Training loss:2.3605... 0.1963 sec/batch\n",
      "Epoch:8/20... Training Step:2053... Training loss:2.3446... 0.1913 sec/batch\n",
      "Epoch:8/20... Training Step:2054... Training loss:2.3881... 0.2049 sec/batch\n",
      "Epoch:8/20... Training Step:2055... Training loss:2.3379... 0.1977 sec/batch\n",
      "Epoch:8/20... Training Step:2056... Training loss:2.3602... 0.2085 sec/batch\n",
      "Epoch:8/20... Training Step:2057... Training loss:2.3718... 0.1932 sec/batch\n",
      "Epoch:8/20... Training Step:2058... Training loss:2.3630... 0.1933 sec/batch\n",
      "Epoch:8/20... Training Step:2059... Training loss:2.3769... 0.1916 sec/batch\n",
      "Epoch:8/20... Training Step:2060... Training loss:2.3702... 0.1934 sec/batch\n",
      "Epoch:8/20... Training Step:2061... Training loss:2.3613... 0.2029 sec/batch\n",
      "Epoch:8/20... Training Step:2062... Training loss:2.3500... 0.2056 sec/batch\n",
      "Epoch:8/20... Training Step:2063... Training loss:2.3485... 0.2193 sec/batch\n",
      "Epoch:8/20... Training Step:2064... Training loss:2.3591... 0.2274 sec/batch\n",
      "Epoch:8/20... Training Step:2065... Training loss:2.3580... 0.1929 sec/batch\n",
      "Epoch:8/20... Training Step:2066... Training loss:2.3375... 0.1940 sec/batch\n",
      "Epoch:8/20... Training Step:2067... Training loss:2.3534... 0.1931 sec/batch\n",
      "Epoch:8/20... Training Step:2068... Training loss:2.3601... 0.1979 sec/batch\n",
      "Epoch:8/20... Training Step:2069... Training loss:2.3354... 0.2045 sec/batch\n",
      "Epoch:8/20... Training Step:2070... Training loss:2.3041... 0.1909 sec/batch\n",
      "Epoch:8/20... Training Step:2071... Training loss:2.3319... 0.2029 sec/batch\n",
      "Epoch:8/20... Training Step:2072... Training loss:2.3539... 0.2012 sec/batch\n",
      "Epoch:8/20... Training Step:2073... Training loss:2.3716... 0.2137 sec/batch\n",
      "Epoch:8/20... Training Step:2074... Training loss:2.3456... 0.1905 sec/batch\n",
      "Epoch:8/20... Training Step:2075... Training loss:2.3739... 0.1926 sec/batch\n",
      "Epoch:8/20... Training Step:2076... Training loss:2.3735... 0.1973 sec/batch\n",
      "Epoch:8/20... Training Step:2077... Training loss:2.3698... 0.2017 sec/batch\n",
      "Epoch:8/20... Training Step:2078... Training loss:2.3480... 0.1950 sec/batch\n",
      "Epoch:8/20... Training Step:2079... Training loss:2.3117... 0.2030 sec/batch\n",
      "Epoch:8/20... Training Step:2080... Training loss:2.3737... 0.1986 sec/batch\n",
      "Epoch:8/20... Training Step:2081... Training loss:2.3279... 0.2100 sec/batch\n",
      "Epoch:8/20... Training Step:2082... Training loss:2.3488... 0.2040 sec/batch\n",
      "Epoch:8/20... Training Step:2083... Training loss:2.3468... 0.2101 sec/batch\n",
      "Epoch:8/20... Training Step:2084... Training loss:2.2968... 0.1977 sec/batch\n",
      "Epoch:8/20... Training Step:2085... Training loss:2.3362... 0.1912 sec/batch\n",
      "Epoch:8/20... Training Step:2086... Training loss:2.3457... 0.1915 sec/batch\n",
      "Epoch:8/20... Training Step:2087... Training loss:2.3216... 0.2014 sec/batch\n",
      "Epoch:8/20... Training Step:2088... Training loss:2.3333... 0.2031 sec/batch\n",
      "Epoch:8/20... Training Step:2089... Training loss:2.3631... 0.2036 sec/batch\n",
      "Epoch:8/20... Training Step:2090... Training loss:2.3539... 0.2002 sec/batch\n",
      "Epoch:8/20... Training Step:2091... Training loss:2.3289... 0.2038 sec/batch\n",
      "Epoch:8/20... Training Step:2092... Training loss:2.3543... 0.2023 sec/batch\n",
      "Epoch:8/20... Training Step:2093... Training loss:2.3250... 0.2019 sec/batch\n",
      "Epoch:8/20... Training Step:2094... Training loss:2.3470... 0.2159 sec/batch\n",
      "Epoch:8/20... Training Step:2095... Training loss:2.3322... 0.1920 sec/batch\n",
      "Epoch:8/20... Training Step:2096... Training loss:2.2886... 0.2069 sec/batch\n",
      "Epoch:8/20... Training Step:2097... Training loss:2.3009... 0.1911 sec/batch\n",
      "Epoch:8/20... Training Step:2098... Training loss:2.3185... 0.1939 sec/batch\n",
      "Epoch:8/20... Training Step:2099... Training loss:2.3453... 0.1940 sec/batch\n",
      "Epoch:8/20... Training Step:2100... Training loss:2.2867... 0.1930 sec/batch\n",
      "Epoch:8/20... Training Step:2101... Training loss:2.2859... 0.1947 sec/batch\n",
      "Epoch:8/20... Training Step:2102... Training loss:2.3154... 0.1994 sec/batch\n",
      "Epoch:8/20... Training Step:2103... Training loss:2.3366... 0.1971 sec/batch\n",
      "Epoch:8/20... Training Step:2104... Training loss:2.3022... 0.2014 sec/batch\n",
      "Epoch:8/20... Training Step:2105... Training loss:2.2701... 0.1979 sec/batch\n",
      "Epoch:8/20... Training Step:2106... Training loss:2.2981... 0.1923 sec/batch\n",
      "Epoch:8/20... Training Step:2107... Training loss:2.2937... 0.1974 sec/batch\n",
      "Epoch:8/20... Training Step:2108... Training loss:2.3036... 0.1959 sec/batch\n",
      "Epoch:8/20... Training Step:2109... Training loss:2.3199... 0.1993 sec/batch\n",
      "Epoch:8/20... Training Step:2110... Training loss:2.2988... 0.1926 sec/batch\n",
      "Epoch:8/20... Training Step:2111... Training loss:2.2915... 0.2117 sec/batch\n",
      "Epoch:8/20... Training Step:2112... Training loss:2.2877... 0.1960 sec/batch\n",
      "Epoch:8/20... Training Step:2113... Training loss:2.3238... 0.1932 sec/batch\n",
      "Epoch:8/20... Training Step:2114... Training loss:2.3587... 0.2144 sec/batch\n",
      "Epoch:8/20... Training Step:2115... Training loss:2.3380... 0.1992 sec/batch\n",
      "Epoch:8/20... Training Step:2116... Training loss:2.2943... 0.2037 sec/batch\n",
      "Epoch:8/20... Training Step:2117... Training loss:2.3253... 0.2082 sec/batch\n",
      "Epoch:8/20... Training Step:2118... Training loss:2.3316... 0.2065 sec/batch\n",
      "Epoch:8/20... Training Step:2119... Training loss:2.3213... 0.2116 sec/batch\n",
      "Epoch:8/20... Training Step:2120... Training loss:2.3305... 0.2015 sec/batch\n",
      "Epoch:8/20... Training Step:2121... Training loss:2.3243... 0.1997 sec/batch\n",
      "Epoch:8/20... Training Step:2122... Training loss:2.3470... 0.2074 sec/batch\n",
      "Epoch:8/20... Training Step:2123... Training loss:2.3613... 0.2033 sec/batch\n",
      "Epoch:8/20... Training Step:2124... Training loss:2.3269... 0.1992 sec/batch\n",
      "Epoch:8/20... Training Step:2125... Training loss:2.2890... 0.1914 sec/batch\n",
      "Epoch:8/20... Training Step:2126... Training loss:2.3228... 0.1939 sec/batch\n",
      "Epoch:8/20... Training Step:2127... Training loss:2.3274... 0.1907 sec/batch\n",
      "Epoch:8/20... Training Step:2128... Training loss:2.3505... 0.1923 sec/batch\n",
      "Epoch:8/20... Training Step:2129... Training loss:2.2978... 0.2073 sec/batch\n",
      "Epoch:8/20... Training Step:2130... Training loss:2.3346... 0.1916 sec/batch\n",
      "Epoch:8/20... Training Step:2131... Training loss:2.3271... 0.2013 sec/batch\n",
      "Epoch:8/20... Training Step:2132... Training loss:2.3292... 0.1929 sec/batch\n",
      "Epoch:8/20... Training Step:2133... Training loss:2.3436... 0.2077 sec/batch\n",
      "Epoch:8/20... Training Step:2134... Training loss:2.3258... 0.2015 sec/batch\n",
      "Epoch:8/20... Training Step:2135... Training loss:2.3706... 0.1917 sec/batch\n",
      "Epoch:8/20... Training Step:2136... Training loss:2.3476... 0.2043 sec/batch\n",
      "Epoch:8/20... Training Step:2137... Training loss:2.3606... 0.2078 sec/batch\n",
      "Epoch:8/20... Training Step:2138... Training loss:2.3304... 0.2108 sec/batch\n",
      "Epoch:8/20... Training Step:2139... Training loss:2.3382... 0.1916 sec/batch\n",
      "Epoch:8/20... Training Step:2140... Training loss:2.3304... 0.1934 sec/batch\n",
      "Epoch:8/20... Training Step:2141... Training loss:2.3240... 0.1964 sec/batch\n",
      "Epoch:8/20... Training Step:2142... Training loss:2.2950... 0.2139 sec/batch\n",
      "Epoch:8/20... Training Step:2143... Training loss:2.2796... 0.2117 sec/batch\n",
      "Epoch:8/20... Training Step:2144... Training loss:2.3393... 0.2016 sec/batch\n",
      "Epoch:8/20... Training Step:2145... Training loss:2.3147... 0.1912 sec/batch\n",
      "Epoch:8/20... Training Step:2146... Training loss:2.3516... 0.2016 sec/batch\n",
      "Epoch:8/20... Training Step:2147... Training loss:2.3450... 0.2041 sec/batch\n",
      "Epoch:8/20... Training Step:2148... Training loss:2.3186... 0.1929 sec/batch\n",
      "Epoch:8/20... Training Step:2149... Training loss:2.3369... 0.1919 sec/batch\n",
      "Epoch:8/20... Training Step:2150... Training loss:2.3431... 0.2040 sec/batch\n",
      "Epoch:8/20... Training Step:2151... Training loss:2.3473... 0.1959 sec/batch\n",
      "Epoch:8/20... Training Step:2152... Training loss:2.3574... 0.1925 sec/batch\n",
      "Epoch:8/20... Training Step:2153... Training loss:2.3236... 0.1912 sec/batch\n",
      "Epoch:8/20... Training Step:2154... Training loss:2.3522... 0.1914 sec/batch\n",
      "Epoch:8/20... Training Step:2155... Training loss:2.3244... 0.1939 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8/20... Training Step:2156... Training loss:2.3424... 0.2102 sec/batch\n",
      "Epoch:8/20... Training Step:2157... Training loss:2.3577... 0.2037 sec/batch\n",
      "Epoch:8/20... Training Step:2158... Training loss:2.3657... 0.2073 sec/batch\n",
      "Epoch:8/20... Training Step:2159... Training loss:2.3783... 0.2040 sec/batch\n",
      "Epoch:8/20... Training Step:2160... Training loss:2.4002... 0.1949 sec/batch\n",
      "Epoch:8/20... Training Step:2161... Training loss:2.3595... 0.1938 sec/batch\n",
      "Epoch:8/20... Training Step:2162... Training loss:2.3572... 0.2038 sec/batch\n",
      "Epoch:8/20... Training Step:2163... Training loss:2.3733... 0.2049 sec/batch\n",
      "Epoch:8/20... Training Step:2164... Training loss:2.3777... 0.1936 sec/batch\n",
      "Epoch:8/20... Training Step:2165... Training loss:2.3500... 0.1914 sec/batch\n",
      "Epoch:8/20... Training Step:2166... Training loss:2.3337... 0.1921 sec/batch\n",
      "Epoch:8/20... Training Step:2167... Training loss:2.3307... 0.1919 sec/batch\n",
      "Epoch:8/20... Training Step:2168... Training loss:2.3432... 0.2070 sec/batch\n",
      "Epoch:8/20... Training Step:2169... Training loss:2.3396... 0.1927 sec/batch\n",
      "Epoch:8/20... Training Step:2170... Training loss:2.3306... 0.1947 sec/batch\n",
      "Epoch:8/20... Training Step:2171... Training loss:2.3252... 0.1973 sec/batch\n",
      "Epoch:8/20... Training Step:2172... Training loss:2.3915... 0.1972 sec/batch\n",
      "Epoch:8/20... Training Step:2173... Training loss:2.3419... 0.2044 sec/batch\n",
      "Epoch:8/20... Training Step:2174... Training loss:2.3163... 0.2109 sec/batch\n",
      "Epoch:8/20... Training Step:2175... Training loss:2.3372... 0.2071 sec/batch\n",
      "Epoch:8/20... Training Step:2176... Training loss:2.3393... 0.2090 sec/batch\n",
      "Epoch:8/20... Training Step:2177... Training loss:2.3087... 0.1912 sec/batch\n",
      "Epoch:8/20... Training Step:2178... Training loss:2.3140... 0.2067 sec/batch\n",
      "Epoch:8/20... Training Step:2179... Training loss:2.3416... 0.2094 sec/batch\n",
      "Epoch:8/20... Training Step:2180... Training loss:2.3638... 0.2043 sec/batch\n",
      "Epoch:8/20... Training Step:2181... Training loss:2.3119... 0.2066 sec/batch\n",
      "Epoch:8/20... Training Step:2182... Training loss:2.3192... 0.1999 sec/batch\n",
      "Epoch:8/20... Training Step:2183... Training loss:2.3411... 0.2103 sec/batch\n",
      "Epoch:8/20... Training Step:2184... Training loss:2.3178... 0.1980 sec/batch\n",
      "Epoch:8/20... Training Step:2185... Training loss:2.3223... 0.2221 sec/batch\n",
      "Epoch:8/20... Training Step:2186... Training loss:2.3436... 0.1972 sec/batch\n",
      "Epoch:8/20... Training Step:2187... Training loss:2.3364... 0.1937 sec/batch\n",
      "Epoch:8/20... Training Step:2188... Training loss:2.3229... 0.1915 sec/batch\n",
      "Epoch:8/20... Training Step:2189... Training loss:2.3119... 0.1946 sec/batch\n",
      "Epoch:8/20... Training Step:2190... Training loss:2.3094... 0.1916 sec/batch\n",
      "Epoch:8/20... Training Step:2191... Training loss:2.3170... 0.1935 sec/batch\n",
      "Epoch:8/20... Training Step:2192... Training loss:2.3165... 0.2046 sec/batch\n",
      "Epoch:8/20... Training Step:2193... Training loss:2.3024... 0.1993 sec/batch\n",
      "Epoch:8/20... Training Step:2194... Training loss:2.3479... 0.2032 sec/batch\n",
      "Epoch:8/20... Training Step:2195... Training loss:2.3396... 0.2030 sec/batch\n",
      "Epoch:8/20... Training Step:2196... Training loss:2.3720... 0.2102 sec/batch\n",
      "Epoch:8/20... Training Step:2197... Training loss:2.3652... 0.1982 sec/batch\n",
      "Epoch:8/20... Training Step:2198... Training loss:2.3711... 0.1980 sec/batch\n",
      "Epoch:8/20... Training Step:2199... Training loss:2.3209... 0.2055 sec/batch\n",
      "Epoch:8/20... Training Step:2200... Training loss:2.3086... 0.2082 sec/batch\n",
      "Epoch:8/20... Training Step:2201... Training loss:2.3275... 0.1910 sec/batch\n",
      "Epoch:8/20... Training Step:2202... Training loss:2.3000... 0.1923 sec/batch\n",
      "Epoch:8/20... Training Step:2203... Training loss:2.3043... 0.1984 sec/batch\n",
      "Epoch:8/20... Training Step:2204... Training loss:2.3289... 0.2179 sec/batch\n",
      "Epoch:8/20... Training Step:2205... Training loss:2.3294... 0.1931 sec/batch\n",
      "Epoch:8/20... Training Step:2206... Training loss:2.3274... 0.2055 sec/batch\n",
      "Epoch:8/20... Training Step:2207... Training loss:2.3078... 0.2042 sec/batch\n",
      "Epoch:8/20... Training Step:2208... Training loss:2.2978... 0.2105 sec/batch\n",
      "Epoch:8/20... Training Step:2209... Training loss:2.2958... 0.2072 sec/batch\n",
      "Epoch:8/20... Training Step:2210... Training loss:2.3182... 0.2014 sec/batch\n",
      "Epoch:8/20... Training Step:2211... Training loss:2.3087... 0.1972 sec/batch\n",
      "Epoch:8/20... Training Step:2212... Training loss:2.3296... 0.1923 sec/batch\n",
      "Epoch:8/20... Training Step:2213... Training loss:2.3272... 0.1925 sec/batch\n",
      "Epoch:8/20... Training Step:2214... Training loss:2.3124... 0.1961 sec/batch\n",
      "Epoch:8/20... Training Step:2215... Training loss:2.3292... 0.1917 sec/batch\n",
      "Epoch:8/20... Training Step:2216... Training loss:2.3057... 0.1926 sec/batch\n",
      "Epoch:8/20... Training Step:2217... Training loss:2.3445... 0.1943 sec/batch\n",
      "Epoch:8/20... Training Step:2218... Training loss:2.3462... 0.1979 sec/batch\n",
      "Epoch:8/20... Training Step:2219... Training loss:2.3355... 0.1981 sec/batch\n",
      "Epoch:8/20... Training Step:2220... Training loss:2.3280... 0.1934 sec/batch\n",
      "Epoch:8/20... Training Step:2221... Training loss:2.3023... 0.1949 sec/batch\n",
      "Epoch:8/20... Training Step:2222... Training loss:2.3370... 0.2141 sec/batch\n",
      "Epoch:8/20... Training Step:2223... Training loss:2.3619... 0.1931 sec/batch\n",
      "Epoch:8/20... Training Step:2224... Training loss:2.3262... 0.1954 sec/batch\n",
      "Epoch:8/20... Training Step:2225... Training loss:2.3370... 0.1909 sec/batch\n",
      "Epoch:8/20... Training Step:2226... Training loss:2.3306... 0.1948 sec/batch\n",
      "Epoch:8/20... Training Step:2227... Training loss:2.3077... 0.2028 sec/batch\n",
      "Epoch:8/20... Training Step:2228... Training loss:2.3322... 0.1945 sec/batch\n",
      "Epoch:8/20... Training Step:2229... Training loss:2.3192... 0.1932 sec/batch\n",
      "Epoch:8/20... Training Step:2230... Training loss:2.3256... 0.1936 sec/batch\n",
      "Epoch:8/20... Training Step:2231... Training loss:2.3392... 0.1925 sec/batch\n",
      "Epoch:8/20... Training Step:2232... Training loss:2.3264... 0.2019 sec/batch\n",
      "Epoch:8/20... Training Step:2233... Training loss:2.3442... 0.2035 sec/batch\n",
      "Epoch:8/20... Training Step:2234... Training loss:2.3087... 0.1913 sec/batch\n",
      "Epoch:8/20... Training Step:2235... Training loss:2.2732... 0.1931 sec/batch\n",
      "Epoch:8/20... Training Step:2236... Training loss:2.3195... 0.1990 sec/batch\n",
      "Epoch:8/20... Training Step:2237... Training loss:2.3093... 0.2068 sec/batch\n",
      "Epoch:8/20... Training Step:2238... Training loss:2.3036... 0.2015 sec/batch\n",
      "Epoch:8/20... Training Step:2239... Training loss:2.2854... 0.2028 sec/batch\n",
      "Epoch:8/20... Training Step:2240... Training loss:2.2968... 0.1918 sec/batch\n",
      "Epoch:8/20... Training Step:2241... Training loss:2.2969... 0.2028 sec/batch\n",
      "Epoch:8/20... Training Step:2242... Training loss:2.2709... 0.2114 sec/batch\n",
      "Epoch:8/20... Training Step:2243... Training loss:2.2941... 0.1973 sec/batch\n",
      "Epoch:8/20... Training Step:2244... Training loss:2.3494... 0.2130 sec/batch\n",
      "Epoch:8/20... Training Step:2245... Training loss:2.2754... 0.1948 sec/batch\n",
      "Epoch:8/20... Training Step:2246... Training loss:2.2683... 0.1970 sec/batch\n",
      "Epoch:8/20... Training Step:2247... Training loss:2.2816... 0.1935 sec/batch\n",
      "Epoch:8/20... Training Step:2248... Training loss:2.2726... 0.1949 sec/batch\n",
      "Epoch:8/20... Training Step:2249... Training loss:2.3023... 0.1972 sec/batch\n",
      "Epoch:8/20... Training Step:2250... Training loss:2.3034... 0.1970 sec/batch\n",
      "Epoch:8/20... Training Step:2251... Training loss:2.3238... 0.1955 sec/batch\n",
      "Epoch:8/20... Training Step:2252... Training loss:2.3448... 0.2033 sec/batch\n",
      "Epoch:8/20... Training Step:2253... Training loss:2.3109... 0.1961 sec/batch\n",
      "Epoch:8/20... Training Step:2254... Training loss:2.2860... 0.1935 sec/batch\n",
      "Epoch:8/20... Training Step:2255... Training loss:2.3314... 0.1954 sec/batch\n",
      "Epoch:8/20... Training Step:2256... Training loss:2.2915... 0.2000 sec/batch\n",
      "Epoch:8/20... Training Step:2257... Training loss:2.3074... 0.1915 sec/batch\n",
      "Epoch:8/20... Training Step:2258... Training loss:2.3081... 0.1915 sec/batch\n",
      "Epoch:8/20... Training Step:2259... Training loss:2.3175... 0.1931 sec/batch\n",
      "Epoch:8/20... Training Step:2260... Training loss:2.2909... 0.2052 sec/batch\n",
      "Epoch:8/20... Training Step:2261... Training loss:2.2716... 0.2016 sec/batch\n",
      "Epoch:8/20... Training Step:2262... Training loss:2.3174... 0.2014 sec/batch\n",
      "Epoch:8/20... Training Step:2263... Training loss:2.2949... 0.1940 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8/20... Training Step:2264... Training loss:2.3109... 0.1916 sec/batch\n",
      "Epoch:8/20... Training Step:2265... Training loss:2.3186... 0.1932 sec/batch\n",
      "Epoch:8/20... Training Step:2266... Training loss:2.3171... 0.1956 sec/batch\n",
      "Epoch:8/20... Training Step:2267... Training loss:2.2864... 0.1959 sec/batch\n",
      "Epoch:8/20... Training Step:2268... Training loss:2.3044... 0.1973 sec/batch\n",
      "Epoch:8/20... Training Step:2269... Training loss:2.2704... 0.1923 sec/batch\n",
      "Epoch:8/20... Training Step:2270... Training loss:2.2917... 0.1915 sec/batch\n",
      "Epoch:8/20... Training Step:2271... Training loss:2.2958... 0.1977 sec/batch\n",
      "Epoch:8/20... Training Step:2272... Training loss:2.3392... 0.2025 sec/batch\n",
      "Epoch:8/20... Training Step:2273... Training loss:2.3474... 0.1931 sec/batch\n",
      "Epoch:8/20... Training Step:2274... Training loss:2.2980... 0.2168 sec/batch\n",
      "Epoch:8/20... Training Step:2275... Training loss:2.3226... 0.1960 sec/batch\n",
      "Epoch:8/20... Training Step:2276... Training loss:2.3158... 0.2073 sec/batch\n",
      "Epoch:8/20... Training Step:2277... Training loss:2.3056... 0.1935 sec/batch\n",
      "Epoch:8/20... Training Step:2278... Training loss:2.2978... 0.2067 sec/batch\n",
      "Epoch:8/20... Training Step:2279... Training loss:2.2889... 0.1920 sec/batch\n",
      "Epoch:8/20... Training Step:2280... Training loss:2.3079... 0.2031 sec/batch\n",
      "Epoch:8/20... Training Step:2281... Training loss:2.3069... 0.2039 sec/batch\n",
      "Epoch:8/20... Training Step:2282... Training loss:2.3387... 0.1918 sec/batch\n",
      "Epoch:8/20... Training Step:2283... Training loss:2.2853... 0.2034 sec/batch\n",
      "Epoch:8/20... Training Step:2284... Training loss:2.3023... 0.1932 sec/batch\n",
      "Epoch:8/20... Training Step:2285... Training loss:2.3311... 0.2067 sec/batch\n",
      "Epoch:8/20... Training Step:2286... Training loss:2.3153... 0.2027 sec/batch\n",
      "Epoch:8/20... Training Step:2287... Training loss:2.2895... 0.1928 sec/batch\n",
      "Epoch:8/20... Training Step:2288... Training loss:2.3285... 0.1928 sec/batch\n",
      "Epoch:8/20... Training Step:2289... Training loss:2.3108... 0.2074 sec/batch\n",
      "Epoch:8/20... Training Step:2290... Training loss:2.3240... 0.2017 sec/batch\n",
      "Epoch:8/20... Training Step:2291... Training loss:2.3303... 0.1924 sec/batch\n",
      "Epoch:8/20... Training Step:2292... Training loss:2.3062... 0.1991 sec/batch\n",
      "Epoch:8/20... Training Step:2293... Training loss:2.2870... 0.1915 sec/batch\n",
      "Epoch:8/20... Training Step:2294... Training loss:2.2644... 0.1925 sec/batch\n",
      "Epoch:8/20... Training Step:2295... Training loss:2.2605... 0.1932 sec/batch\n",
      "Epoch:8/20... Training Step:2296... Training loss:2.2770... 0.1921 sec/batch\n",
      "Epoch:8/20... Training Step:2297... Training loss:2.3353... 0.1904 sec/batch\n",
      "Epoch:8/20... Training Step:2298... Training loss:2.2887... 0.1928 sec/batch\n",
      "Epoch:8/20... Training Step:2299... Training loss:2.2883... 0.1915 sec/batch\n",
      "Epoch:8/20... Training Step:2300... Training loss:2.2794... 0.1932 sec/batch\n",
      "Epoch:8/20... Training Step:2301... Training loss:2.2702... 0.1924 sec/batch\n",
      "Epoch:8/20... Training Step:2302... Training loss:2.3219... 0.1922 sec/batch\n",
      "Epoch:8/20... Training Step:2303... Training loss:2.3230... 0.1937 sec/batch\n",
      "Epoch:8/20... Training Step:2304... Training loss:2.2690... 0.2001 sec/batch\n",
      "Epoch:8/20... Training Step:2305... Training loss:2.3167... 0.1966 sec/batch\n",
      "Epoch:8/20... Training Step:2306... Training loss:2.3091... 0.2068 sec/batch\n",
      "Epoch:8/20... Training Step:2307... Training loss:2.3185... 0.2086 sec/batch\n",
      "Epoch:8/20... Training Step:2308... Training loss:2.3303... 0.1984 sec/batch\n",
      "Epoch:8/20... Training Step:2309... Training loss:2.3016... 0.2024 sec/batch\n",
      "Epoch:8/20... Training Step:2310... Training loss:2.3169... 0.1923 sec/batch\n",
      "Epoch:8/20... Training Step:2311... Training loss:2.2984... 0.2004 sec/batch\n",
      "Epoch:8/20... Training Step:2312... Training loss:2.3107... 0.2054 sec/batch\n",
      "Epoch:8/20... Training Step:2313... Training loss:2.3121... 0.1936 sec/batch\n",
      "Epoch:8/20... Training Step:2314... Training loss:2.3388... 0.1959 sec/batch\n",
      "Epoch:8/20... Training Step:2315... Training loss:2.3527... 0.1925 sec/batch\n",
      "Epoch:8/20... Training Step:2316... Training loss:2.3334... 0.2145 sec/batch\n",
      "Epoch:8/20... Training Step:2317... Training loss:2.3028... 0.1942 sec/batch\n",
      "Epoch:8/20... Training Step:2318... Training loss:2.3025... 0.1924 sec/batch\n",
      "Epoch:8/20... Training Step:2319... Training loss:2.2974... 0.1916 sec/batch\n",
      "Epoch:8/20... Training Step:2320... Training loss:2.2897... 0.2236 sec/batch\n",
      "Epoch:9/20... Training Step:2321... Training loss:2.4541... 0.1919 sec/batch\n",
      "Epoch:9/20... Training Step:2322... Training loss:2.3025... 0.1944 sec/batch\n",
      "Epoch:9/20... Training Step:2323... Training loss:2.3195... 0.1932 sec/batch\n",
      "Epoch:9/20... Training Step:2324... Training loss:2.3363... 0.1966 sec/batch\n",
      "Epoch:9/20... Training Step:2325... Training loss:2.3081... 0.1906 sec/batch\n",
      "Epoch:9/20... Training Step:2326... Training loss:2.3014... 0.1909 sec/batch\n",
      "Epoch:9/20... Training Step:2327... Training loss:2.3004... 0.2120 sec/batch\n",
      "Epoch:9/20... Training Step:2328... Training loss:2.3117... 0.2121 sec/batch\n",
      "Epoch:9/20... Training Step:2329... Training loss:2.3007... 0.1920 sec/batch\n",
      "Epoch:9/20... Training Step:2330... Training loss:2.2908... 0.2104 sec/batch\n",
      "Epoch:9/20... Training Step:2331... Training loss:2.3402... 0.1919 sec/batch\n",
      "Epoch:9/20... Training Step:2332... Training loss:2.3326... 0.2093 sec/batch\n",
      "Epoch:9/20... Training Step:2333... Training loss:2.3301... 0.1916 sec/batch\n",
      "Epoch:9/20... Training Step:2334... Training loss:2.3196... 0.1978 sec/batch\n",
      "Epoch:9/20... Training Step:2335... Training loss:2.2916... 0.1921 sec/batch\n",
      "Epoch:9/20... Training Step:2336... Training loss:2.3184... 0.1944 sec/batch\n",
      "Epoch:9/20... Training Step:2337... Training loss:2.3222... 0.2026 sec/batch\n",
      "Epoch:9/20... Training Step:2338... Training loss:2.3372... 0.2007 sec/batch\n",
      "Epoch:9/20... Training Step:2339... Training loss:2.3276... 0.1959 sec/batch\n",
      "Epoch:9/20... Training Step:2340... Training loss:2.3288... 0.2090 sec/batch\n",
      "Epoch:9/20... Training Step:2341... Training loss:2.3250... 0.2019 sec/batch\n",
      "Epoch:9/20... Training Step:2342... Training loss:2.3073... 0.1923 sec/batch\n",
      "Epoch:9/20... Training Step:2343... Training loss:2.3024... 0.1924 sec/batch\n",
      "Epoch:9/20... Training Step:2344... Training loss:2.3436... 0.1987 sec/batch\n",
      "Epoch:9/20... Training Step:2345... Training loss:2.2888... 0.2125 sec/batch\n",
      "Epoch:9/20... Training Step:2346... Training loss:2.3053... 0.2003 sec/batch\n",
      "Epoch:9/20... Training Step:2347... Training loss:2.3199... 0.1950 sec/batch\n",
      "Epoch:9/20... Training Step:2348... Training loss:2.3212... 0.2225 sec/batch\n",
      "Epoch:9/20... Training Step:2349... Training loss:2.3346... 0.1930 sec/batch\n",
      "Epoch:9/20... Training Step:2350... Training loss:2.3263... 0.2095 sec/batch\n",
      "Epoch:9/20... Training Step:2351... Training loss:2.3099... 0.1992 sec/batch\n",
      "Epoch:9/20... Training Step:2352... Training loss:2.2959... 0.2166 sec/batch\n",
      "Epoch:9/20... Training Step:2353... Training loss:2.2939... 0.1915 sec/batch\n",
      "Epoch:9/20... Training Step:2354... Training loss:2.3190... 0.1924 sec/batch\n",
      "Epoch:9/20... Training Step:2355... Training loss:2.3146... 0.2030 sec/batch\n",
      "Epoch:9/20... Training Step:2356... Training loss:2.2941... 0.2197 sec/batch\n",
      "Epoch:9/20... Training Step:2357... Training loss:2.3131... 0.1928 sec/batch\n",
      "Epoch:9/20... Training Step:2358... Training loss:2.3219... 0.2136 sec/batch\n",
      "Epoch:9/20... Training Step:2359... Training loss:2.2920... 0.1919 sec/batch\n",
      "Epoch:9/20... Training Step:2360... Training loss:2.2608... 0.1941 sec/batch\n",
      "Epoch:9/20... Training Step:2361... Training loss:2.2858... 0.2120 sec/batch\n",
      "Epoch:9/20... Training Step:2362... Training loss:2.3133... 0.1963 sec/batch\n",
      "Epoch:9/20... Training Step:2363... Training loss:2.3271... 0.2145 sec/batch\n",
      "Epoch:9/20... Training Step:2364... Training loss:2.3089... 0.1941 sec/batch\n",
      "Epoch:9/20... Training Step:2365... Training loss:2.3306... 0.1913 sec/batch\n",
      "Epoch:9/20... Training Step:2366... Training loss:2.3340... 0.1915 sec/batch\n",
      "Epoch:9/20... Training Step:2367... Training loss:2.3317... 0.2073 sec/batch\n",
      "Epoch:9/20... Training Step:2368... Training loss:2.3043... 0.1927 sec/batch\n",
      "Epoch:9/20... Training Step:2369... Training loss:2.2651... 0.1965 sec/batch\n",
      "Epoch:9/20... Training Step:2370... Training loss:2.3271... 0.1913 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9/20... Training Step:2371... Training loss:2.2804... 0.2034 sec/batch\n",
      "Epoch:9/20... Training Step:2372... Training loss:2.2993... 0.2100 sec/batch\n",
      "Epoch:9/20... Training Step:2373... Training loss:2.2890... 0.1922 sec/batch\n",
      "Epoch:9/20... Training Step:2374... Training loss:2.2508... 0.1974 sec/batch\n",
      "Epoch:9/20... Training Step:2375... Training loss:2.2804... 0.2065 sec/batch\n",
      "Epoch:9/20... Training Step:2376... Training loss:2.2868... 0.2068 sec/batch\n",
      "Epoch:9/20... Training Step:2377... Training loss:2.2709... 0.1987 sec/batch\n",
      "Epoch:9/20... Training Step:2378... Training loss:2.2904... 0.2081 sec/batch\n",
      "Epoch:9/20... Training Step:2379... Training loss:2.3240... 0.1917 sec/batch\n",
      "Epoch:9/20... Training Step:2380... Training loss:2.3144... 0.1937 sec/batch\n",
      "Epoch:9/20... Training Step:2381... Training loss:2.2946... 0.1921 sec/batch\n",
      "Epoch:9/20... Training Step:2382... Training loss:2.3088... 0.1981 sec/batch\n",
      "Epoch:9/20... Training Step:2383... Training loss:2.2729... 0.2017 sec/batch\n",
      "Epoch:9/20... Training Step:2384... Training loss:2.2999... 0.2144 sec/batch\n",
      "Epoch:9/20... Training Step:2385... Training loss:2.2882... 0.1904 sec/batch\n",
      "Epoch:9/20... Training Step:2386... Training loss:2.2534... 0.2140 sec/batch\n",
      "Epoch:9/20... Training Step:2387... Training loss:2.2523... 0.2112 sec/batch\n",
      "Epoch:9/20... Training Step:2388... Training loss:2.2878... 0.2037 sec/batch\n",
      "Epoch:9/20... Training Step:2389... Training loss:2.2948... 0.1973 sec/batch\n",
      "Epoch:9/20... Training Step:2390... Training loss:2.2440... 0.1926 sec/batch\n",
      "Epoch:9/20... Training Step:2391... Training loss:2.2466... 0.2015 sec/batch\n",
      "Epoch:9/20... Training Step:2392... Training loss:2.2646... 0.2074 sec/batch\n",
      "Epoch:9/20... Training Step:2393... Training loss:2.2917... 0.1995 sec/batch\n",
      "Epoch:9/20... Training Step:2394... Training loss:2.2583... 0.1995 sec/batch\n",
      "Epoch:9/20... Training Step:2395... Training loss:2.2288... 0.1992 sec/batch\n",
      "Epoch:9/20... Training Step:2396... Training loss:2.2562... 0.2152 sec/batch\n",
      "Epoch:9/20... Training Step:2397... Training loss:2.2468... 0.1938 sec/batch\n",
      "Epoch:9/20... Training Step:2398... Training loss:2.2611... 0.1958 sec/batch\n",
      "Epoch:9/20... Training Step:2399... Training loss:2.2777... 0.1924 sec/batch\n",
      "Epoch:9/20... Training Step:2400... Training loss:2.2597... 0.1942 sec/batch\n",
      "Epoch:9/20... Training Step:2401... Training loss:2.2437... 0.1931 sec/batch\n",
      "Epoch:9/20... Training Step:2402... Training loss:2.2447... 0.1974 sec/batch\n",
      "Epoch:9/20... Training Step:2403... Training loss:2.2804... 0.1933 sec/batch\n",
      "Epoch:9/20... Training Step:2404... Training loss:2.3164... 0.2053 sec/batch\n",
      "Epoch:9/20... Training Step:2405... Training loss:2.2955... 0.1965 sec/batch\n",
      "Epoch:9/20... Training Step:2406... Training loss:2.2528... 0.1928 sec/batch\n",
      "Epoch:9/20... Training Step:2407... Training loss:2.2758... 0.2288 sec/batch\n",
      "Epoch:9/20... Training Step:2408... Training loss:2.2797... 0.2085 sec/batch\n",
      "Epoch:9/20... Training Step:2409... Training loss:2.2823... 0.2055 sec/batch\n",
      "Epoch:9/20... Training Step:2410... Training loss:2.2910... 0.2035 sec/batch\n",
      "Epoch:9/20... Training Step:2411... Training loss:2.2951... 0.1985 sec/batch\n",
      "Epoch:9/20... Training Step:2412... Training loss:2.2928... 0.2001 sec/batch\n",
      "Epoch:9/20... Training Step:2413... Training loss:2.3186... 0.1942 sec/batch\n",
      "Epoch:9/20... Training Step:2414... Training loss:2.2796... 0.2114 sec/batch\n",
      "Epoch:9/20... Training Step:2415... Training loss:2.2581... 0.1951 sec/batch\n",
      "Epoch:9/20... Training Step:2416... Training loss:2.2751... 0.2079 sec/batch\n",
      "Epoch:9/20... Training Step:2417... Training loss:2.2822... 0.1998 sec/batch\n",
      "Epoch:9/20... Training Step:2418... Training loss:2.3092... 0.2010 sec/batch\n",
      "Epoch:9/20... Training Step:2419... Training loss:2.2484... 0.1988 sec/batch\n",
      "Epoch:9/20... Training Step:2420... Training loss:2.2910... 0.1930 sec/batch\n",
      "Epoch:9/20... Training Step:2421... Training loss:2.2909... 0.1937 sec/batch\n",
      "Epoch:9/20... Training Step:2422... Training loss:2.2898... 0.2111 sec/batch\n",
      "Epoch:9/20... Training Step:2423... Training loss:2.3055... 0.2057 sec/batch\n",
      "Epoch:9/20... Training Step:2424... Training loss:2.2773... 0.2097 sec/batch\n",
      "Epoch:9/20... Training Step:2425... Training loss:2.3319... 0.1934 sec/batch\n",
      "Epoch:9/20... Training Step:2426... Training loss:2.3111... 0.1967 sec/batch\n",
      "Epoch:9/20... Training Step:2427... Training loss:2.3139... 0.1951 sec/batch\n",
      "Epoch:9/20... Training Step:2428... Training loss:2.2887... 0.1992 sec/batch\n",
      "Epoch:9/20... Training Step:2429... Training loss:2.2943... 0.1916 sec/batch\n",
      "Epoch:9/20... Training Step:2430... Training loss:2.2897... 0.1921 sec/batch\n",
      "Epoch:9/20... Training Step:2431... Training loss:2.2803... 0.2114 sec/batch\n",
      "Epoch:9/20... Training Step:2432... Training loss:2.2442... 0.1920 sec/batch\n",
      "Epoch:9/20... Training Step:2433... Training loss:2.2324... 0.1980 sec/batch\n",
      "Epoch:9/20... Training Step:2434... Training loss:2.2959... 0.1955 sec/batch\n",
      "Epoch:9/20... Training Step:2435... Training loss:2.2777... 0.2051 sec/batch\n",
      "Epoch:9/20... Training Step:2436... Training loss:2.3107... 0.2064 sec/batch\n",
      "Epoch:9/20... Training Step:2437... Training loss:2.2988... 0.2080 sec/batch\n",
      "Epoch:9/20... Training Step:2438... Training loss:2.2759... 0.2021 sec/batch\n",
      "Epoch:9/20... Training Step:2439... Training loss:2.2995... 0.1998 sec/batch\n",
      "Epoch:9/20... Training Step:2440... Training loss:2.2887... 0.2023 sec/batch\n",
      "Epoch:9/20... Training Step:2441... Training loss:2.2953... 0.1933 sec/batch\n",
      "Epoch:9/20... Training Step:2442... Training loss:2.3225... 0.1995 sec/batch\n",
      "Epoch:9/20... Training Step:2443... Training loss:2.2823... 0.2121 sec/batch\n",
      "Epoch:9/20... Training Step:2444... Training loss:2.3081... 0.1975 sec/batch\n",
      "Epoch:9/20... Training Step:2445... Training loss:2.2845... 0.2099 sec/batch\n",
      "Epoch:9/20... Training Step:2446... Training loss:2.2971... 0.2197 sec/batch\n",
      "Epoch:9/20... Training Step:2447... Training loss:2.3198... 0.1921 sec/batch\n",
      "Epoch:9/20... Training Step:2448... Training loss:2.3275... 0.2114 sec/batch\n",
      "Epoch:9/20... Training Step:2449... Training loss:2.3267... 0.2077 sec/batch\n",
      "Epoch:9/20... Training Step:2450... Training loss:2.3618... 0.2239 sec/batch\n",
      "Epoch:9/20... Training Step:2451... Training loss:2.3147... 0.2121 sec/batch\n",
      "Epoch:9/20... Training Step:2452... Training loss:2.3235... 0.1924 sec/batch\n",
      "Epoch:9/20... Training Step:2453... Training loss:2.3311... 0.1957 sec/batch\n",
      "Epoch:9/20... Training Step:2454... Training loss:2.3340... 0.2092 sec/batch\n",
      "Epoch:9/20... Training Step:2455... Training loss:2.3008... 0.2032 sec/batch\n",
      "Epoch:9/20... Training Step:2456... Training loss:2.2867... 0.2202 sec/batch\n",
      "Epoch:9/20... Training Step:2457... Training loss:2.2891... 0.2093 sec/batch\n",
      "Epoch:9/20... Training Step:2458... Training loss:2.2957... 0.1929 sec/batch\n",
      "Epoch:9/20... Training Step:2459... Training loss:2.2970... 0.2055 sec/batch\n",
      "Epoch:9/20... Training Step:2460... Training loss:2.2833... 0.1943 sec/batch\n",
      "Epoch:9/20... Training Step:2461... Training loss:2.2914... 0.2010 sec/batch\n",
      "Epoch:9/20... Training Step:2462... Training loss:2.3559... 0.1923 sec/batch\n",
      "Epoch:9/20... Training Step:2463... Training loss:2.3047... 0.1946 sec/batch\n",
      "Epoch:9/20... Training Step:2464... Training loss:2.2726... 0.2113 sec/batch\n",
      "Epoch:9/20... Training Step:2465... Training loss:2.2917... 0.2043 sec/batch\n",
      "Epoch:9/20... Training Step:2466... Training loss:2.2936... 0.2003 sec/batch\n",
      "Epoch:9/20... Training Step:2467... Training loss:2.2725... 0.1941 sec/batch\n",
      "Epoch:9/20... Training Step:2468... Training loss:2.2772... 0.1913 sec/batch\n",
      "Epoch:9/20... Training Step:2469... Training loss:2.2859... 0.1937 sec/batch\n",
      "Epoch:9/20... Training Step:2470... Training loss:2.3049... 0.1918 sec/batch\n",
      "Epoch:9/20... Training Step:2471... Training loss:2.2591... 0.1948 sec/batch\n",
      "Epoch:9/20... Training Step:2472... Training loss:2.2772... 0.1925 sec/batch\n",
      "Epoch:9/20... Training Step:2473... Training loss:2.2984... 0.1994 sec/batch\n",
      "Epoch:9/20... Training Step:2474... Training loss:2.2733... 0.2082 sec/batch\n",
      "Epoch:9/20... Training Step:2475... Training loss:2.2794... 0.2074 sec/batch\n",
      "Epoch:9/20... Training Step:2476... Training loss:2.3061... 0.2224 sec/batch\n",
      "Epoch:9/20... Training Step:2477... Training loss:2.2991... 0.1966 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9/20... Training Step:2478... Training loss:2.2785... 0.1926 sec/batch\n",
      "Epoch:9/20... Training Step:2479... Training loss:2.2692... 0.1923 sec/batch\n",
      "Epoch:9/20... Training Step:2480... Training loss:2.2692... 0.1948 sec/batch\n",
      "Epoch:9/20... Training Step:2481... Training loss:2.2619... 0.2030 sec/batch\n",
      "Epoch:9/20... Training Step:2482... Training loss:2.2632... 0.2030 sec/batch\n",
      "Epoch:9/20... Training Step:2483... Training loss:2.2533... 0.2053 sec/batch\n",
      "Epoch:9/20... Training Step:2484... Training loss:2.3070... 0.1924 sec/batch\n",
      "Epoch:9/20... Training Step:2485... Training loss:2.2997... 0.1933 sec/batch\n",
      "Epoch:9/20... Training Step:2486... Training loss:2.3263... 0.1923 sec/batch\n",
      "Epoch:9/20... Training Step:2487... Training loss:2.3292... 0.2183 sec/batch\n",
      "Epoch:9/20... Training Step:2488... Training loss:2.3207... 0.1931 sec/batch\n",
      "Epoch:9/20... Training Step:2489... Training loss:2.2734... 0.1927 sec/batch\n",
      "Epoch:9/20... Training Step:2490... Training loss:2.2692... 0.1930 sec/batch\n",
      "Epoch:9/20... Training Step:2491... Training loss:2.2852... 0.1930 sec/batch\n",
      "Epoch:9/20... Training Step:2492... Training loss:2.2640... 0.2098 sec/batch\n",
      "Epoch:9/20... Training Step:2493... Training loss:2.2516... 0.1945 sec/batch\n",
      "Epoch:9/20... Training Step:2494... Training loss:2.2977... 0.2075 sec/batch\n",
      "Epoch:9/20... Training Step:2495... Training loss:2.2882... 0.1954 sec/batch\n",
      "Epoch:9/20... Training Step:2496... Training loss:2.2816... 0.1928 sec/batch\n",
      "Epoch:9/20... Training Step:2497... Training loss:2.2689... 0.1927 sec/batch\n",
      "Epoch:9/20... Training Step:2498... Training loss:2.2524... 0.1938 sec/batch\n",
      "Epoch:9/20... Training Step:2499... Training loss:2.2626... 0.1934 sec/batch\n",
      "Epoch:9/20... Training Step:2500... Training loss:2.2808... 0.2004 sec/batch\n",
      "Epoch:9/20... Training Step:2501... Training loss:2.2699... 0.1952 sec/batch\n",
      "Epoch:9/20... Training Step:2502... Training loss:2.2913... 0.1942 sec/batch\n",
      "Epoch:9/20... Training Step:2503... Training loss:2.2931... 0.2027 sec/batch\n",
      "Epoch:9/20... Training Step:2504... Training loss:2.2685... 0.2064 sec/batch\n",
      "Epoch:9/20... Training Step:2505... Training loss:2.2833... 0.1977 sec/batch\n",
      "Epoch:9/20... Training Step:2506... Training loss:2.2657... 0.2047 sec/batch\n",
      "Epoch:9/20... Training Step:2507... Training loss:2.3006... 0.1938 sec/batch\n",
      "Epoch:9/20... Training Step:2508... Training loss:2.2989... 0.1937 sec/batch\n",
      "Epoch:9/20... Training Step:2509... Training loss:2.2842... 0.2011 sec/batch\n",
      "Epoch:9/20... Training Step:2510... Training loss:2.2964... 0.2016 sec/batch\n",
      "Epoch:9/20... Training Step:2511... Training loss:2.2588... 0.2102 sec/batch\n",
      "Epoch:9/20... Training Step:2512... Training loss:2.2987... 0.1986 sec/batch\n",
      "Epoch:9/20... Training Step:2513... Training loss:2.3216... 0.2128 sec/batch\n",
      "Epoch:9/20... Training Step:2514... Training loss:2.2750... 0.1921 sec/batch\n",
      "Epoch:9/20... Training Step:2515... Training loss:2.3023... 0.2011 sec/batch\n",
      "Epoch:9/20... Training Step:2516... Training loss:2.2937... 0.1933 sec/batch\n",
      "Epoch:9/20... Training Step:2517... Training loss:2.2625... 0.1921 sec/batch\n",
      "Epoch:9/20... Training Step:2518... Training loss:2.3015... 0.1956 sec/batch\n",
      "Epoch:9/20... Training Step:2519... Training loss:2.2839... 0.2003 sec/batch\n",
      "Epoch:9/20... Training Step:2520... Training loss:2.2767... 0.2085 sec/batch\n",
      "Epoch:9/20... Training Step:2521... Training loss:2.2941... 0.1918 sec/batch\n",
      "Epoch:9/20... Training Step:2522... Training loss:2.2839... 0.1969 sec/batch\n",
      "Epoch:9/20... Training Step:2523... Training loss:2.3015... 0.1923 sec/batch\n",
      "Epoch:9/20... Training Step:2524... Training loss:2.2663... 0.2133 sec/batch\n",
      "Epoch:9/20... Training Step:2525... Training loss:2.2352... 0.1914 sec/batch\n",
      "Epoch:9/20... Training Step:2526... Training loss:2.2751... 0.1961 sec/batch\n",
      "Epoch:9/20... Training Step:2527... Training loss:2.2620... 0.1918 sec/batch\n",
      "Epoch:9/20... Training Step:2528... Training loss:2.2598... 0.2121 sec/batch\n",
      "Epoch:9/20... Training Step:2529... Training loss:2.2340... 0.1921 sec/batch\n",
      "Epoch:9/20... Training Step:2530... Training loss:2.2496... 0.2061 sec/batch\n",
      "Epoch:9/20... Training Step:2531... Training loss:2.2504... 0.2179 sec/batch\n",
      "Epoch:9/20... Training Step:2532... Training loss:2.2375... 0.1933 sec/batch\n",
      "Epoch:9/20... Training Step:2533... Training loss:2.2418... 0.1996 sec/batch\n",
      "Epoch:9/20... Training Step:2534... Training loss:2.3059... 0.2170 sec/batch\n",
      "Epoch:9/20... Training Step:2535... Training loss:2.2384... 0.1926 sec/batch\n",
      "Epoch:9/20... Training Step:2536... Training loss:2.2269... 0.1941 sec/batch\n",
      "Epoch:9/20... Training Step:2537... Training loss:2.2359... 0.2001 sec/batch\n",
      "Epoch:9/20... Training Step:2538... Training loss:2.2364... 0.1938 sec/batch\n",
      "Epoch:9/20... Training Step:2539... Training loss:2.2592... 0.2006 sec/batch\n",
      "Epoch:9/20... Training Step:2540... Training loss:2.2658... 0.1934 sec/batch\n",
      "Epoch:9/20... Training Step:2541... Training loss:2.2863... 0.1946 sec/batch\n",
      "Epoch:9/20... Training Step:2542... Training loss:2.2958... 0.2009 sec/batch\n",
      "Epoch:9/20... Training Step:2543... Training loss:2.2728... 0.2082 sec/batch\n",
      "Epoch:9/20... Training Step:2544... Training loss:2.2474... 0.2095 sec/batch\n",
      "Epoch:9/20... Training Step:2545... Training loss:2.2819... 0.1920 sec/batch\n",
      "Epoch:9/20... Training Step:2546... Training loss:2.2536... 0.1947 sec/batch\n",
      "Epoch:9/20... Training Step:2547... Training loss:2.2571... 0.1910 sec/batch\n",
      "Epoch:9/20... Training Step:2548... Training loss:2.2702... 0.1923 sec/batch\n",
      "Epoch:9/20... Training Step:2549... Training loss:2.2717... 0.1917 sec/batch\n",
      "Epoch:9/20... Training Step:2550... Training loss:2.2425... 0.2016 sec/batch\n",
      "Epoch:9/20... Training Step:2551... Training loss:2.2340... 0.2050 sec/batch\n",
      "Epoch:9/20... Training Step:2552... Training loss:2.2739... 0.2057 sec/batch\n",
      "Epoch:9/20... Training Step:2553... Training loss:2.2467... 0.2028 sec/batch\n",
      "Epoch:9/20... Training Step:2554... Training loss:2.2647... 0.2083 sec/batch\n",
      "Epoch:9/20... Training Step:2555... Training loss:2.2685... 0.2013 sec/batch\n",
      "Epoch:9/20... Training Step:2556... Training loss:2.2818... 0.1912 sec/batch\n",
      "Epoch:9/20... Training Step:2557... Training loss:2.2512... 0.1989 sec/batch\n",
      "Epoch:9/20... Training Step:2558... Training loss:2.2699... 0.1920 sec/batch\n",
      "Epoch:9/20... Training Step:2559... Training loss:2.2381... 0.1928 sec/batch\n",
      "Epoch:9/20... Training Step:2560... Training loss:2.2490... 0.2144 sec/batch\n",
      "Epoch:9/20... Training Step:2561... Training loss:2.2572... 0.2115 sec/batch\n",
      "Epoch:9/20... Training Step:2562... Training loss:2.2961... 0.1990 sec/batch\n",
      "Epoch:9/20... Training Step:2563... Training loss:2.2974... 0.2129 sec/batch\n",
      "Epoch:9/20... Training Step:2564... Training loss:2.2545... 0.1963 sec/batch\n",
      "Epoch:9/20... Training Step:2565... Training loss:2.2839... 0.1955 sec/batch\n",
      "Epoch:9/20... Training Step:2566... Training loss:2.2826... 0.1958 sec/batch\n",
      "Epoch:9/20... Training Step:2567... Training loss:2.2582... 0.1942 sec/batch\n",
      "Epoch:9/20... Training Step:2568... Training loss:2.2597... 0.2070 sec/batch\n",
      "Epoch:9/20... Training Step:2569... Training loss:2.2452... 0.1946 sec/batch\n",
      "Epoch:9/20... Training Step:2570... Training loss:2.2719... 0.1993 sec/batch\n",
      "Epoch:9/20... Training Step:2571... Training loss:2.2638... 0.1925 sec/batch\n",
      "Epoch:9/20... Training Step:2572... Training loss:2.2921... 0.1938 sec/batch\n",
      "Epoch:9/20... Training Step:2573... Training loss:2.2535... 0.1937 sec/batch\n",
      "Epoch:9/20... Training Step:2574... Training loss:2.2623... 0.2171 sec/batch\n",
      "Epoch:9/20... Training Step:2575... Training loss:2.2940... 0.1899 sec/batch\n",
      "Epoch:9/20... Training Step:2576... Training loss:2.2717... 0.1990 sec/batch\n",
      "Epoch:9/20... Training Step:2577... Training loss:2.2369... 0.1975 sec/batch\n",
      "Epoch:9/20... Training Step:2578... Training loss:2.2903... 0.2058 sec/batch\n",
      "Epoch:9/20... Training Step:2579... Training loss:2.2867... 0.2039 sec/batch\n",
      "Epoch:9/20... Training Step:2580... Training loss:2.2808... 0.1985 sec/batch\n",
      "Epoch:9/20... Training Step:2581... Training loss:2.2898... 0.2075 sec/batch\n",
      "Epoch:9/20... Training Step:2582... Training loss:2.2679... 0.1923 sec/batch\n",
      "Epoch:9/20... Training Step:2583... Training loss:2.2416... 0.1915 sec/batch\n",
      "Epoch:9/20... Training Step:2584... Training loss:2.2138... 0.2018 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9/20... Training Step:2585... Training loss:2.2316... 0.2114 sec/batch\n",
      "Epoch:9/20... Training Step:2586... Training loss:2.2457... 0.1919 sec/batch\n",
      "Epoch:9/20... Training Step:2587... Training loss:2.2888... 0.1929 sec/batch\n",
      "Epoch:9/20... Training Step:2588... Training loss:2.2482... 0.1951 sec/batch\n",
      "Epoch:9/20... Training Step:2589... Training loss:2.2389... 0.1941 sec/batch\n",
      "Epoch:9/20... Training Step:2590... Training loss:2.2353... 0.1939 sec/batch\n",
      "Epoch:9/20... Training Step:2591... Training loss:2.2376... 0.1950 sec/batch\n",
      "Epoch:9/20... Training Step:2592... Training loss:2.2743... 0.1910 sec/batch\n",
      "Epoch:9/20... Training Step:2593... Training loss:2.2729... 0.2036 sec/batch\n",
      "Epoch:9/20... Training Step:2594... Training loss:2.2303... 0.1950 sec/batch\n",
      "Epoch:9/20... Training Step:2595... Training loss:2.2684... 0.1930 sec/batch\n",
      "Epoch:9/20... Training Step:2596... Training loss:2.2708... 0.2111 sec/batch\n",
      "Epoch:9/20... Training Step:2597... Training loss:2.2710... 0.2000 sec/batch\n",
      "Epoch:9/20... Training Step:2598... Training loss:2.2820... 0.1985 sec/batch\n",
      "Epoch:9/20... Training Step:2599... Training loss:2.2609... 0.2002 sec/batch\n",
      "Epoch:9/20... Training Step:2600... Training loss:2.2871... 0.1991 sec/batch\n",
      "Epoch:9/20... Training Step:2601... Training loss:2.2557... 0.1959 sec/batch\n",
      "Epoch:9/20... Training Step:2602... Training loss:2.2682... 0.1944 sec/batch\n",
      "Epoch:9/20... Training Step:2603... Training loss:2.2771... 0.1977 sec/batch\n",
      "Epoch:9/20... Training Step:2604... Training loss:2.2938... 0.2085 sec/batch\n",
      "Epoch:9/20... Training Step:2605... Training loss:2.3048... 0.1954 sec/batch\n",
      "Epoch:9/20... Training Step:2606... Training loss:2.2932... 0.2072 sec/batch\n",
      "Epoch:9/20... Training Step:2607... Training loss:2.2605... 0.1928 sec/batch\n",
      "Epoch:9/20... Training Step:2608... Training loss:2.2608... 0.1926 sec/batch\n",
      "Epoch:9/20... Training Step:2609... Training loss:2.2553... 0.2055 sec/batch\n",
      "Epoch:9/20... Training Step:2610... Training loss:2.2551... 0.1903 sec/batch\n",
      "Epoch:10/20... Training Step:2611... Training loss:2.4053... 0.1935 sec/batch\n",
      "Epoch:10/20... Training Step:2612... Training loss:2.2614... 0.1916 sec/batch\n",
      "Epoch:10/20... Training Step:2613... Training loss:2.2647... 0.2097 sec/batch\n",
      "Epoch:10/20... Training Step:2614... Training loss:2.2883... 0.1951 sec/batch\n",
      "Epoch:10/20... Training Step:2615... Training loss:2.2751... 0.1979 sec/batch\n",
      "Epoch:10/20... Training Step:2616... Training loss:2.2621... 0.2104 sec/batch\n",
      "Epoch:10/20... Training Step:2617... Training loss:2.2562... 0.1915 sec/batch\n",
      "Epoch:10/20... Training Step:2618... Training loss:2.2720... 0.1952 sec/batch\n",
      "Epoch:10/20... Training Step:2619... Training loss:2.2620... 0.2329 sec/batch\n",
      "Epoch:10/20... Training Step:2620... Training loss:2.2606... 0.1939 sec/batch\n",
      "Epoch:10/20... Training Step:2621... Training loss:2.2945... 0.1941 sec/batch\n",
      "Epoch:10/20... Training Step:2622... Training loss:2.2809... 0.1935 sec/batch\n",
      "Epoch:10/20... Training Step:2623... Training loss:2.2786... 0.1941 sec/batch\n",
      "Epoch:10/20... Training Step:2624... Training loss:2.2875... 0.2097 sec/batch\n",
      "Epoch:10/20... Training Step:2625... Training loss:2.2627... 0.1920 sec/batch\n",
      "Epoch:10/20... Training Step:2626... Training loss:2.2785... 0.1929 sec/batch\n",
      "Epoch:10/20... Training Step:2627... Training loss:2.2892... 0.1993 sec/batch\n",
      "Epoch:10/20... Training Step:2628... Training loss:2.2984... 0.1923 sec/batch\n",
      "Epoch:10/20... Training Step:2629... Training loss:2.2858... 0.1946 sec/batch\n",
      "Epoch:10/20... Training Step:2630... Training loss:2.2883... 0.2120 sec/batch\n",
      "Epoch:10/20... Training Step:2631... Training loss:2.2787... 0.1926 sec/batch\n",
      "Epoch:10/20... Training Step:2632... Training loss:2.2641... 0.2069 sec/batch\n",
      "Epoch:10/20... Training Step:2633... Training loss:2.2621... 0.2106 sec/batch\n",
      "Epoch:10/20... Training Step:2634... Training loss:2.3024... 0.2120 sec/batch\n",
      "Epoch:10/20... Training Step:2635... Training loss:2.2566... 0.2045 sec/batch\n",
      "Epoch:10/20... Training Step:2636... Training loss:2.2647... 0.2017 sec/batch\n",
      "Epoch:10/20... Training Step:2637... Training loss:2.2916... 0.2000 sec/batch\n",
      "Epoch:10/20... Training Step:2638... Training loss:2.2794... 0.1992 sec/batch\n",
      "Epoch:10/20... Training Step:2639... Training loss:2.2976... 0.1929 sec/batch\n",
      "Epoch:10/20... Training Step:2640... Training loss:2.2936... 0.1932 sec/batch\n",
      "Epoch:10/20... Training Step:2641... Training loss:2.2633... 0.2107 sec/batch\n",
      "Epoch:10/20... Training Step:2642... Training loss:2.2481... 0.1922 sec/batch\n",
      "Epoch:10/20... Training Step:2643... Training loss:2.2608... 0.2028 sec/batch\n",
      "Epoch:10/20... Training Step:2644... Training loss:2.2656... 0.1999 sec/batch\n",
      "Epoch:10/20... Training Step:2645... Training loss:2.2738... 0.1965 sec/batch\n",
      "Epoch:10/20... Training Step:2646... Training loss:2.2496... 0.2035 sec/batch\n",
      "Epoch:10/20... Training Step:2647... Training loss:2.2742... 0.2022 sec/batch\n",
      "Epoch:10/20... Training Step:2648... Training loss:2.2715... 0.1922 sec/batch\n",
      "Epoch:10/20... Training Step:2649... Training loss:2.2600... 0.1930 sec/batch\n",
      "Epoch:10/20... Training Step:2650... Training loss:2.2203... 0.1927 sec/batch\n",
      "Epoch:10/20... Training Step:2651... Training loss:2.2591... 0.1997 sec/batch\n",
      "Epoch:10/20... Training Step:2652... Training loss:2.2695... 0.2540 sec/batch\n",
      "Epoch:10/20... Training Step:2653... Training loss:2.2811... 0.1940 sec/batch\n",
      "Epoch:10/20... Training Step:2654... Training loss:2.2665... 0.2112 sec/batch\n",
      "Epoch:10/20... Training Step:2655... Training loss:2.2873... 0.2049 sec/batch\n",
      "Epoch:10/20... Training Step:2656... Training loss:2.2837... 0.2018 sec/batch\n",
      "Epoch:10/20... Training Step:2657... Training loss:2.2874... 0.2093 sec/batch\n",
      "Epoch:10/20... Training Step:2658... Training loss:2.2647... 0.2130 sec/batch\n",
      "Epoch:10/20... Training Step:2659... Training loss:2.2255... 0.1916 sec/batch\n",
      "Epoch:10/20... Training Step:2660... Training loss:2.2932... 0.1967 sec/batch\n",
      "Epoch:10/20... Training Step:2661... Training loss:2.2404... 0.1921 sec/batch\n",
      "Epoch:10/20... Training Step:2662... Training loss:2.2608... 0.1924 sec/batch\n",
      "Epoch:10/20... Training Step:2663... Training loss:2.2590... 0.1963 sec/batch\n",
      "Epoch:10/20... Training Step:2664... Training loss:2.2179... 0.1934 sec/batch\n",
      "Epoch:10/20... Training Step:2665... Training loss:2.2424... 0.2112 sec/batch\n",
      "Epoch:10/20... Training Step:2666... Training loss:2.2514... 0.2126 sec/batch\n",
      "Epoch:10/20... Training Step:2667... Training loss:2.2335... 0.2106 sec/batch\n",
      "Epoch:10/20... Training Step:2668... Training loss:2.2532... 0.1965 sec/batch\n",
      "Epoch:10/20... Training Step:2669... Training loss:2.2712... 0.1934 sec/batch\n",
      "Epoch:10/20... Training Step:2670... Training loss:2.2673... 0.2088 sec/batch\n",
      "Epoch:10/20... Training Step:2671... Training loss:2.2487... 0.2025 sec/batch\n",
      "Epoch:10/20... Training Step:2672... Training loss:2.2603... 0.1925 sec/batch\n",
      "Epoch:10/20... Training Step:2673... Training loss:2.2249... 0.2090 sec/batch\n",
      "Epoch:10/20... Training Step:2674... Training loss:2.2655... 0.1911 sec/batch\n",
      "Epoch:10/20... Training Step:2675... Training loss:2.2415... 0.1941 sec/batch\n",
      "Epoch:10/20... Training Step:2676... Training loss:2.2130... 0.1922 sec/batch\n",
      "Epoch:10/20... Training Step:2677... Training loss:2.2217... 0.1931 sec/batch\n",
      "Epoch:10/20... Training Step:2678... Training loss:2.2385... 0.1930 sec/batch\n",
      "Epoch:10/20... Training Step:2679... Training loss:2.2539... 0.1916 sec/batch\n",
      "Epoch:10/20... Training Step:2680... Training loss:2.2074... 0.1979 sec/batch\n",
      "Epoch:10/20... Training Step:2681... Training loss:2.2122... 0.2058 sec/batch\n",
      "Epoch:10/20... Training Step:2682... Training loss:2.2232... 0.2016 sec/batch\n",
      "Epoch:10/20... Training Step:2683... Training loss:2.2508... 0.1923 sec/batch\n",
      "Epoch:10/20... Training Step:2684... Training loss:2.2201... 0.1950 sec/batch\n",
      "Epoch:10/20... Training Step:2685... Training loss:2.1802... 0.1927 sec/batch\n",
      "Epoch:10/20... Training Step:2686... Training loss:2.2048... 0.2040 sec/batch\n",
      "Epoch:10/20... Training Step:2687... Training loss:2.2017... 0.1943 sec/batch\n",
      "Epoch:10/20... Training Step:2688... Training loss:2.2100... 0.1958 sec/batch\n",
      "Epoch:10/20... Training Step:2689... Training loss:2.2411... 0.1920 sec/batch\n",
      "Epoch:10/20... Training Step:2690... Training loss:2.2103... 0.1975 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/20... Training Step:2691... Training loss:2.2096... 0.2000 sec/batch\n",
      "Epoch:10/20... Training Step:2692... Training loss:2.2135... 0.2103 sec/batch\n",
      "Epoch:10/20... Training Step:2693... Training loss:2.2449... 0.1962 sec/batch\n",
      "Epoch:10/20... Training Step:2694... Training loss:2.2687... 0.2034 sec/batch\n",
      "Epoch:10/20... Training Step:2695... Training loss:2.2582... 0.1924 sec/batch\n",
      "Epoch:10/20... Training Step:2696... Training loss:2.2129... 0.1963 sec/batch\n",
      "Epoch:10/20... Training Step:2697... Training loss:2.2455... 0.2017 sec/batch\n",
      "Epoch:10/20... Training Step:2698... Training loss:2.2444... 0.2046 sec/batch\n",
      "Epoch:10/20... Training Step:2699... Training loss:2.2288... 0.1971 sec/batch\n",
      "Epoch:10/20... Training Step:2700... Training loss:2.2483... 0.2178 sec/batch\n",
      "Epoch:10/20... Training Step:2701... Training loss:2.2432... 0.1942 sec/batch\n",
      "Epoch:10/20... Training Step:2702... Training loss:2.2572... 0.1928 sec/batch\n",
      "Epoch:10/20... Training Step:2703... Training loss:2.2737... 0.1909 sec/batch\n",
      "Epoch:10/20... Training Step:2704... Training loss:2.2411... 0.2031 sec/batch\n",
      "Epoch:10/20... Training Step:2705... Training loss:2.2091... 0.2225 sec/batch\n",
      "Epoch:10/20... Training Step:2706... Training loss:2.2362... 0.1957 sec/batch\n",
      "Epoch:10/20... Training Step:2707... Training loss:2.2377... 0.2064 sec/batch\n",
      "Epoch:10/20... Training Step:2708... Training loss:2.2641... 0.1964 sec/batch\n",
      "Epoch:10/20... Training Step:2709... Training loss:2.2194... 0.1926 sec/batch\n",
      "Epoch:10/20... Training Step:2710... Training loss:2.2532... 0.1953 sec/batch\n",
      "Epoch:10/20... Training Step:2711... Training loss:2.2425... 0.2042 sec/batch\n",
      "Epoch:10/20... Training Step:2712... Training loss:2.2534... 0.1944 sec/batch\n",
      "Epoch:10/20... Training Step:2713... Training loss:2.2682... 0.2126 sec/batch\n",
      "Epoch:10/20... Training Step:2714... Training loss:2.2436... 0.1952 sec/batch\n",
      "Epoch:10/20... Training Step:2715... Training loss:2.2924... 0.2070 sec/batch\n",
      "Epoch:10/20... Training Step:2716... Training loss:2.2720... 0.2142 sec/batch\n",
      "Epoch:10/20... Training Step:2717... Training loss:2.2762... 0.1926 sec/batch\n",
      "Epoch:10/20... Training Step:2718... Training loss:2.2393... 0.2179 sec/batch\n",
      "Epoch:10/20... Training Step:2719... Training loss:2.2636... 0.1913 sec/batch\n",
      "Epoch:10/20... Training Step:2720... Training loss:2.2502... 0.1945 sec/batch\n",
      "Epoch:10/20... Training Step:2721... Training loss:2.2423... 0.1927 sec/batch\n",
      "Epoch:10/20... Training Step:2722... Training loss:2.2095... 0.2092 sec/batch\n",
      "Epoch:10/20... Training Step:2723... Training loss:2.2065... 0.2043 sec/batch\n",
      "Epoch:10/20... Training Step:2724... Training loss:2.2525... 0.1934 sec/batch\n",
      "Epoch:10/20... Training Step:2725... Training loss:2.2313... 0.1947 sec/batch\n",
      "Epoch:10/20... Training Step:2726... Training loss:2.2646... 0.2042 sec/batch\n",
      "Epoch:10/20... Training Step:2727... Training loss:2.2590... 0.2014 sec/batch\n",
      "Epoch:10/20... Training Step:2728... Training loss:2.2377... 0.2018 sec/batch\n",
      "Epoch:10/20... Training Step:2729... Training loss:2.2542... 0.1948 sec/batch\n",
      "Epoch:10/20... Training Step:2730... Training loss:2.2599... 0.1946 sec/batch\n",
      "Epoch:10/20... Training Step:2731... Training loss:2.2622... 0.1985 sec/batch\n",
      "Epoch:10/20... Training Step:2732... Training loss:2.2811... 0.2031 sec/batch\n",
      "Epoch:10/20... Training Step:2733... Training loss:2.2339... 0.1916 sec/batch\n",
      "Epoch:10/20... Training Step:2734... Training loss:2.2773... 0.2112 sec/batch\n",
      "Epoch:10/20... Training Step:2735... Training loss:2.2431... 0.1920 sec/batch\n",
      "Epoch:10/20... Training Step:2736... Training loss:2.2578... 0.2092 sec/batch\n",
      "Epoch:10/20... Training Step:2737... Training loss:2.2802... 0.1998 sec/batch\n",
      "Epoch:10/20... Training Step:2738... Training loss:2.2873... 0.2033 sec/batch\n",
      "Epoch:10/20... Training Step:2739... Training loss:2.2939... 0.2024 sec/batch\n",
      "Epoch:10/20... Training Step:2740... Training loss:2.3380... 0.2055 sec/batch\n",
      "Epoch:10/20... Training Step:2741... Training loss:2.2835... 0.1923 sec/batch\n",
      "Epoch:10/20... Training Step:2742... Training loss:2.2665... 0.1940 sec/batch\n",
      "Epoch:10/20... Training Step:2743... Training loss:2.2852... 0.2121 sec/batch\n",
      "Epoch:10/20... Training Step:2744... Training loss:2.2904... 0.2074 sec/batch\n",
      "Epoch:10/20... Training Step:2745... Training loss:2.2713... 0.2169 sec/batch\n",
      "Epoch:10/20... Training Step:2746... Training loss:2.2556... 0.2049 sec/batch\n",
      "Epoch:10/20... Training Step:2747... Training loss:2.2557... 0.1908 sec/batch\n",
      "Epoch:10/20... Training Step:2748... Training loss:2.2509... 0.1933 sec/batch\n",
      "Epoch:10/20... Training Step:2749... Training loss:2.2614... 0.1918 sec/batch\n",
      "Epoch:10/20... Training Step:2750... Training loss:2.2516... 0.1944 sec/batch\n",
      "Epoch:10/20... Training Step:2751... Training loss:2.2510... 0.1917 sec/batch\n",
      "Epoch:10/20... Training Step:2752... Training loss:2.3110... 0.2463 sec/batch\n",
      "Epoch:10/20... Training Step:2753... Training loss:2.2648... 0.1918 sec/batch\n",
      "Epoch:10/20... Training Step:2754... Training loss:2.2441... 0.1964 sec/batch\n",
      "Epoch:10/20... Training Step:2755... Training loss:2.2531... 0.2016 sec/batch\n",
      "Epoch:10/20... Training Step:2756... Training loss:2.2538... 0.2143 sec/batch\n",
      "Epoch:10/20... Training Step:2757... Training loss:2.2275... 0.1922 sec/batch\n",
      "Epoch:10/20... Training Step:2758... Training loss:2.2387... 0.2050 sec/batch\n",
      "Epoch:10/20... Training Step:2759... Training loss:2.2462... 0.1966 sec/batch\n",
      "Epoch:10/20... Training Step:2760... Training loss:2.2755... 0.2024 sec/batch\n",
      "Epoch:10/20... Training Step:2761... Training loss:2.2226... 0.2050 sec/batch\n",
      "Epoch:10/20... Training Step:2762... Training loss:2.2443... 0.1984 sec/batch\n",
      "Epoch:10/20... Training Step:2763... Training loss:2.2642... 0.2076 sec/batch\n",
      "Epoch:10/20... Training Step:2764... Training loss:2.2306... 0.1911 sec/batch\n",
      "Epoch:10/20... Training Step:2765... Training loss:2.2350... 0.1974 sec/batch\n",
      "Epoch:10/20... Training Step:2766... Training loss:2.2744... 0.1920 sec/batch\n",
      "Epoch:10/20... Training Step:2767... Training loss:2.2461... 0.1979 sec/batch\n",
      "Epoch:10/20... Training Step:2768... Training loss:2.2392... 0.1986 sec/batch\n",
      "Epoch:10/20... Training Step:2769... Training loss:2.2380... 0.2077 sec/batch\n",
      "Epoch:10/20... Training Step:2770... Training loss:2.2330... 0.1920 sec/batch\n",
      "Epoch:10/20... Training Step:2771... Training loss:2.2242... 0.2240 sec/batch\n",
      "Epoch:10/20... Training Step:2772... Training loss:2.2285... 0.2152 sec/batch\n",
      "Epoch:10/20... Training Step:2773... Training loss:2.2110... 0.2016 sec/batch\n",
      "Epoch:10/20... Training Step:2774... Training loss:2.2726... 0.2039 sec/batch\n",
      "Epoch:10/20... Training Step:2775... Training loss:2.2563... 0.1938 sec/batch\n",
      "Epoch:10/20... Training Step:2776... Training loss:2.2868... 0.1980 sec/batch\n",
      "Epoch:10/20... Training Step:2777... Training loss:2.2921... 0.1911 sec/batch\n",
      "Epoch:10/20... Training Step:2778... Training loss:2.2869... 0.2051 sec/batch\n",
      "Epoch:10/20... Training Step:2779... Training loss:2.2276... 0.1932 sec/batch\n",
      "Epoch:10/20... Training Step:2780... Training loss:2.2142... 0.1934 sec/batch\n",
      "Epoch:10/20... Training Step:2781... Training loss:2.2394... 0.2083 sec/batch\n",
      "Epoch:10/20... Training Step:2782... Training loss:2.2326... 0.1986 sec/batch\n",
      "Epoch:10/20... Training Step:2783... Training loss:2.2138... 0.2089 sec/batch\n",
      "Epoch:10/20... Training Step:2784... Training loss:2.2536... 0.1972 sec/batch\n",
      "Epoch:10/20... Training Step:2785... Training loss:2.2441... 0.2014 sec/batch\n",
      "Epoch:10/20... Training Step:2786... Training loss:2.2447... 0.1910 sec/batch\n",
      "Epoch:10/20... Training Step:2787... Training loss:2.2309... 0.1939 sec/batch\n",
      "Epoch:10/20... Training Step:2788... Training loss:2.2125... 0.1936 sec/batch\n",
      "Epoch:10/20... Training Step:2789... Training loss:2.2205... 0.2052 sec/batch\n",
      "Epoch:10/20... Training Step:2790... Training loss:2.2483... 0.1907 sec/batch\n",
      "Epoch:10/20... Training Step:2791... Training loss:2.2341... 0.1977 sec/batch\n",
      "Epoch:10/20... Training Step:2792... Training loss:2.2560... 0.1921 sec/batch\n",
      "Epoch:10/20... Training Step:2793... Training loss:2.2574... 0.1944 sec/batch\n",
      "Epoch:10/20... Training Step:2794... Training loss:2.2426... 0.2040 sec/batch\n",
      "Epoch:10/20... Training Step:2795... Training loss:2.2511... 0.2185 sec/batch\n",
      "Epoch:10/20... Training Step:2796... Training loss:2.2261... 0.2072 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/20... Training Step:2797... Training loss:2.2626... 0.1978 sec/batch\n",
      "Epoch:10/20... Training Step:2798... Training loss:2.2615... 0.2063 sec/batch\n",
      "Epoch:10/20... Training Step:2799... Training loss:2.2575... 0.2150 sec/batch\n",
      "Epoch:10/20... Training Step:2800... Training loss:2.2569... 0.1945 sec/batch\n",
      "Epoch:10/20... Training Step:2801... Training loss:2.2275... 0.2134 sec/batch\n",
      "Epoch:10/20... Training Step:2802... Training loss:2.2560... 0.1916 sec/batch\n",
      "Epoch:10/20... Training Step:2803... Training loss:2.2863... 0.2138 sec/batch\n",
      "Epoch:10/20... Training Step:2804... Training loss:2.2418... 0.1925 sec/batch\n",
      "Epoch:10/20... Training Step:2805... Training loss:2.2595... 0.1918 sec/batch\n",
      "Epoch:10/20... Training Step:2806... Training loss:2.2595... 0.1921 sec/batch\n",
      "Epoch:10/20... Training Step:2807... Training loss:2.2292... 0.1931 sec/batch\n",
      "Epoch:10/20... Training Step:2808... Training loss:2.2564... 0.2012 sec/batch\n",
      "Epoch:10/20... Training Step:2809... Training loss:2.2400... 0.2033 sec/batch\n",
      "Epoch:10/20... Training Step:2810... Training loss:2.2441... 0.2143 sec/batch\n",
      "Epoch:10/20... Training Step:2811... Training loss:2.2693... 0.2025 sec/batch\n",
      "Epoch:10/20... Training Step:2812... Training loss:2.2597... 0.2045 sec/batch\n",
      "Epoch:10/20... Training Step:2813... Training loss:2.2658... 0.1932 sec/batch\n",
      "Epoch:10/20... Training Step:2814... Training loss:2.2303... 0.2070 sec/batch\n",
      "Epoch:10/20... Training Step:2815... Training loss:2.1909... 0.2030 sec/batch\n",
      "Epoch:10/20... Training Step:2816... Training loss:2.2371... 0.1950 sec/batch\n",
      "Epoch:10/20... Training Step:2817... Training loss:2.2320... 0.2020 sec/batch\n",
      "Epoch:10/20... Training Step:2818... Training loss:2.2263... 0.2070 sec/batch\n",
      "Epoch:10/20... Training Step:2819... Training loss:2.2149... 0.1917 sec/batch\n",
      "Epoch:10/20... Training Step:2820... Training loss:2.2135... 0.1977 sec/batch\n",
      "Epoch:10/20... Training Step:2821... Training loss:2.2114... 0.1920 sec/batch\n",
      "Epoch:10/20... Training Step:2822... Training loss:2.1959... 0.1937 sec/batch\n",
      "Epoch:10/20... Training Step:2823... Training loss:2.2112... 0.1913 sec/batch\n",
      "Epoch:10/20... Training Step:2824... Training loss:2.2667... 0.2096 sec/batch\n",
      "Epoch:10/20... Training Step:2825... Training loss:2.1987... 0.2007 sec/batch\n",
      "Epoch:10/20... Training Step:2826... Training loss:2.1944... 0.2139 sec/batch\n",
      "Epoch:10/20... Training Step:2827... Training loss:2.2053... 0.2142 sec/batch\n",
      "Epoch:10/20... Training Step:2828... Training loss:2.1918... 0.2093 sec/batch\n",
      "Epoch:10/20... Training Step:2829... Training loss:2.2180... 0.2191 sec/batch\n",
      "Epoch:10/20... Training Step:2830... Training loss:2.2287... 0.1912 sec/batch\n",
      "Epoch:10/20... Training Step:2831... Training loss:2.2429... 0.1933 sec/batch\n",
      "Epoch:10/20... Training Step:2832... Training loss:2.2569... 0.2169 sec/batch\n",
      "Epoch:10/20... Training Step:2833... Training loss:2.2271... 0.1931 sec/batch\n",
      "Epoch:10/20... Training Step:2834... Training loss:2.2012... 0.1961 sec/batch\n",
      "Epoch:10/20... Training Step:2835... Training loss:2.2401... 0.2096 sec/batch\n",
      "Epoch:10/20... Training Step:2836... Training loss:2.2004... 0.2077 sec/batch\n",
      "Epoch:10/20... Training Step:2837... Training loss:2.2160... 0.1923 sec/batch\n",
      "Epoch:10/20... Training Step:2838... Training loss:2.2196... 0.2020 sec/batch\n",
      "Epoch:10/20... Training Step:2839... Training loss:2.2335... 0.1941 sec/batch\n",
      "Epoch:10/20... Training Step:2840... Training loss:2.2036... 0.1997 sec/batch\n",
      "Epoch:10/20... Training Step:2841... Training loss:2.1966... 0.1972 sec/batch\n",
      "Epoch:10/20... Training Step:2842... Training loss:2.2401... 0.1968 sec/batch\n",
      "Epoch:10/20... Training Step:2843... Training loss:2.2038... 0.1945 sec/batch\n",
      "Epoch:10/20... Training Step:2844... Training loss:2.2123... 0.1921 sec/batch\n",
      "Epoch:10/20... Training Step:2845... Training loss:2.2349... 0.2034 sec/batch\n",
      "Epoch:10/20... Training Step:2846... Training loss:2.2382... 0.1931 sec/batch\n",
      "Epoch:10/20... Training Step:2847... Training loss:2.2093... 0.1954 sec/batch\n",
      "Epoch:10/20... Training Step:2848... Training loss:2.2263... 0.1925 sec/batch\n",
      "Epoch:10/20... Training Step:2849... Training loss:2.1898... 0.1982 sec/batch\n",
      "Epoch:10/20... Training Step:2850... Training loss:2.2067... 0.1920 sec/batch\n",
      "Epoch:10/20... Training Step:2851... Training loss:2.2242... 0.2012 sec/batch\n",
      "Epoch:10/20... Training Step:2852... Training loss:2.2478... 0.2016 sec/batch\n",
      "Epoch:10/20... Training Step:2853... Training loss:2.2680... 0.1996 sec/batch\n",
      "Epoch:10/20... Training Step:2854... Training loss:2.2195... 0.1957 sec/batch\n",
      "Epoch:10/20... Training Step:2855... Training loss:2.2532... 0.1965 sec/batch\n",
      "Epoch:10/20... Training Step:2856... Training loss:2.2512... 0.1905 sec/batch\n",
      "Epoch:10/20... Training Step:2857... Training loss:2.2254... 0.1968 sec/batch\n",
      "Epoch:10/20... Training Step:2858... Training loss:2.2285... 0.1947 sec/batch\n",
      "Epoch:10/20... Training Step:2859... Training loss:2.2167... 0.1968 sec/batch\n",
      "Epoch:10/20... Training Step:2860... Training loss:2.2425... 0.1942 sec/batch\n",
      "Epoch:10/20... Training Step:2861... Training loss:2.2312... 0.1948 sec/batch\n",
      "Epoch:10/20... Training Step:2862... Training loss:2.2620... 0.1930 sec/batch\n",
      "Epoch:10/20... Training Step:2863... Training loss:2.2185... 0.2006 sec/batch\n",
      "Epoch:10/20... Training Step:2864... Training loss:2.2249... 0.1997 sec/batch\n",
      "Epoch:10/20... Training Step:2865... Training loss:2.2598... 0.2082 sec/batch\n",
      "Epoch:10/20... Training Step:2866... Training loss:2.2422... 0.2050 sec/batch\n",
      "Epoch:10/20... Training Step:2867... Training loss:2.2042... 0.2160 sec/batch\n",
      "Epoch:10/20... Training Step:2868... Training loss:2.2567... 0.2116 sec/batch\n",
      "Epoch:10/20... Training Step:2869... Training loss:2.2404... 0.1975 sec/batch\n",
      "Epoch:10/20... Training Step:2870... Training loss:2.2543... 0.2152 sec/batch\n",
      "Epoch:10/20... Training Step:2871... Training loss:2.2548... 0.1980 sec/batch\n",
      "Epoch:10/20... Training Step:2872... Training loss:2.2225... 0.1937 sec/batch\n",
      "Epoch:10/20... Training Step:2873... Training loss:2.2028... 0.2006 sec/batch\n",
      "Epoch:10/20... Training Step:2874... Training loss:2.1846... 0.1938 sec/batch\n",
      "Epoch:10/20... Training Step:2875... Training loss:2.1946... 0.1944 sec/batch\n",
      "Epoch:10/20... Training Step:2876... Training loss:2.1959... 0.2134 sec/batch\n",
      "Epoch:10/20... Training Step:2877... Training loss:2.2527... 0.1919 sec/batch\n",
      "Epoch:10/20... Training Step:2878... Training loss:2.2047... 0.1945 sec/batch\n",
      "Epoch:10/20... Training Step:2879... Training loss:2.2019... 0.1918 sec/batch\n",
      "Epoch:10/20... Training Step:2880... Training loss:2.2025... 0.1928 sec/batch\n",
      "Epoch:10/20... Training Step:2881... Training loss:2.1934... 0.2033 sec/batch\n",
      "Epoch:10/20... Training Step:2882... Training loss:2.2430... 0.1924 sec/batch\n",
      "Epoch:10/20... Training Step:2883... Training loss:2.2501... 0.2091 sec/batch\n",
      "Epoch:10/20... Training Step:2884... Training loss:2.1941... 0.1944 sec/batch\n",
      "Epoch:10/20... Training Step:2885... Training loss:2.2357... 0.1998 sec/batch\n",
      "Epoch:10/20... Training Step:2886... Training loss:2.2371... 0.2001 sec/batch\n",
      "Epoch:10/20... Training Step:2887... Training loss:2.2318... 0.2162 sec/batch\n",
      "Epoch:10/20... Training Step:2888... Training loss:2.2396... 0.1919 sec/batch\n",
      "Epoch:10/20... Training Step:2889... Training loss:2.2175... 0.1932 sec/batch\n",
      "Epoch:10/20... Training Step:2890... Training loss:2.2405... 0.1919 sec/batch\n",
      "Epoch:10/20... Training Step:2891... Training loss:2.2239... 0.1922 sec/batch\n",
      "Epoch:10/20... Training Step:2892... Training loss:2.2307... 0.2015 sec/batch\n",
      "Epoch:10/20... Training Step:2893... Training loss:2.2469... 0.2051 sec/batch\n",
      "Epoch:10/20... Training Step:2894... Training loss:2.2555... 0.1933 sec/batch\n",
      "Epoch:10/20... Training Step:2895... Training loss:2.2648... 0.2032 sec/batch\n",
      "Epoch:10/20... Training Step:2896... Training loss:2.2608... 0.1948 sec/batch\n",
      "Epoch:10/20... Training Step:2897... Training loss:2.2161... 0.1944 sec/batch\n",
      "Epoch:10/20... Training Step:2898... Training loss:2.2180... 0.2023 sec/batch\n",
      "Epoch:10/20... Training Step:2899... Training loss:2.2148... 0.1911 sec/batch\n",
      "Epoch:10/20... Training Step:2900... Training loss:2.2166... 0.2078 sec/batch\n",
      "Epoch:11/20... Training Step:2901... Training loss:2.3619... 0.1931 sec/batch\n",
      "Epoch:11/20... Training Step:2902... Training loss:2.2240... 0.1936 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11/20... Training Step:2903... Training loss:2.2329... 0.2054 sec/batch\n",
      "Epoch:11/20... Training Step:2904... Training loss:2.2521... 0.1921 sec/batch\n",
      "Epoch:11/20... Training Step:2905... Training loss:2.2324... 0.1916 sec/batch\n",
      "Epoch:11/20... Training Step:2906... Training loss:2.2212... 0.2147 sec/batch\n",
      "Epoch:11/20... Training Step:2907... Training loss:2.2196... 0.1945 sec/batch\n",
      "Epoch:11/20... Training Step:2908... Training loss:2.2376... 0.1935 sec/batch\n",
      "Epoch:11/20... Training Step:2909... Training loss:2.2348... 0.1938 sec/batch\n",
      "Epoch:11/20... Training Step:2910... Training loss:2.2169... 0.1946 sec/batch\n",
      "Epoch:11/20... Training Step:2911... Training loss:2.2579... 0.1937 sec/batch\n",
      "Epoch:11/20... Training Step:2912... Training loss:2.2391... 0.1945 sec/batch\n",
      "Epoch:11/20... Training Step:2913... Training loss:2.2517... 0.1990 sec/batch\n",
      "Epoch:11/20... Training Step:2914... Training loss:2.2492... 0.2096 sec/batch\n",
      "Epoch:11/20... Training Step:2915... Training loss:2.2139... 0.2054 sec/batch\n",
      "Epoch:11/20... Training Step:2916... Training loss:2.2516... 0.1947 sec/batch\n",
      "Epoch:11/20... Training Step:2917... Training loss:2.2430... 0.1935 sec/batch\n",
      "Epoch:11/20... Training Step:2918... Training loss:2.2671... 0.2015 sec/batch\n",
      "Epoch:11/20... Training Step:2919... Training loss:2.2532... 0.2148 sec/batch\n",
      "Epoch:11/20... Training Step:2920... Training loss:2.2512... 0.1938 sec/batch\n",
      "Epoch:11/20... Training Step:2921... Training loss:2.2557... 0.1946 sec/batch\n",
      "Epoch:11/20... Training Step:2922... Training loss:2.2319... 0.1966 sec/batch\n",
      "Epoch:11/20... Training Step:2923... Training loss:2.2200... 0.2077 sec/batch\n",
      "Epoch:11/20... Training Step:2924... Training loss:2.2656... 0.1920 sec/batch\n",
      "Epoch:11/20... Training Step:2925... Training loss:2.2211... 0.2024 sec/batch\n",
      "Epoch:11/20... Training Step:2926... Training loss:2.2306... 0.1937 sec/batch\n",
      "Epoch:11/20... Training Step:2927... Training loss:2.2507... 0.2109 sec/batch\n",
      "Epoch:11/20... Training Step:2928... Training loss:2.2458... 0.2138 sec/batch\n",
      "Epoch:11/20... Training Step:2929... Training loss:2.2606... 0.1963 sec/batch\n",
      "Epoch:11/20... Training Step:2930... Training loss:2.2555... 0.2034 sec/batch\n",
      "Epoch:11/20... Training Step:2931... Training loss:2.2389... 0.2017 sec/batch\n",
      "Epoch:11/20... Training Step:2932... Training loss:2.2161... 0.1939 sec/batch\n",
      "Epoch:11/20... Training Step:2933... Training loss:2.2230... 0.1932 sec/batch\n",
      "Epoch:11/20... Training Step:2934... Training loss:2.2280... 0.2036 sec/batch\n",
      "Epoch:11/20... Training Step:2935... Training loss:2.2407... 0.1937 sec/batch\n",
      "Epoch:11/20... Training Step:2936... Training loss:2.2168... 0.1926 sec/batch\n",
      "Epoch:11/20... Training Step:2937... Training loss:2.2415... 0.1910 sec/batch\n",
      "Epoch:11/20... Training Step:2938... Training loss:2.2449... 0.1936 sec/batch\n",
      "Epoch:11/20... Training Step:2939... Training loss:2.2191... 0.1985 sec/batch\n",
      "Epoch:11/20... Training Step:2940... Training loss:2.1844... 0.1939 sec/batch\n",
      "Epoch:11/20... Training Step:2941... Training loss:2.2100... 0.1924 sec/batch\n",
      "Epoch:11/20... Training Step:2942... Training loss:2.2362... 0.1940 sec/batch\n",
      "Epoch:11/20... Training Step:2943... Training loss:2.2483... 0.1916 sec/batch\n",
      "Epoch:11/20... Training Step:2944... Training loss:2.2277... 0.2095 sec/batch\n",
      "Epoch:11/20... Training Step:2945... Training loss:2.2509... 0.1914 sec/batch\n",
      "Epoch:11/20... Training Step:2946... Training loss:2.2595... 0.1916 sec/batch\n",
      "Epoch:11/20... Training Step:2947... Training loss:2.2541... 0.1945 sec/batch\n",
      "Epoch:11/20... Training Step:2948... Training loss:2.2202... 0.1995 sec/batch\n",
      "Epoch:11/20... Training Step:2949... Training loss:2.1885... 0.2035 sec/batch\n",
      "Epoch:11/20... Training Step:2950... Training loss:2.2522... 0.1929 sec/batch\n",
      "Epoch:11/20... Training Step:2951... Training loss:2.2170... 0.2053 sec/batch\n",
      "Epoch:11/20... Training Step:2952... Training loss:2.2301... 0.1938 sec/batch\n",
      "Epoch:11/20... Training Step:2953... Training loss:2.2274... 0.2108 sec/batch\n",
      "Epoch:11/20... Training Step:2954... Training loss:2.1816... 0.2032 sec/batch\n",
      "Epoch:11/20... Training Step:2955... Training loss:2.1909... 0.1927 sec/batch\n",
      "Epoch:11/20... Training Step:2956... Training loss:2.2147... 0.1936 sec/batch\n",
      "Epoch:11/20... Training Step:2957... Training loss:2.1960... 0.2076 sec/batch\n",
      "Epoch:11/20... Training Step:2958... Training loss:2.2214... 0.1932 sec/batch\n",
      "Epoch:11/20... Training Step:2959... Training loss:2.2379... 0.1941 sec/batch\n",
      "Epoch:11/20... Training Step:2960... Training loss:2.2248... 0.2099 sec/batch\n",
      "Epoch:11/20... Training Step:2961... Training loss:2.2128... 0.2027 sec/batch\n",
      "Epoch:11/20... Training Step:2962... Training loss:2.2188... 0.1931 sec/batch\n",
      "Epoch:11/20... Training Step:2963... Training loss:2.1979... 0.1926 sec/batch\n",
      "Epoch:11/20... Training Step:2964... Training loss:2.2171... 0.1955 sec/batch\n",
      "Epoch:11/20... Training Step:2965... Training loss:2.2130... 0.1948 sec/batch\n",
      "Epoch:11/20... Training Step:2966... Training loss:2.1672... 0.2096 sec/batch\n",
      "Epoch:11/20... Training Step:2967... Training loss:2.1773... 0.1976 sec/batch\n",
      "Epoch:11/20... Training Step:2968... Training loss:2.2002... 0.2084 sec/batch\n",
      "Epoch:11/20... Training Step:2969... Training loss:2.2174... 0.2089 sec/batch\n",
      "Epoch:11/20... Training Step:2970... Training loss:2.1674... 0.1956 sec/batch\n",
      "Epoch:11/20... Training Step:2971... Training loss:2.1642... 0.1998 sec/batch\n",
      "Epoch:11/20... Training Step:2972... Training loss:2.1898... 0.1944 sec/batch\n",
      "Epoch:11/20... Training Step:2973... Training loss:2.2074... 0.1946 sec/batch\n",
      "Epoch:11/20... Training Step:2974... Training loss:2.1846... 0.1929 sec/batch\n",
      "Epoch:11/20... Training Step:2975... Training loss:2.1475... 0.1936 sec/batch\n",
      "Epoch:11/20... Training Step:2976... Training loss:2.1701... 0.1921 sec/batch\n",
      "Epoch:11/20... Training Step:2977... Training loss:2.1710... 0.1957 sec/batch\n",
      "Epoch:11/20... Training Step:2978... Training loss:2.1770... 0.2190 sec/batch\n",
      "Epoch:11/20... Training Step:2979... Training loss:2.2036... 0.1921 sec/batch\n",
      "Epoch:11/20... Training Step:2980... Training loss:2.1801... 0.1938 sec/batch\n",
      "Epoch:11/20... Training Step:2981... Training loss:2.1553... 0.1938 sec/batch\n",
      "Epoch:11/20... Training Step:2982... Training loss:2.1791... 0.1935 sec/batch\n",
      "Epoch:11/20... Training Step:2983... Training loss:2.2097... 0.1923 sec/batch\n",
      "Epoch:11/20... Training Step:2984... Training loss:2.2322... 0.1937 sec/batch\n",
      "Epoch:11/20... Training Step:2985... Training loss:2.2163... 0.1915 sec/batch\n",
      "Epoch:11/20... Training Step:2986... Training loss:2.1780... 0.2027 sec/batch\n",
      "Epoch:11/20... Training Step:2987... Training loss:2.2010... 0.2053 sec/batch\n",
      "Epoch:11/20... Training Step:2988... Training loss:2.2170... 0.1985 sec/batch\n",
      "Epoch:11/20... Training Step:2989... Training loss:2.1956... 0.2160 sec/batch\n",
      "Epoch:11/20... Training Step:2990... Training loss:2.2116... 0.1967 sec/batch\n",
      "Epoch:11/20... Training Step:2991... Training loss:2.2109... 0.1978 sec/batch\n",
      "Epoch:11/20... Training Step:2992... Training loss:2.2095... 0.1998 sec/batch\n",
      "Epoch:11/20... Training Step:2993... Training loss:2.2316... 0.1918 sec/batch\n",
      "Epoch:11/20... Training Step:2994... Training loss:2.2064... 0.2065 sec/batch\n",
      "Epoch:11/20... Training Step:2995... Training loss:2.1829... 0.1946 sec/batch\n",
      "Epoch:11/20... Training Step:2996... Training loss:2.2051... 0.2005 sec/batch\n",
      "Epoch:11/20... Training Step:2997... Training loss:2.2107... 0.1966 sec/batch\n",
      "Epoch:11/20... Training Step:2998... Training loss:2.2366... 0.2079 sec/batch\n",
      "Epoch:11/20... Training Step:2999... Training loss:2.1752... 0.1943 sec/batch\n",
      "Epoch:11/20... Training Step:3000... Training loss:2.2224... 0.2148 sec/batch\n",
      "Epoch:11/20... Training Step:3001... Training loss:2.2099... 0.2039 sec/batch\n",
      "Epoch:11/20... Training Step:3002... Training loss:2.2268... 0.2031 sec/batch\n",
      "Epoch:11/20... Training Step:3003... Training loss:2.2342... 0.1921 sec/batch\n",
      "Epoch:11/20... Training Step:3004... Training loss:2.2077... 0.2038 sec/batch\n",
      "Epoch:11/20... Training Step:3005... Training loss:2.2553... 0.2125 sec/batch\n",
      "Epoch:11/20... Training Step:3006... Training loss:2.2319... 0.1973 sec/batch\n",
      "Epoch:11/20... Training Step:3007... Training loss:2.2361... 0.2085 sec/batch\n",
      "Epoch:11/20... Training Step:3008... Training loss:2.2017... 0.1917 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11/20... Training Step:3009... Training loss:2.2264... 0.1923 sec/batch\n",
      "Epoch:11/20... Training Step:3010... Training loss:2.2117... 0.1978 sec/batch\n",
      "Epoch:11/20... Training Step:3011... Training loss:2.2085... 0.1977 sec/batch\n",
      "Epoch:11/20... Training Step:3012... Training loss:2.1756... 0.1948 sec/batch\n",
      "Epoch:11/20... Training Step:3013... Training loss:2.1676... 0.1912 sec/batch\n",
      "Epoch:11/20... Training Step:3014... Training loss:2.2173... 0.1954 sec/batch\n",
      "Epoch:11/20... Training Step:3015... Training loss:2.1959... 0.2031 sec/batch\n",
      "Epoch:11/20... Training Step:3016... Training loss:2.2303... 0.1929 sec/batch\n",
      "Epoch:11/20... Training Step:3017... Training loss:2.2202... 0.1928 sec/batch\n",
      "Epoch:11/20... Training Step:3018... Training loss:2.2017... 0.1928 sec/batch\n",
      "Epoch:11/20... Training Step:3019... Training loss:2.2211... 0.1941 sec/batch\n",
      "Epoch:11/20... Training Step:3020... Training loss:2.2193... 0.2051 sec/batch\n",
      "Epoch:11/20... Training Step:3021... Training loss:2.2277... 0.1930 sec/batch\n",
      "Epoch:11/20... Training Step:3022... Training loss:2.2370... 0.2085 sec/batch\n",
      "Epoch:11/20... Training Step:3023... Training loss:2.1970... 0.2073 sec/batch\n",
      "Epoch:11/20... Training Step:3024... Training loss:2.2368... 0.1918 sec/batch\n",
      "Epoch:11/20... Training Step:3025... Training loss:2.2114... 0.1924 sec/batch\n",
      "Epoch:11/20... Training Step:3026... Training loss:2.2321... 0.2068 sec/batch\n",
      "Epoch:11/20... Training Step:3027... Training loss:2.2438... 0.1928 sec/batch\n",
      "Epoch:11/20... Training Step:3028... Training loss:2.2501... 0.2070 sec/batch\n",
      "Epoch:11/20... Training Step:3029... Training loss:2.2650... 0.2068 sec/batch\n",
      "Epoch:11/20... Training Step:3030... Training loss:2.2937... 0.1990 sec/batch\n",
      "Epoch:11/20... Training Step:3031... Training loss:2.2452... 0.2019 sec/batch\n",
      "Epoch:11/20... Training Step:3032... Training loss:2.2401... 0.1928 sec/batch\n",
      "Epoch:11/20... Training Step:3033... Training loss:2.2505... 0.1942 sec/batch\n",
      "Epoch:11/20... Training Step:3034... Training loss:2.2640... 0.1986 sec/batch\n",
      "Epoch:11/20... Training Step:3035... Training loss:2.2356... 0.2038 sec/batch\n",
      "Epoch:11/20... Training Step:3036... Training loss:2.2108... 0.1933 sec/batch\n",
      "Epoch:11/20... Training Step:3037... Training loss:2.2188... 0.1957 sec/batch\n",
      "Epoch:11/20... Training Step:3038... Training loss:2.2182... 0.2068 sec/batch\n",
      "Epoch:11/20... Training Step:3039... Training loss:2.2240... 0.2077 sec/batch\n",
      "Epoch:11/20... Training Step:3040... Training loss:2.2143... 0.2018 sec/batch\n",
      "Epoch:11/20... Training Step:3041... Training loss:2.2135... 0.2013 sec/batch\n",
      "Epoch:11/20... Training Step:3042... Training loss:2.2791... 0.1946 sec/batch\n",
      "Epoch:11/20... Training Step:3043... Training loss:2.2280... 0.1967 sec/batch\n",
      "Epoch:11/20... Training Step:3044... Training loss:2.2039... 0.1930 sec/batch\n",
      "Epoch:11/20... Training Step:3045... Training loss:2.2088... 0.2001 sec/batch\n",
      "Epoch:11/20... Training Step:3046... Training loss:2.2339... 0.2011 sec/batch\n",
      "Epoch:11/20... Training Step:3047... Training loss:2.1932... 0.1966 sec/batch\n",
      "Epoch:11/20... Training Step:3048... Training loss:2.2026... 0.1945 sec/batch\n",
      "Epoch:11/20... Training Step:3049... Training loss:2.2156... 0.1916 sec/batch\n",
      "Epoch:11/20... Training Step:3050... Training loss:2.2298... 0.1972 sec/batch\n",
      "Epoch:11/20... Training Step:3051... Training loss:2.1885... 0.2116 sec/batch\n",
      "Epoch:11/20... Training Step:3052... Training loss:2.2021... 0.2001 sec/batch\n",
      "Epoch:11/20... Training Step:3053... Training loss:2.2260... 0.1913 sec/batch\n",
      "Epoch:11/20... Training Step:3054... Training loss:2.1883... 0.1955 sec/batch\n",
      "Epoch:11/20... Training Step:3055... Training loss:2.2051... 0.2089 sec/batch\n",
      "Epoch:11/20... Training Step:3056... Training loss:2.2351... 0.2030 sec/batch\n",
      "Epoch:11/20... Training Step:3057... Training loss:2.2123... 0.2026 sec/batch\n",
      "Epoch:11/20... Training Step:3058... Training loss:2.2013... 0.1926 sec/batch\n",
      "Epoch:11/20... Training Step:3059... Training loss:2.1926... 0.1999 sec/batch\n",
      "Epoch:11/20... Training Step:3060... Training loss:2.2003... 0.1921 sec/batch\n",
      "Epoch:11/20... Training Step:3061... Training loss:2.1877... 0.1933 sec/batch\n",
      "Epoch:11/20... Training Step:3062... Training loss:2.1996... 0.2040 sec/batch\n",
      "Epoch:11/20... Training Step:3063... Training loss:2.1770... 0.2016 sec/batch\n",
      "Epoch:11/20... Training Step:3064... Training loss:2.2354... 0.2134 sec/batch\n",
      "Epoch:11/20... Training Step:3065... Training loss:2.2237... 0.1923 sec/batch\n",
      "Epoch:11/20... Training Step:3066... Training loss:2.2486... 0.2071 sec/batch\n",
      "Epoch:11/20... Training Step:3067... Training loss:2.2531... 0.2006 sec/batch\n",
      "Epoch:11/20... Training Step:3068... Training loss:2.2506... 0.2335 sec/batch\n",
      "Epoch:11/20... Training Step:3069... Training loss:2.1988... 0.1992 sec/batch\n",
      "Epoch:11/20... Training Step:3070... Training loss:2.1963... 0.2049 sec/batch\n",
      "Epoch:11/20... Training Step:3071... Training loss:2.2114... 0.1911 sec/batch\n",
      "Epoch:11/20... Training Step:3072... Training loss:2.1934... 0.1934 sec/batch\n",
      "Epoch:11/20... Training Step:3073... Training loss:2.1872... 0.2056 sec/batch\n",
      "Epoch:11/20... Training Step:3074... Training loss:2.2159... 0.1944 sec/batch\n",
      "Epoch:11/20... Training Step:3075... Training loss:2.2174... 0.2110 sec/batch\n",
      "Epoch:11/20... Training Step:3076... Training loss:2.2070... 0.1930 sec/batch\n",
      "Epoch:11/20... Training Step:3077... Training loss:2.1887... 0.1934 sec/batch\n",
      "Epoch:11/20... Training Step:3078... Training loss:2.1844... 0.1973 sec/batch\n",
      "Epoch:11/20... Training Step:3079... Training loss:2.1929... 0.1976 sec/batch\n",
      "Epoch:11/20... Training Step:3080... Training loss:2.2195... 0.1906 sec/batch\n",
      "Epoch:11/20... Training Step:3081... Training loss:2.1970... 0.1938 sec/batch\n",
      "Epoch:11/20... Training Step:3082... Training loss:2.2193... 0.1953 sec/batch\n",
      "Epoch:11/20... Training Step:3083... Training loss:2.2068... 0.1927 sec/batch\n",
      "Epoch:11/20... Training Step:3084... Training loss:2.2076... 0.1933 sec/batch\n",
      "Epoch:11/20... Training Step:3085... Training loss:2.2128... 0.1915 sec/batch\n",
      "Epoch:11/20... Training Step:3086... Training loss:2.1887... 0.2081 sec/batch\n",
      "Epoch:11/20... Training Step:3087... Training loss:2.2189... 0.2020 sec/batch\n",
      "Epoch:11/20... Training Step:3088... Training loss:2.2275... 0.2022 sec/batch\n",
      "Epoch:11/20... Training Step:3089... Training loss:2.2184... 0.1917 sec/batch\n",
      "Epoch:11/20... Training Step:3090... Training loss:2.2229... 0.1949 sec/batch\n",
      "Epoch:11/20... Training Step:3091... Training loss:2.1816... 0.1912 sec/batch\n",
      "Epoch:11/20... Training Step:3092... Training loss:2.2228... 0.1960 sec/batch\n",
      "Epoch:11/20... Training Step:3093... Training loss:2.2465... 0.1973 sec/batch\n",
      "Epoch:11/20... Training Step:3094... Training loss:2.2224... 0.2098 sec/batch\n",
      "Epoch:11/20... Training Step:3095... Training loss:2.2353... 0.1931 sec/batch\n",
      "Epoch:11/20... Training Step:3096... Training loss:2.2159... 0.2051 sec/batch\n",
      "Epoch:11/20... Training Step:3097... Training loss:2.1970... 0.1990 sec/batch\n",
      "Epoch:11/20... Training Step:3098... Training loss:2.2378... 0.1929 sec/batch\n",
      "Epoch:11/20... Training Step:3099... Training loss:2.2116... 0.1942 sec/batch\n",
      "Epoch:11/20... Training Step:3100... Training loss:2.2128... 0.2047 sec/batch\n",
      "Epoch:11/20... Training Step:3101... Training loss:2.2288... 0.2029 sec/batch\n",
      "Epoch:11/20... Training Step:3102... Training loss:2.2226... 0.1989 sec/batch\n",
      "Epoch:11/20... Training Step:3103... Training loss:2.2316... 0.2042 sec/batch\n",
      "Epoch:11/20... Training Step:3104... Training loss:2.1915... 0.1935 sec/batch\n",
      "Epoch:11/20... Training Step:3105... Training loss:2.1609... 0.2099 sec/batch\n",
      "Epoch:11/20... Training Step:3106... Training loss:2.2010... 0.1924 sec/batch\n",
      "Epoch:11/20... Training Step:3107... Training loss:2.2036... 0.2053 sec/batch\n",
      "Epoch:11/20... Training Step:3108... Training loss:2.1973... 0.2014 sec/batch\n",
      "Epoch:11/20... Training Step:3109... Training loss:2.1724... 0.2096 sec/batch\n",
      "Epoch:11/20... Training Step:3110... Training loss:2.1767... 0.2043 sec/batch\n",
      "Epoch:11/20... Training Step:3111... Training loss:2.1830... 0.1949 sec/batch\n",
      "Epoch:11/20... Training Step:3112... Training loss:2.1650... 0.1932 sec/batch\n",
      "Epoch:11/20... Training Step:3113... Training loss:2.1644... 0.1973 sec/batch\n",
      "Epoch:11/20... Training Step:3114... Training loss:2.2222... 0.1934 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11/20... Training Step:3115... Training loss:2.1593... 0.2005 sec/batch\n",
      "Epoch:11/20... Training Step:3116... Training loss:2.1642... 0.2030 sec/batch\n",
      "Epoch:11/20... Training Step:3117... Training loss:2.1694... 0.1927 sec/batch\n",
      "Epoch:11/20... Training Step:3118... Training loss:2.1601... 0.2095 sec/batch\n",
      "Epoch:11/20... Training Step:3119... Training loss:2.1788... 0.1950 sec/batch\n",
      "Epoch:11/20... Training Step:3120... Training loss:2.1958... 0.2016 sec/batch\n",
      "Epoch:11/20... Training Step:3121... Training loss:2.2086... 0.2033 sec/batch\n",
      "Epoch:11/20... Training Step:3122... Training loss:2.2183... 0.1916 sec/batch\n",
      "Epoch:11/20... Training Step:3123... Training loss:2.1924... 0.2102 sec/batch\n",
      "Epoch:11/20... Training Step:3124... Training loss:2.1715... 0.2011 sec/batch\n",
      "Epoch:11/20... Training Step:3125... Training loss:2.2088... 0.2045 sec/batch\n",
      "Epoch:11/20... Training Step:3126... Training loss:2.1760... 0.1924 sec/batch\n",
      "Epoch:11/20... Training Step:3127... Training loss:2.1801... 0.1953 sec/batch\n",
      "Epoch:11/20... Training Step:3128... Training loss:2.1869... 0.2003 sec/batch\n",
      "Epoch:11/20... Training Step:3129... Training loss:2.1921... 0.2017 sec/batch\n",
      "Epoch:11/20... Training Step:3130... Training loss:2.1734... 0.2000 sec/batch\n",
      "Epoch:11/20... Training Step:3131... Training loss:2.1632... 0.1920 sec/batch\n",
      "Epoch:11/20... Training Step:3132... Training loss:2.2010... 0.1991 sec/batch\n",
      "Epoch:11/20... Training Step:3133... Training loss:2.1740... 0.1917 sec/batch\n",
      "Epoch:11/20... Training Step:3134... Training loss:2.1854... 0.2048 sec/batch\n",
      "Epoch:11/20... Training Step:3135... Training loss:2.1938... 0.1964 sec/batch\n",
      "Epoch:11/20... Training Step:3136... Training loss:2.2071... 0.1927 sec/batch\n",
      "Epoch:11/20... Training Step:3137... Training loss:2.1690... 0.2066 sec/batch\n",
      "Epoch:11/20... Training Step:3138... Training loss:2.2020... 0.1925 sec/batch\n",
      "Epoch:11/20... Training Step:3139... Training loss:2.1523... 0.1941 sec/batch\n",
      "Epoch:11/20... Training Step:3140... Training loss:2.1736... 0.1970 sec/batch\n",
      "Epoch:11/20... Training Step:3141... Training loss:2.1821... 0.1974 sec/batch\n",
      "Epoch:11/20... Training Step:3142... Training loss:2.2165... 0.2064 sec/batch\n",
      "Epoch:11/20... Training Step:3143... Training loss:2.2272... 0.1917 sec/batch\n",
      "Epoch:11/20... Training Step:3144... Training loss:2.1840... 0.1928 sec/batch\n",
      "Epoch:11/20... Training Step:3145... Training loss:2.2125... 0.1960 sec/batch\n",
      "Epoch:11/20... Training Step:3146... Training loss:2.2115... 0.1941 sec/batch\n",
      "Epoch:11/20... Training Step:3147... Training loss:2.1942... 0.1949 sec/batch\n",
      "Epoch:11/20... Training Step:3148... Training loss:2.2012... 0.1922 sec/batch\n",
      "Epoch:11/20... Training Step:3149... Training loss:2.1689... 0.1935 sec/batch\n",
      "Epoch:11/20... Training Step:3150... Training loss:2.1990... 0.1945 sec/batch\n",
      "Epoch:11/20... Training Step:3151... Training loss:2.1980... 0.1999 sec/batch\n",
      "Epoch:11/20... Training Step:3152... Training loss:2.2280... 0.2063 sec/batch\n",
      "Epoch:11/20... Training Step:3153... Training loss:2.1747... 0.2041 sec/batch\n",
      "Epoch:11/20... Training Step:3154... Training loss:2.1962... 0.2106 sec/batch\n",
      "Epoch:11/20... Training Step:3155... Training loss:2.2295... 0.1922 sec/batch\n",
      "Epoch:11/20... Training Step:3156... Training loss:2.2001... 0.1947 sec/batch\n",
      "Epoch:11/20... Training Step:3157... Training loss:2.1756... 0.2020 sec/batch\n",
      "Epoch:11/20... Training Step:3158... Training loss:2.2180... 0.1939 sec/batch\n",
      "Epoch:11/20... Training Step:3159... Training loss:2.2025... 0.1951 sec/batch\n",
      "Epoch:11/20... Training Step:3160... Training loss:2.2095... 0.1958 sec/batch\n",
      "Epoch:11/20... Training Step:3161... Training loss:2.2217... 0.2175 sec/batch\n",
      "Epoch:11/20... Training Step:3162... Training loss:2.1939... 0.1960 sec/batch\n",
      "Epoch:11/20... Training Step:3163... Training loss:2.1689... 0.1935 sec/batch\n",
      "Epoch:11/20... Training Step:3164... Training loss:2.1507... 0.2040 sec/batch\n",
      "Epoch:11/20... Training Step:3165... Training loss:2.1672... 0.1973 sec/batch\n",
      "Epoch:11/20... Training Step:3166... Training loss:2.1661... 0.1958 sec/batch\n",
      "Epoch:11/20... Training Step:3167... Training loss:2.2189... 0.1911 sec/batch\n",
      "Epoch:11/20... Training Step:3168... Training loss:2.1775... 0.2058 sec/batch\n",
      "Epoch:11/20... Training Step:3169... Training loss:2.1628... 0.1905 sec/batch\n",
      "Epoch:11/20... Training Step:3170... Training loss:2.1699... 0.1950 sec/batch\n",
      "Epoch:11/20... Training Step:3171... Training loss:2.1671... 0.1977 sec/batch\n",
      "Epoch:11/20... Training Step:3172... Training loss:2.2014... 0.1917 sec/batch\n",
      "Epoch:11/20... Training Step:3173... Training loss:2.2257... 0.1924 sec/batch\n",
      "Epoch:11/20... Training Step:3174... Training loss:2.1560... 0.1966 sec/batch\n",
      "Epoch:11/20... Training Step:3175... Training loss:2.1887... 0.1976 sec/batch\n",
      "Epoch:11/20... Training Step:3176... Training loss:2.1981... 0.2078 sec/batch\n",
      "Epoch:11/20... Training Step:3177... Training loss:2.1958... 0.1924 sec/batch\n",
      "Epoch:11/20... Training Step:3178... Training loss:2.2066... 0.1938 sec/batch\n",
      "Epoch:11/20... Training Step:3179... Training loss:2.1827... 0.2163 sec/batch\n",
      "Epoch:11/20... Training Step:3180... Training loss:2.1978... 0.1933 sec/batch\n",
      "Epoch:11/20... Training Step:3181... Training loss:2.1852... 0.1928 sec/batch\n",
      "Epoch:11/20... Training Step:3182... Training loss:2.1940... 0.1929 sec/batch\n",
      "Epoch:11/20... Training Step:3183... Training loss:2.1961... 0.1944 sec/batch\n",
      "Epoch:11/20... Training Step:3184... Training loss:2.2152... 0.1915 sec/batch\n",
      "Epoch:11/20... Training Step:3185... Training loss:2.2363... 0.2099 sec/batch\n",
      "Epoch:11/20... Training Step:3186... Training loss:2.2125... 0.2041 sec/batch\n",
      "Epoch:11/20... Training Step:3187... Training loss:2.1790... 0.1915 sec/batch\n",
      "Epoch:11/20... Training Step:3188... Training loss:2.1858... 0.2000 sec/batch\n",
      "Epoch:11/20... Training Step:3189... Training loss:2.1726... 0.1922 sec/batch\n",
      "Epoch:11/20... Training Step:3190... Training loss:2.1742... 0.1932 sec/batch\n",
      "Epoch:12/20... Training Step:3191... Training loss:2.3215... 0.1913 sec/batch\n",
      "Epoch:12/20... Training Step:3192... Training loss:2.1903... 0.1956 sec/batch\n",
      "Epoch:12/20... Training Step:3193... Training loss:2.1922... 0.1923 sec/batch\n",
      "Epoch:12/20... Training Step:3194... Training loss:2.2168... 0.2058 sec/batch\n",
      "Epoch:12/20... Training Step:3195... Training loss:2.1954... 0.2082 sec/batch\n",
      "Epoch:12/20... Training Step:3196... Training loss:2.1998... 0.1998 sec/batch\n",
      "Epoch:12/20... Training Step:3197... Training loss:2.1852... 0.2053 sec/batch\n",
      "Epoch:12/20... Training Step:3198... Training loss:2.2028... 0.1923 sec/batch\n",
      "Epoch:12/20... Training Step:3199... Training loss:2.1850... 0.1933 sec/batch\n",
      "Epoch:12/20... Training Step:3200... Training loss:2.1863... 0.1924 sec/batch\n",
      "Epoch:12/20... Training Step:3201... Training loss:2.2199... 0.1922 sec/batch\n",
      "Epoch:12/20... Training Step:3202... Training loss:2.2037... 0.1916 sec/batch\n",
      "Epoch:12/20... Training Step:3203... Training loss:2.2204... 0.2041 sec/batch\n",
      "Epoch:12/20... Training Step:3204... Training loss:2.2124... 0.1907 sec/batch\n",
      "Epoch:12/20... Training Step:3205... Training loss:2.1817... 0.2015 sec/batch\n",
      "Epoch:12/20... Training Step:3206... Training loss:2.2058... 0.2028 sec/batch\n",
      "Epoch:12/20... Training Step:3207... Training loss:2.2018... 0.2046 sec/batch\n",
      "Epoch:12/20... Training Step:3208... Training loss:2.2180... 0.1936 sec/batch\n",
      "Epoch:12/20... Training Step:3209... Training loss:2.2179... 0.1933 sec/batch\n",
      "Epoch:12/20... Training Step:3210... Training loss:2.2206... 0.1917 sec/batch\n",
      "Epoch:12/20... Training Step:3211... Training loss:2.2171... 0.1961 sec/batch\n",
      "Epoch:12/20... Training Step:3212... Training loss:2.1977... 0.2120 sec/batch\n",
      "Epoch:12/20... Training Step:3213... Training loss:2.1908... 0.1929 sec/batch\n",
      "Epoch:12/20... Training Step:3214... Training loss:2.2335... 0.1936 sec/batch\n",
      "Epoch:12/20... Training Step:3215... Training loss:2.1835... 0.2065 sec/batch\n",
      "Epoch:12/20... Training Step:3216... Training loss:2.1912... 0.2148 sec/batch\n",
      "Epoch:12/20... Training Step:3217... Training loss:2.2150... 0.1933 sec/batch\n",
      "Epoch:12/20... Training Step:3218... Training loss:2.2113... 0.2137 sec/batch\n",
      "Epoch:12/20... Training Step:3219... Training loss:2.2205... 0.2003 sec/batch\n",
      "Epoch:12/20... Training Step:3220... Training loss:2.2265... 0.1949 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12/20... Training Step:3221... Training loss:2.2028... 0.1924 sec/batch\n",
      "Epoch:12/20... Training Step:3222... Training loss:2.1821... 0.2050 sec/batch\n",
      "Epoch:12/20... Training Step:3223... Training loss:2.1860... 0.1921 sec/batch\n",
      "Epoch:12/20... Training Step:3224... Training loss:2.1957... 0.2169 sec/batch\n",
      "Epoch:12/20... Training Step:3225... Training loss:2.2105... 0.1956 sec/batch\n",
      "Epoch:12/20... Training Step:3226... Training loss:2.2031... 0.1945 sec/batch\n",
      "Epoch:12/20... Training Step:3227... Training loss:2.2164... 0.2028 sec/batch\n",
      "Epoch:12/20... Training Step:3228... Training loss:2.2029... 0.2088 sec/batch\n",
      "Epoch:12/20... Training Step:3229... Training loss:2.1910... 0.2079 sec/batch\n",
      "Epoch:12/20... Training Step:3230... Training loss:2.1543... 0.1955 sec/batch\n",
      "Epoch:12/20... Training Step:3231... Training loss:2.1828... 0.2021 sec/batch\n",
      "Epoch:12/20... Training Step:3232... Training loss:2.1977... 0.1932 sec/batch\n",
      "Epoch:12/20... Training Step:3233... Training loss:2.2170... 0.2093 sec/batch\n",
      "Epoch:12/20... Training Step:3234... Training loss:2.2003... 0.1979 sec/batch\n",
      "Epoch:12/20... Training Step:3235... Training loss:2.2240... 0.2064 sec/batch\n",
      "Epoch:12/20... Training Step:3236... Training loss:2.2184... 0.1948 sec/batch\n",
      "Epoch:12/20... Training Step:3237... Training loss:2.2241... 0.2062 sec/batch\n",
      "Epoch:12/20... Training Step:3238... Training loss:2.1971... 0.1951 sec/batch\n",
      "Epoch:12/20... Training Step:3239... Training loss:2.1573... 0.2135 sec/batch\n",
      "Epoch:12/20... Training Step:3240... Training loss:2.2244... 0.2148 sec/batch\n",
      "Epoch:12/20... Training Step:3241... Training loss:2.1810... 0.1919 sec/batch\n",
      "Epoch:12/20... Training Step:3242... Training loss:2.1837... 0.2091 sec/batch\n",
      "Epoch:12/20... Training Step:3243... Training loss:2.1884... 0.1948 sec/batch\n",
      "Epoch:12/20... Training Step:3244... Training loss:2.1496... 0.2022 sec/batch\n",
      "Epoch:12/20... Training Step:3245... Training loss:2.1709... 0.2193 sec/batch\n",
      "Epoch:12/20... Training Step:3246... Training loss:2.1833... 0.1927 sec/batch\n",
      "Epoch:12/20... Training Step:3247... Training loss:2.1645... 0.1956 sec/batch\n",
      "Epoch:12/20... Training Step:3248... Training loss:2.1800... 0.1984 sec/batch\n",
      "Epoch:12/20... Training Step:3249... Training loss:2.1972... 0.1920 sec/batch\n",
      "Epoch:12/20... Training Step:3250... Training loss:2.1911... 0.1982 sec/batch\n",
      "Epoch:12/20... Training Step:3251... Training loss:2.1806... 0.1998 sec/batch\n",
      "Epoch:12/20... Training Step:3252... Training loss:2.1847... 0.2088 sec/batch\n",
      "Epoch:12/20... Training Step:3253... Training loss:2.1727... 0.1989 sec/batch\n",
      "Epoch:12/20... Training Step:3254... Training loss:2.1916... 0.1950 sec/batch\n",
      "Epoch:12/20... Training Step:3255... Training loss:2.1781... 0.2040 sec/batch\n",
      "Epoch:12/20... Training Step:3256... Training loss:2.1413... 0.1930 sec/batch\n",
      "Epoch:12/20... Training Step:3257... Training loss:2.1392... 0.1994 sec/batch\n",
      "Epoch:12/20... Training Step:3258... Training loss:2.1671... 0.2018 sec/batch\n",
      "Epoch:12/20... Training Step:3259... Training loss:2.1724... 0.1925 sec/batch\n",
      "Epoch:12/20... Training Step:3260... Training loss:2.1290... 0.1948 sec/batch\n",
      "Epoch:12/20... Training Step:3261... Training loss:2.1363... 0.1930 sec/batch\n",
      "Epoch:12/20... Training Step:3262... Training loss:2.1571... 0.1946 sec/batch\n",
      "Epoch:12/20... Training Step:3263... Training loss:2.1794... 0.2015 sec/batch\n",
      "Epoch:12/20... Training Step:3264... Training loss:2.1481... 0.2014 sec/batch\n",
      "Epoch:12/20... Training Step:3265... Training loss:2.1119... 0.2002 sec/batch\n",
      "Epoch:12/20... Training Step:3266... Training loss:2.1436... 0.2152 sec/batch\n",
      "Epoch:12/20... Training Step:3267... Training loss:2.1427... 0.1980 sec/batch\n",
      "Epoch:12/20... Training Step:3268... Training loss:2.1442... 0.2074 sec/batch\n",
      "Epoch:12/20... Training Step:3269... Training loss:2.1682... 0.1923 sec/batch\n",
      "Epoch:12/20... Training Step:3270... Training loss:2.1476... 0.2145 sec/batch\n",
      "Epoch:12/20... Training Step:3271... Training loss:2.1267... 0.2127 sec/batch\n",
      "Epoch:12/20... Training Step:3272... Training loss:2.1418... 0.2070 sec/batch\n",
      "Epoch:12/20... Training Step:3273... Training loss:2.1591... 0.2120 sec/batch\n",
      "Epoch:12/20... Training Step:3274... Training loss:2.1931... 0.2055 sec/batch\n",
      "Epoch:12/20... Training Step:3275... Training loss:2.1847... 0.1920 sec/batch\n",
      "Epoch:12/20... Training Step:3276... Training loss:2.1451... 0.2114 sec/batch\n",
      "Epoch:12/20... Training Step:3277... Training loss:2.1697... 0.2155 sec/batch\n",
      "Epoch:12/20... Training Step:3278... Training loss:2.1729... 0.1992 sec/batch\n",
      "Epoch:12/20... Training Step:3279... Training loss:2.1642... 0.2065 sec/batch\n",
      "Epoch:12/20... Training Step:3280... Training loss:2.1760... 0.1926 sec/batch\n",
      "Epoch:12/20... Training Step:3281... Training loss:2.1733... 0.1930 sec/batch\n",
      "Epoch:12/20... Training Step:3282... Training loss:2.1741... 0.1982 sec/batch\n",
      "Epoch:12/20... Training Step:3283... Training loss:2.1963... 0.2007 sec/batch\n",
      "Epoch:12/20... Training Step:3284... Training loss:2.1731... 0.2172 sec/batch\n",
      "Epoch:12/20... Training Step:3285... Training loss:2.1386... 0.1946 sec/batch\n",
      "Epoch:12/20... Training Step:3286... Training loss:2.1690... 0.1944 sec/batch\n",
      "Epoch:12/20... Training Step:3287... Training loss:2.1702... 0.1989 sec/batch\n",
      "Epoch:12/20... Training Step:3288... Training loss:2.2005... 0.1951 sec/batch\n",
      "Epoch:12/20... Training Step:3289... Training loss:2.1474... 0.1966 sec/batch\n",
      "Epoch:12/20... Training Step:3290... Training loss:2.1826... 0.1932 sec/batch\n",
      "Epoch:12/20... Training Step:3291... Training loss:2.1729... 0.2023 sec/batch\n",
      "Epoch:12/20... Training Step:3292... Training loss:2.1805... 0.1931 sec/batch\n",
      "Epoch:12/20... Training Step:3293... Training loss:2.1930... 0.2071 sec/batch\n",
      "Epoch:12/20... Training Step:3294... Training loss:2.1741... 0.1929 sec/batch\n",
      "Epoch:12/20... Training Step:3295... Training loss:2.2229... 0.1948 sec/batch\n",
      "Epoch:12/20... Training Step:3296... Training loss:2.1890... 0.1933 sec/batch\n",
      "Epoch:12/20... Training Step:3297... Training loss:2.2169... 0.1914 sec/batch\n",
      "Epoch:12/20... Training Step:3298... Training loss:2.1783... 0.1999 sec/batch\n",
      "Epoch:12/20... Training Step:3299... Training loss:2.1788... 0.2070 sec/batch\n",
      "Epoch:12/20... Training Step:3300... Training loss:2.1814... 0.1953 sec/batch\n",
      "Epoch:12/20... Training Step:3301... Training loss:2.1718... 0.1938 sec/batch\n",
      "Epoch:12/20... Training Step:3302... Training loss:2.1389... 0.1918 sec/batch\n",
      "Epoch:12/20... Training Step:3303... Training loss:2.1315... 0.1980 sec/batch\n",
      "Epoch:12/20... Training Step:3304... Training loss:2.1807... 0.1917 sec/batch\n",
      "Epoch:12/20... Training Step:3305... Training loss:2.1656... 0.1932 sec/batch\n",
      "Epoch:12/20... Training Step:3306... Training loss:2.1933... 0.2199 sec/batch\n",
      "Epoch:12/20... Training Step:3307... Training loss:2.1946... 0.1923 sec/batch\n",
      "Epoch:12/20... Training Step:3308... Training loss:2.1707... 0.2164 sec/batch\n",
      "Epoch:12/20... Training Step:3309... Training loss:2.1882... 0.1949 sec/batch\n",
      "Epoch:12/20... Training Step:3310... Training loss:2.1820... 0.1998 sec/batch\n",
      "Epoch:12/20... Training Step:3311... Training loss:2.1986... 0.2036 sec/batch\n",
      "Epoch:12/20... Training Step:3312... Training loss:2.2038... 0.1910 sec/batch\n",
      "Epoch:12/20... Training Step:3313... Training loss:2.1608... 0.2081 sec/batch\n",
      "Epoch:12/20... Training Step:3314... Training loss:2.2023... 0.1919 sec/batch\n",
      "Epoch:12/20... Training Step:3315... Training loss:2.1668... 0.1966 sec/batch\n",
      "Epoch:12/20... Training Step:3316... Training loss:2.1978... 0.1921 sec/batch\n",
      "Epoch:12/20... Training Step:3317... Training loss:2.2084... 0.1965 sec/batch\n",
      "Epoch:12/20... Training Step:3318... Training loss:2.2238... 0.2096 sec/batch\n",
      "Epoch:12/20... Training Step:3319... Training loss:2.2184... 0.1936 sec/batch\n",
      "Epoch:12/20... Training Step:3320... Training loss:2.2573... 0.1969 sec/batch\n",
      "Epoch:12/20... Training Step:3321... Training loss:2.2100... 0.2060 sec/batch\n",
      "Epoch:12/20... Training Step:3322... Training loss:2.2052... 0.1930 sec/batch\n",
      "Epoch:12/20... Training Step:3323... Training loss:2.2228... 0.1991 sec/batch\n",
      "Epoch:12/20... Training Step:3324... Training loss:2.2259... 0.1928 sec/batch\n",
      "Epoch:12/20... Training Step:3325... Training loss:2.1947... 0.1931 sec/batch\n",
      "Epoch:12/20... Training Step:3326... Training loss:2.1833... 0.1923 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12/20... Training Step:3327... Training loss:2.1941... 0.2009 sec/batch\n",
      "Epoch:12/20... Training Step:3328... Training loss:2.1922... 0.1917 sec/batch\n",
      "Epoch:12/20... Training Step:3329... Training loss:2.1883... 0.1946 sec/batch\n",
      "Epoch:12/20... Training Step:3330... Training loss:2.1831... 0.2036 sec/batch\n",
      "Epoch:12/20... Training Step:3331... Training loss:2.1803... 0.1917 sec/batch\n",
      "Epoch:12/20... Training Step:3332... Training loss:2.2455... 0.1962 sec/batch\n",
      "Epoch:12/20... Training Step:3333... Training loss:2.1954... 0.2089 sec/batch\n",
      "Epoch:12/20... Training Step:3334... Training loss:2.1699... 0.2105 sec/batch\n",
      "Epoch:12/20... Training Step:3335... Training loss:2.1850... 0.2076 sec/batch\n",
      "Epoch:12/20... Training Step:3336... Training loss:2.1901... 0.1921 sec/batch\n",
      "Epoch:12/20... Training Step:3337... Training loss:2.1562... 0.2012 sec/batch\n",
      "Epoch:12/20... Training Step:3338... Training loss:2.1631... 0.1985 sec/batch\n",
      "Epoch:12/20... Training Step:3339... Training loss:2.1825... 0.2000 sec/batch\n",
      "Epoch:12/20... Training Step:3340... Training loss:2.2056... 0.1941 sec/batch\n",
      "Epoch:12/20... Training Step:3341... Training loss:2.1578... 0.1929 sec/batch\n",
      "Epoch:12/20... Training Step:3342... Training loss:2.1609... 0.1932 sec/batch\n",
      "Epoch:12/20... Training Step:3343... Training loss:2.1905... 0.1944 sec/batch\n",
      "Epoch:12/20... Training Step:3344... Training loss:2.1595... 0.2141 sec/batch\n",
      "Epoch:12/20... Training Step:3345... Training loss:2.1703... 0.2122 sec/batch\n",
      "Epoch:12/20... Training Step:3346... Training loss:2.1988... 0.1925 sec/batch\n",
      "Epoch:12/20... Training Step:3347... Training loss:2.1799... 0.1939 sec/batch\n",
      "Epoch:12/20... Training Step:3348... Training loss:2.1659... 0.1921 sec/batch\n",
      "Epoch:12/20... Training Step:3349... Training loss:2.1518... 0.1938 sec/batch\n",
      "Epoch:12/20... Training Step:3350... Training loss:2.1548... 0.1969 sec/batch\n",
      "Epoch:12/20... Training Step:3351... Training loss:2.1508... 0.1964 sec/batch\n",
      "Epoch:12/20... Training Step:3352... Training loss:2.1596... 0.1950 sec/batch\n",
      "Epoch:12/20... Training Step:3353... Training loss:2.1433... 0.1940 sec/batch\n",
      "Epoch:12/20... Training Step:3354... Training loss:2.1994... 0.2044 sec/batch\n",
      "Epoch:12/20... Training Step:3355... Training loss:2.1913... 0.2070 sec/batch\n",
      "Epoch:12/20... Training Step:3356... Training loss:2.2227... 0.1914 sec/batch\n",
      "Epoch:12/20... Training Step:3357... Training loss:2.2208... 0.2148 sec/batch\n",
      "Epoch:12/20... Training Step:3358... Training loss:2.2154... 0.2130 sec/batch\n",
      "Epoch:12/20... Training Step:3359... Training loss:2.1625... 0.1964 sec/batch\n",
      "Epoch:12/20... Training Step:3360... Training loss:2.1552... 0.1926 sec/batch\n",
      "Epoch:12/20... Training Step:3361... Training loss:2.1713... 0.1956 sec/batch\n",
      "Epoch:12/20... Training Step:3362... Training loss:2.1545... 0.2119 sec/batch\n",
      "Epoch:12/20... Training Step:3363... Training loss:2.1483... 0.2147 sec/batch\n",
      "Epoch:12/20... Training Step:3364... Training loss:2.1869... 0.1955 sec/batch\n",
      "Epoch:12/20... Training Step:3365... Training loss:2.1761... 0.1944 sec/batch\n",
      "Epoch:12/20... Training Step:3366... Training loss:2.1731... 0.1941 sec/batch\n",
      "Epoch:12/20... Training Step:3367... Training loss:2.1505... 0.2066 sec/batch\n",
      "Epoch:12/20... Training Step:3368... Training loss:2.1565... 0.2102 sec/batch\n",
      "Epoch:12/20... Training Step:3369... Training loss:2.1423... 0.2085 sec/batch\n",
      "Epoch:12/20... Training Step:3370... Training loss:2.1829... 0.1965 sec/batch\n",
      "Epoch:12/20... Training Step:3371... Training loss:2.1691... 0.1915 sec/batch\n",
      "Epoch:12/20... Training Step:3372... Training loss:2.1943... 0.1922 sec/batch\n",
      "Epoch:12/20... Training Step:3373... Training loss:2.1885... 0.2022 sec/batch\n",
      "Epoch:12/20... Training Step:3374... Training loss:2.1664... 0.2060 sec/batch\n",
      "Epoch:12/20... Training Step:3375... Training loss:2.1878... 0.2067 sec/batch\n",
      "Epoch:12/20... Training Step:3376... Training loss:2.1559... 0.1984 sec/batch\n",
      "Epoch:12/20... Training Step:3377... Training loss:2.1929... 0.1970 sec/batch\n",
      "Epoch:12/20... Training Step:3378... Training loss:2.1913... 0.1933 sec/batch\n",
      "Epoch:12/20... Training Step:3379... Training loss:2.1816... 0.1910 sec/batch\n",
      "Epoch:12/20... Training Step:3380... Training loss:2.1887... 0.1974 sec/batch\n",
      "Epoch:12/20... Training Step:3381... Training loss:2.1560... 0.2036 sec/batch\n",
      "Epoch:12/20... Training Step:3382... Training loss:2.1897... 0.1923 sec/batch\n",
      "Epoch:12/20... Training Step:3383... Training loss:2.2098... 0.1939 sec/batch\n",
      "Epoch:12/20... Training Step:3384... Training loss:2.1850... 0.1926 sec/batch\n",
      "Epoch:12/20... Training Step:3385... Training loss:2.1955... 0.1932 sec/batch\n",
      "Epoch:12/20... Training Step:3386... Training loss:2.1920... 0.2036 sec/batch\n",
      "Epoch:12/20... Training Step:3387... Training loss:2.1626... 0.1924 sec/batch\n",
      "Epoch:12/20... Training Step:3388... Training loss:2.1953... 0.2070 sec/batch\n",
      "Epoch:12/20... Training Step:3389... Training loss:2.1759... 0.1930 sec/batch\n",
      "Epoch:12/20... Training Step:3390... Training loss:2.1749... 0.2144 sec/batch\n",
      "Epoch:12/20... Training Step:3391... Training loss:2.1937... 0.1948 sec/batch\n",
      "Epoch:12/20... Training Step:3392... Training loss:2.1947... 0.1946 sec/batch\n",
      "Epoch:12/20... Training Step:3393... Training loss:2.2093... 0.1937 sec/batch\n",
      "Epoch:12/20... Training Step:3394... Training loss:2.1590... 0.2047 sec/batch\n",
      "Epoch:12/20... Training Step:3395... Training loss:2.1329... 0.1998 sec/batch\n",
      "Epoch:12/20... Training Step:3396... Training loss:2.1698... 0.2042 sec/batch\n",
      "Epoch:12/20... Training Step:3397... Training loss:2.1522... 0.1929 sec/batch\n",
      "Epoch:12/20... Training Step:3398... Training loss:2.1664... 0.1943 sec/batch\n",
      "Epoch:12/20... Training Step:3399... Training loss:2.1369... 0.2040 sec/batch\n",
      "Epoch:12/20... Training Step:3400... Training loss:2.1388... 0.1939 sec/batch\n",
      "Epoch:12/20... Training Step:3401... Training loss:2.1471... 0.1926 sec/batch\n",
      "Epoch:12/20... Training Step:3402... Training loss:2.1219... 0.1997 sec/batch\n",
      "Epoch:12/20... Training Step:3403... Training loss:2.1174... 0.2049 sec/batch\n",
      "Epoch:12/20... Training Step:3404... Training loss:2.1975... 0.2108 sec/batch\n",
      "Epoch:12/20... Training Step:3405... Training loss:2.1312... 0.2061 sec/batch\n",
      "Epoch:12/20... Training Step:3406... Training loss:2.1282... 0.1964 sec/batch\n",
      "Epoch:12/20... Training Step:3407... Training loss:2.1374... 0.2030 sec/batch\n",
      "Epoch:12/20... Training Step:3408... Training loss:2.1240... 0.1928 sec/batch\n",
      "Epoch:12/20... Training Step:3409... Training loss:2.1636... 0.1968 sec/batch\n",
      "Epoch:12/20... Training Step:3410... Training loss:2.1628... 0.1931 sec/batch\n",
      "Epoch:12/20... Training Step:3411... Training loss:2.1779... 0.2045 sec/batch\n",
      "Epoch:12/20... Training Step:3412... Training loss:2.1918... 0.2042 sec/batch\n",
      "Epoch:12/20... Training Step:3413... Training loss:2.1616... 0.1919 sec/batch\n",
      "Epoch:12/20... Training Step:3414... Training loss:2.1390... 0.2061 sec/batch\n",
      "Epoch:12/20... Training Step:3415... Training loss:2.1789... 0.1930 sec/batch\n",
      "Epoch:12/20... Training Step:3416... Training loss:2.1424... 0.1983 sec/batch\n",
      "Epoch:12/20... Training Step:3417... Training loss:2.1440... 0.2042 sec/batch\n",
      "Epoch:12/20... Training Step:3418... Training loss:2.1556... 0.1938 sec/batch\n",
      "Epoch:12/20... Training Step:3419... Training loss:2.1582... 0.2107 sec/batch\n",
      "Epoch:12/20... Training Step:3420... Training loss:2.1415... 0.1931 sec/batch\n",
      "Epoch:12/20... Training Step:3421... Training loss:2.1232... 0.1940 sec/batch\n",
      "Epoch:12/20... Training Step:3422... Training loss:2.1726... 0.2128 sec/batch\n",
      "Epoch:12/20... Training Step:3423... Training loss:2.1397... 0.1916 sec/batch\n",
      "Epoch:12/20... Training Step:3424... Training loss:2.1508... 0.2040 sec/batch\n",
      "Epoch:12/20... Training Step:3425... Training loss:2.1620... 0.1933 sec/batch\n",
      "Epoch:12/20... Training Step:3426... Training loss:2.1666... 0.2064 sec/batch\n",
      "Epoch:12/20... Training Step:3427... Training loss:2.1404... 0.1936 sec/batch\n",
      "Epoch:12/20... Training Step:3428... Training loss:2.1654... 0.1943 sec/batch\n",
      "Epoch:12/20... Training Step:3429... Training loss:2.1213... 0.1976 sec/batch\n",
      "Epoch:12/20... Training Step:3430... Training loss:2.1437... 0.2114 sec/batch\n",
      "Epoch:12/20... Training Step:3431... Training loss:2.1495... 0.1999 sec/batch\n",
      "Epoch:12/20... Training Step:3432... Training loss:2.1841... 0.2070 sec/batch\n",
      "Epoch:12/20... Training Step:3433... Training loss:2.1910... 0.1914 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12/20... Training Step:3434... Training loss:2.1497... 0.2079 sec/batch\n",
      "Epoch:12/20... Training Step:3435... Training loss:2.1744... 0.1948 sec/batch\n",
      "Epoch:12/20... Training Step:3436... Training loss:2.1762... 0.2076 sec/batch\n",
      "Epoch:12/20... Training Step:3437... Training loss:2.1697... 0.1946 sec/batch\n",
      "Epoch:12/20... Training Step:3438... Training loss:2.1531... 0.2099 sec/batch\n",
      "Epoch:12/20... Training Step:3439... Training loss:2.1411... 0.1952 sec/batch\n",
      "Epoch:12/20... Training Step:3440... Training loss:2.1609... 0.1984 sec/batch\n",
      "Epoch:12/20... Training Step:3441... Training loss:2.1569... 0.1958 sec/batch\n",
      "Epoch:12/20... Training Step:3442... Training loss:2.1943... 0.1938 sec/batch\n",
      "Epoch:12/20... Training Step:3443... Training loss:2.1487... 0.1991 sec/batch\n",
      "Epoch:12/20... Training Step:3444... Training loss:2.1587... 0.2103 sec/batch\n",
      "Epoch:12/20... Training Step:3445... Training loss:2.1913... 0.2148 sec/batch\n",
      "Epoch:12/20... Training Step:3446... Training loss:2.1681... 0.1921 sec/batch\n",
      "Epoch:12/20... Training Step:3447... Training loss:2.1417... 0.1935 sec/batch\n",
      "Epoch:12/20... Training Step:3448... Training loss:2.1899... 0.2061 sec/batch\n",
      "Epoch:12/20... Training Step:3449... Training loss:2.1703... 0.2031 sec/batch\n",
      "Epoch:12/20... Training Step:3450... Training loss:2.1777... 0.1927 sec/batch\n",
      "Epoch:12/20... Training Step:3451... Training loss:2.1892... 0.2069 sec/batch\n",
      "Epoch:12/20... Training Step:3452... Training loss:2.1599... 0.1920 sec/batch\n",
      "Epoch:12/20... Training Step:3453... Training loss:2.1377... 0.1952 sec/batch\n",
      "Epoch:12/20... Training Step:3454... Training loss:2.1140... 0.1974 sec/batch\n",
      "Epoch:12/20... Training Step:3455... Training loss:2.1250... 0.2071 sec/batch\n",
      "Epoch:12/20... Training Step:3456... Training loss:2.1264... 0.2104 sec/batch\n",
      "Epoch:12/20... Training Step:3457... Training loss:2.1844... 0.1974 sec/batch\n",
      "Epoch:12/20... Training Step:3458... Training loss:2.1494... 0.2174 sec/batch\n",
      "Epoch:12/20... Training Step:3459... Training loss:2.1210... 0.2016 sec/batch\n",
      "Epoch:12/20... Training Step:3460... Training loss:2.1357... 0.2087 sec/batch\n",
      "Epoch:12/20... Training Step:3461... Training loss:2.1348... 0.1987 sec/batch\n",
      "Epoch:12/20... Training Step:3462... Training loss:2.1768... 0.1966 sec/batch\n",
      "Epoch:12/20... Training Step:3463... Training loss:2.1859... 0.1945 sec/batch\n",
      "Epoch:12/20... Training Step:3464... Training loss:2.1236... 0.1931 sec/batch\n",
      "Epoch:12/20... Training Step:3465... Training loss:2.1682... 0.1956 sec/batch\n",
      "Epoch:12/20... Training Step:3466... Training loss:2.1682... 0.1942 sec/batch\n",
      "Epoch:12/20... Training Step:3467... Training loss:2.1688... 0.1997 sec/batch\n",
      "Epoch:12/20... Training Step:3468... Training loss:2.1754... 0.1999 sec/batch\n",
      "Epoch:12/20... Training Step:3469... Training loss:2.1406... 0.1945 sec/batch\n",
      "Epoch:12/20... Training Step:3470... Training loss:2.1726... 0.1948 sec/batch\n",
      "Epoch:12/20... Training Step:3471... Training loss:2.1604... 0.1906 sec/batch\n",
      "Epoch:12/20... Training Step:3472... Training loss:2.1640... 0.1970 sec/batch\n",
      "Epoch:12/20... Training Step:3473... Training loss:2.1738... 0.1910 sec/batch\n",
      "Epoch:12/20... Training Step:3474... Training loss:2.1806... 0.2078 sec/batch\n",
      "Epoch:12/20... Training Step:3475... Training loss:2.1989... 0.1973 sec/batch\n",
      "Epoch:12/20... Training Step:3476... Training loss:2.1790... 0.2032 sec/batch\n",
      "Epoch:12/20... Training Step:3477... Training loss:2.1524... 0.1924 sec/batch\n",
      "Epoch:12/20... Training Step:3478... Training loss:2.1492... 0.1919 sec/batch\n",
      "Epoch:12/20... Training Step:3479... Training loss:2.1392... 0.1937 sec/batch\n",
      "Epoch:12/20... Training Step:3480... Training loss:2.1495... 0.1952 sec/batch\n",
      "Epoch:13/20... Training Step:3481... Training loss:2.2799... 0.1919 sec/batch\n",
      "Epoch:13/20... Training Step:3482... Training loss:2.1630... 0.1928 sec/batch\n",
      "Epoch:13/20... Training Step:3483... Training loss:2.1613... 0.1948 sec/batch\n",
      "Epoch:13/20... Training Step:3484... Training loss:2.1819... 0.1952 sec/batch\n",
      "Epoch:13/20... Training Step:3485... Training loss:2.1638... 0.1928 sec/batch\n",
      "Epoch:13/20... Training Step:3486... Training loss:2.1587... 0.1929 sec/batch\n",
      "Epoch:13/20... Training Step:3487... Training loss:2.1490... 0.1989 sec/batch\n",
      "Epoch:13/20... Training Step:3488... Training loss:2.1590... 0.2109 sec/batch\n",
      "Epoch:13/20... Training Step:3489... Training loss:2.1569... 0.1935 sec/batch\n",
      "Epoch:13/20... Training Step:3490... Training loss:2.1557... 0.1933 sec/batch\n",
      "Epoch:13/20... Training Step:3491... Training loss:2.1817... 0.1999 sec/batch\n",
      "Epoch:13/20... Training Step:3492... Training loss:2.1788... 0.2132 sec/batch\n",
      "Epoch:13/20... Training Step:3493... Training loss:2.1765... 0.1972 sec/batch\n",
      "Epoch:13/20... Training Step:3494... Training loss:2.1740... 0.2017 sec/batch\n",
      "Epoch:13/20... Training Step:3495... Training loss:2.1547... 0.1932 sec/batch\n",
      "Epoch:13/20... Training Step:3496... Training loss:2.1745... 0.2160 sec/batch\n",
      "Epoch:13/20... Training Step:3497... Training loss:2.1746... 0.1925 sec/batch\n",
      "Epoch:13/20... Training Step:3498... Training loss:2.1865... 0.2034 sec/batch\n",
      "Epoch:13/20... Training Step:3499... Training loss:2.1780... 0.1947 sec/batch\n",
      "Epoch:13/20... Training Step:3500... Training loss:2.1801... 0.2024 sec/batch\n",
      "Epoch:13/20... Training Step:3501... Training loss:2.1669... 0.1915 sec/batch\n",
      "Epoch:13/20... Training Step:3502... Training loss:2.1701... 0.1958 sec/batch\n",
      "Epoch:13/20... Training Step:3503... Training loss:2.1621... 0.2081 sec/batch\n",
      "Epoch:13/20... Training Step:3504... Training loss:2.2001... 0.2041 sec/batch\n",
      "Epoch:13/20... Training Step:3505... Training loss:2.1451... 0.1925 sec/batch\n",
      "Epoch:13/20... Training Step:3506... Training loss:2.1686... 0.1926 sec/batch\n",
      "Epoch:13/20... Training Step:3507... Training loss:2.1804... 0.1981 sec/batch\n",
      "Epoch:13/20... Training Step:3508... Training loss:2.1841... 0.2038 sec/batch\n",
      "Epoch:13/20... Training Step:3509... Training loss:2.1841... 0.1925 sec/batch\n",
      "Epoch:13/20... Training Step:3510... Training loss:2.1913... 0.1949 sec/batch\n",
      "Epoch:13/20... Training Step:3511... Training loss:2.1772... 0.1951 sec/batch\n",
      "Epoch:13/20... Training Step:3512... Training loss:2.1517... 0.1919 sec/batch\n",
      "Epoch:13/20... Training Step:3513... Training loss:2.1518... 0.1926 sec/batch\n",
      "Epoch:13/20... Training Step:3514... Training loss:2.1652... 0.1927 sec/batch\n",
      "Epoch:13/20... Training Step:3515... Training loss:2.1737... 0.1915 sec/batch\n",
      "Epoch:13/20... Training Step:3516... Training loss:2.1545... 0.1988 sec/batch\n",
      "Epoch:13/20... Training Step:3517... Training loss:2.1884... 0.2092 sec/batch\n",
      "Epoch:13/20... Training Step:3518... Training loss:2.1712... 0.2038 sec/batch\n",
      "Epoch:13/20... Training Step:3519... Training loss:2.1564... 0.1940 sec/batch\n",
      "Epoch:13/20... Training Step:3520... Training loss:2.1151... 0.1943 sec/batch\n",
      "Epoch:13/20... Training Step:3521... Training loss:2.1479... 0.2091 sec/batch\n",
      "Epoch:13/20... Training Step:3522... Training loss:2.1724... 0.2123 sec/batch\n",
      "Epoch:13/20... Training Step:3523... Training loss:2.1827... 0.2015 sec/batch\n",
      "Epoch:13/20... Training Step:3524... Training loss:2.1695... 0.1894 sec/batch\n",
      "Epoch:13/20... Training Step:3525... Training loss:2.1867... 0.1971 sec/batch\n",
      "Epoch:13/20... Training Step:3526... Training loss:2.1909... 0.2031 sec/batch\n",
      "Epoch:13/20... Training Step:3527... Training loss:2.1979... 0.1966 sec/batch\n",
      "Epoch:13/20... Training Step:3528... Training loss:2.1701... 0.1966 sec/batch\n",
      "Epoch:13/20... Training Step:3529... Training loss:2.1233... 0.1974 sec/batch\n",
      "Epoch:13/20... Training Step:3530... Training loss:2.1911... 0.1977 sec/batch\n",
      "Epoch:13/20... Training Step:3531... Training loss:2.1474... 0.1932 sec/batch\n",
      "Epoch:13/20... Training Step:3532... Training loss:2.1592... 0.1982 sec/batch\n",
      "Epoch:13/20... Training Step:3533... Training loss:2.1538... 0.1907 sec/batch\n",
      "Epoch:13/20... Training Step:3534... Training loss:2.1145... 0.1934 sec/batch\n",
      "Epoch:13/20... Training Step:3535... Training loss:2.1328... 0.2052 sec/batch\n",
      "Epoch:13/20... Training Step:3536... Training loss:2.1420... 0.2186 sec/batch\n",
      "Epoch:13/20... Training Step:3537... Training loss:2.1291... 0.2057 sec/batch\n",
      "Epoch:13/20... Training Step:3538... Training loss:2.1432... 0.2188 sec/batch\n",
      "Epoch:13/20... Training Step:3539... Training loss:2.1714... 0.2154 sec/batch\n",
      "Epoch:13/20... Training Step:3540... Training loss:2.1628... 0.1927 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13/20... Training Step:3541... Training loss:2.1554... 0.1941 sec/batch\n",
      "Epoch:13/20... Training Step:3542... Training loss:2.1532... 0.1994 sec/batch\n",
      "Epoch:13/20... Training Step:3543... Training loss:2.1297... 0.2019 sec/batch\n",
      "Epoch:13/20... Training Step:3544... Training loss:2.1471... 0.2064 sec/batch\n",
      "Epoch:13/20... Training Step:3545... Training loss:2.1444... 0.1928 sec/batch\n",
      "Epoch:13/20... Training Step:3546... Training loss:2.1013... 0.1962 sec/batch\n",
      "Epoch:13/20... Training Step:3547... Training loss:2.1060... 0.1916 sec/batch\n",
      "Epoch:13/20... Training Step:3548... Training loss:2.1357... 0.1939 sec/batch\n",
      "Epoch:13/20... Training Step:3549... Training loss:2.1491... 0.1931 sec/batch\n",
      "Epoch:13/20... Training Step:3550... Training loss:2.1042... 0.2086 sec/batch\n",
      "Epoch:13/20... Training Step:3551... Training loss:2.0997... 0.2016 sec/batch\n",
      "Epoch:13/20... Training Step:3552... Training loss:2.1169... 0.1955 sec/batch\n",
      "Epoch:13/20... Training Step:3553... Training loss:2.1381... 0.2051 sec/batch\n",
      "Epoch:13/20... Training Step:3554... Training loss:2.1200... 0.1943 sec/batch\n",
      "Epoch:13/20... Training Step:3555... Training loss:2.0817... 0.1932 sec/batch\n",
      "Epoch:13/20... Training Step:3556... Training loss:2.1162... 0.2119 sec/batch\n",
      "Epoch:13/20... Training Step:3557... Training loss:2.0990... 0.2076 sec/batch\n",
      "Epoch:13/20... Training Step:3558... Training loss:2.1132... 0.2078 sec/batch\n",
      "Epoch:13/20... Training Step:3559... Training loss:2.1340... 0.1946 sec/batch\n",
      "Epoch:13/20... Training Step:3560... Training loss:2.1156... 0.1950 sec/batch\n",
      "Epoch:13/20... Training Step:3561... Training loss:2.0836... 0.2030 sec/batch\n",
      "Epoch:13/20... Training Step:3562... Training loss:2.1080... 0.1997 sec/batch\n",
      "Epoch:13/20... Training Step:3563... Training loss:2.1310... 0.2002 sec/batch\n",
      "Epoch:13/20... Training Step:3564... Training loss:2.1601... 0.1919 sec/batch\n",
      "Epoch:13/20... Training Step:3565... Training loss:2.1571... 0.1938 sec/batch\n",
      "Epoch:13/20... Training Step:3566... Training loss:2.0999... 0.1946 sec/batch\n",
      "Epoch:13/20... Training Step:3567... Training loss:2.1437... 0.1919 sec/batch\n",
      "Epoch:13/20... Training Step:3568... Training loss:2.1422... 0.1931 sec/batch\n",
      "Epoch:13/20... Training Step:3569... Training loss:2.1313... 0.2006 sec/batch\n",
      "Epoch:13/20... Training Step:3570... Training loss:2.1489... 0.2147 sec/batch\n",
      "Epoch:13/20... Training Step:3571... Training loss:2.1413... 0.1918 sec/batch\n",
      "Epoch:13/20... Training Step:3572... Training loss:2.1285... 0.1969 sec/batch\n",
      "Epoch:13/20... Training Step:3573... Training loss:2.1482... 0.2041 sec/batch\n",
      "Epoch:13/20... Training Step:3574... Training loss:2.1319... 0.1922 sec/batch\n",
      "Epoch:13/20... Training Step:3575... Training loss:2.1126... 0.1933 sec/batch\n",
      "Epoch:13/20... Training Step:3576... Training loss:2.1488... 0.1988 sec/batch\n",
      "Epoch:13/20... Training Step:3577... Training loss:2.1417... 0.2044 sec/batch\n",
      "Epoch:13/20... Training Step:3578... Training loss:2.1679... 0.1954 sec/batch\n",
      "Epoch:13/20... Training Step:3579... Training loss:2.1229... 0.2007 sec/batch\n",
      "Epoch:13/20... Training Step:3580... Training loss:2.1607... 0.2123 sec/batch\n",
      "Epoch:13/20... Training Step:3581... Training loss:2.1450... 0.1984 sec/batch\n",
      "Epoch:13/20... Training Step:3582... Training loss:2.1481... 0.2021 sec/batch\n",
      "Epoch:13/20... Training Step:3583... Training loss:2.1643... 0.2084 sec/batch\n",
      "Epoch:13/20... Training Step:3584... Training loss:2.1426... 0.1923 sec/batch\n",
      "Epoch:13/20... Training Step:3585... Training loss:2.1882... 0.2037 sec/batch\n",
      "Epoch:13/20... Training Step:3586... Training loss:2.1599... 0.2025 sec/batch\n",
      "Epoch:13/20... Training Step:3587... Training loss:2.1763... 0.1927 sec/batch\n",
      "Epoch:13/20... Training Step:3588... Training loss:2.1386... 0.2066 sec/batch\n",
      "Epoch:13/20... Training Step:3589... Training loss:2.1547... 0.1921 sec/batch\n",
      "Epoch:13/20... Training Step:3590... Training loss:2.1523... 0.1947 sec/batch\n",
      "Epoch:13/20... Training Step:3591... Training loss:2.1422... 0.2045 sec/batch\n",
      "Epoch:13/20... Training Step:3592... Training loss:2.1098... 0.1955 sec/batch\n",
      "Epoch:13/20... Training Step:3593... Training loss:2.0969... 0.1933 sec/batch\n",
      "Epoch:13/20... Training Step:3594... Training loss:2.1511... 0.1950 sec/batch\n",
      "Epoch:13/20... Training Step:3595... Training loss:2.1265... 0.2000 sec/batch\n",
      "Epoch:13/20... Training Step:3596... Training loss:2.1625... 0.1921 sec/batch\n",
      "Epoch:13/20... Training Step:3597... Training loss:2.1616... 0.1951 sec/batch\n",
      "Epoch:13/20... Training Step:3598... Training loss:2.1383... 0.1957 sec/batch\n",
      "Epoch:13/20... Training Step:3599... Training loss:2.1609... 0.1991 sec/batch\n",
      "Epoch:13/20... Training Step:3600... Training loss:2.1594... 0.2101 sec/batch\n",
      "Epoch:13/20... Training Step:3601... Training loss:2.1520... 0.1918 sec/batch\n",
      "Epoch:13/20... Training Step:3602... Training loss:2.1701... 0.2017 sec/batch\n",
      "Epoch:13/20... Training Step:3603... Training loss:2.1285... 0.1935 sec/batch\n",
      "Epoch:13/20... Training Step:3604... Training loss:2.1761... 0.1933 sec/batch\n",
      "Epoch:13/20... Training Step:3605... Training loss:2.1487... 0.1915 sec/batch\n",
      "Epoch:13/20... Training Step:3606... Training loss:2.1642... 0.2063 sec/batch\n",
      "Epoch:13/20... Training Step:3607... Training loss:2.1777... 0.1983 sec/batch\n",
      "Epoch:13/20... Training Step:3608... Training loss:2.1838... 0.2005 sec/batch\n",
      "Epoch:13/20... Training Step:3609... Training loss:2.1862... 0.1933 sec/batch\n",
      "Epoch:13/20... Training Step:3610... Training loss:2.2260... 0.1929 sec/batch\n",
      "Epoch:13/20... Training Step:3611... Training loss:2.1767... 0.1920 sec/batch\n",
      "Epoch:13/20... Training Step:3612... Training loss:2.1696... 0.2158 sec/batch\n",
      "Epoch:13/20... Training Step:3613... Training loss:2.1905... 0.1933 sec/batch\n",
      "Epoch:13/20... Training Step:3614... Training loss:2.1995... 0.1938 sec/batch\n",
      "Epoch:13/20... Training Step:3615... Training loss:2.1700... 0.1954 sec/batch\n",
      "Epoch:13/20... Training Step:3616... Training loss:2.1474... 0.1957 sec/batch\n",
      "Epoch:13/20... Training Step:3617... Training loss:2.1528... 0.1975 sec/batch\n",
      "Epoch:13/20... Training Step:3618... Training loss:2.1561... 0.2021 sec/batch\n",
      "Epoch:13/20... Training Step:3619... Training loss:2.1607... 0.1924 sec/batch\n",
      "Epoch:13/20... Training Step:3620... Training loss:2.1508... 0.1984 sec/batch\n",
      "Epoch:13/20... Training Step:3621... Training loss:2.1504... 0.1955 sec/batch\n",
      "Epoch:13/20... Training Step:3622... Training loss:2.2118... 0.1941 sec/batch\n",
      "Epoch:13/20... Training Step:3623... Training loss:2.1653... 0.1924 sec/batch\n",
      "Epoch:13/20... Training Step:3624... Training loss:2.1277... 0.2077 sec/batch\n",
      "Epoch:13/20... Training Step:3625... Training loss:2.1450... 0.1967 sec/batch\n",
      "Epoch:13/20... Training Step:3626... Training loss:2.1661... 0.1936 sec/batch\n",
      "Epoch:13/20... Training Step:3627... Training loss:2.1316... 0.1914 sec/batch\n",
      "Epoch:13/20... Training Step:3628... Training loss:2.1399... 0.1983 sec/batch\n",
      "Epoch:13/20... Training Step:3629... Training loss:2.1398... 0.1920 sec/batch\n",
      "Epoch:13/20... Training Step:3630... Training loss:2.1635... 0.1934 sec/batch\n",
      "Epoch:13/20... Training Step:3631... Training loss:2.1202... 0.2172 sec/batch\n",
      "Epoch:13/20... Training Step:3632... Training loss:2.1342... 0.2089 sec/batch\n",
      "Epoch:13/20... Training Step:3633... Training loss:2.1526... 0.2176 sec/batch\n",
      "Epoch:13/20... Training Step:3634... Training loss:2.1196... 0.1925 sec/batch\n",
      "Epoch:13/20... Training Step:3635... Training loss:2.1399... 0.1929 sec/batch\n",
      "Epoch:13/20... Training Step:3636... Training loss:2.1727... 0.1918 sec/batch\n",
      "Epoch:13/20... Training Step:3637... Training loss:2.1489... 0.1934 sec/batch\n",
      "Epoch:13/20... Training Step:3638... Training loss:2.1361... 0.2012 sec/batch\n",
      "Epoch:13/20... Training Step:3639... Training loss:2.1299... 0.1927 sec/batch\n",
      "Epoch:13/20... Training Step:3640... Training loss:2.1270... 0.2010 sec/batch\n",
      "Epoch:13/20... Training Step:3641... Training loss:2.1155... 0.2065 sec/batch\n",
      "Epoch:13/20... Training Step:3642... Training loss:2.1262... 0.2062 sec/batch\n",
      "Epoch:13/20... Training Step:3643... Training loss:2.1137... 0.1966 sec/batch\n",
      "Epoch:13/20... Training Step:3644... Training loss:2.1757... 0.2011 sec/batch\n",
      "Epoch:13/20... Training Step:3645... Training loss:2.1538... 0.2095 sec/batch\n",
      "Epoch:13/20... Training Step:3646... Training loss:2.1883... 0.1934 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13/20... Training Step:3647... Training loss:2.1959... 0.1991 sec/batch\n",
      "Epoch:13/20... Training Step:3648... Training loss:2.1833... 0.2016 sec/batch\n",
      "Epoch:13/20... Training Step:3649... Training loss:2.1375... 0.1971 sec/batch\n",
      "Epoch:13/20... Training Step:3650... Training loss:2.1268... 0.1971 sec/batch\n",
      "Epoch:13/20... Training Step:3651... Training loss:2.1458... 0.2080 sec/batch\n",
      "Epoch:13/20... Training Step:3652... Training loss:2.1298... 0.1979 sec/batch\n",
      "Epoch:13/20... Training Step:3653... Training loss:2.1267... 0.2035 sec/batch\n",
      "Epoch:13/20... Training Step:3654... Training loss:2.1536... 0.1992 sec/batch\n",
      "Epoch:13/20... Training Step:3655... Training loss:2.1443... 0.2040 sec/batch\n",
      "Epoch:13/20... Training Step:3656... Training loss:2.1408... 0.1926 sec/batch\n",
      "Epoch:13/20... Training Step:3657... Training loss:2.1218... 0.1919 sec/batch\n",
      "Epoch:13/20... Training Step:3658... Training loss:2.1292... 0.2228 sec/batch\n",
      "Epoch:13/20... Training Step:3659... Training loss:2.1146... 0.2142 sec/batch\n",
      "Epoch:13/20... Training Step:3660... Training loss:2.1555... 0.2023 sec/batch\n",
      "Epoch:13/20... Training Step:3661... Training loss:2.1411... 0.2621 sec/batch\n",
      "Epoch:13/20... Training Step:3662... Training loss:2.1583... 0.1999 sec/batch\n",
      "Epoch:13/20... Training Step:3663... Training loss:2.1475... 0.1949 sec/batch\n",
      "Epoch:13/20... Training Step:3664... Training loss:2.1322... 0.1943 sec/batch\n",
      "Epoch:13/20... Training Step:3665... Training loss:2.1443... 0.1978 sec/batch\n",
      "Epoch:13/20... Training Step:3666... Training loss:2.1223... 0.2597 sec/batch\n",
      "Epoch:13/20... Training Step:3667... Training loss:2.1637... 0.2027 sec/batch\n",
      "Epoch:13/20... Training Step:3668... Training loss:2.1580... 0.1922 sec/batch\n",
      "Epoch:13/20... Training Step:3669... Training loss:2.1532... 0.1973 sec/batch\n",
      "Epoch:13/20... Training Step:3670... Training loss:2.1487... 0.2032 sec/batch\n",
      "Epoch:13/20... Training Step:3671... Training loss:2.1214... 0.2019 sec/batch\n",
      "Epoch:13/20... Training Step:3672... Training loss:2.1591... 0.1919 sec/batch\n",
      "Epoch:13/20... Training Step:3673... Training loss:2.1866... 0.2101 sec/batch\n",
      "Epoch:13/20... Training Step:3674... Training loss:2.1490... 0.2014 sec/batch\n",
      "Epoch:13/20... Training Step:3675... Training loss:2.1648... 0.1932 sec/batch\n",
      "Epoch:13/20... Training Step:3676... Training loss:2.1573... 0.1926 sec/batch\n",
      "Epoch:13/20... Training Step:3677... Training loss:2.1357... 0.1959 sec/batch\n",
      "Epoch:13/20... Training Step:3678... Training loss:2.1616... 0.1937 sec/batch\n",
      "Epoch:13/20... Training Step:3679... Training loss:2.1456... 0.2117 sec/batch\n",
      "Epoch:13/20... Training Step:3680... Training loss:2.1431... 0.2019 sec/batch\n",
      "Epoch:13/20... Training Step:3681... Training loss:2.1621... 0.1924 sec/batch\n",
      "Epoch:13/20... Training Step:3682... Training loss:2.1549... 0.1935 sec/batch\n",
      "Epoch:13/20... Training Step:3683... Training loss:2.1706... 0.2018 sec/batch\n",
      "Epoch:13/20... Training Step:3684... Training loss:2.1294... 0.2085 sec/batch\n",
      "Epoch:13/20... Training Step:3685... Training loss:2.0980... 0.2017 sec/batch\n",
      "Epoch:13/20... Training Step:3686... Training loss:2.1368... 0.2088 sec/batch\n",
      "Epoch:13/20... Training Step:3687... Training loss:2.1215... 0.1925 sec/batch\n",
      "Epoch:13/20... Training Step:3688... Training loss:2.1257... 0.1953 sec/batch\n",
      "Epoch:13/20... Training Step:3689... Training loss:2.1182... 0.1953 sec/batch\n",
      "Epoch:13/20... Training Step:3690... Training loss:2.1103... 0.2023 sec/batch\n",
      "Epoch:13/20... Training Step:3691... Training loss:2.1192... 0.1933 sec/batch\n",
      "Epoch:13/20... Training Step:3692... Training loss:2.1026... 0.2060 sec/batch\n",
      "Epoch:13/20... Training Step:3693... Training loss:2.0872... 0.1918 sec/batch\n",
      "Epoch:13/20... Training Step:3694... Training loss:2.1558... 0.1967 sec/batch\n",
      "Epoch:13/20... Training Step:3695... Training loss:2.0851... 0.2015 sec/batch\n",
      "Epoch:13/20... Training Step:3696... Training loss:2.0939... 0.1944 sec/batch\n",
      "Epoch:13/20... Training Step:3697... Training loss:2.0964... 0.1992 sec/batch\n",
      "Epoch:13/20... Training Step:3698... Training loss:2.0860... 0.2032 sec/batch\n",
      "Epoch:13/20... Training Step:3699... Training loss:2.1219... 0.1935 sec/batch\n",
      "Epoch:13/20... Training Step:3700... Training loss:2.1252... 0.1940 sec/batch\n",
      "Epoch:13/20... Training Step:3701... Training loss:2.1453... 0.2001 sec/batch\n",
      "Epoch:13/20... Training Step:3702... Training loss:2.1644... 0.1943 sec/batch\n",
      "Epoch:13/20... Training Step:3703... Training loss:2.1321... 0.1925 sec/batch\n",
      "Epoch:13/20... Training Step:3704... Training loss:2.1028... 0.1928 sec/batch\n",
      "Epoch:13/20... Training Step:3705... Training loss:2.1420... 0.2011 sec/batch\n",
      "Epoch:13/20... Training Step:3706... Training loss:2.1007... 0.1927 sec/batch\n",
      "Epoch:13/20... Training Step:3707... Training loss:2.1100... 0.2019 sec/batch\n",
      "Epoch:13/20... Training Step:3708... Training loss:2.1257... 0.1919 sec/batch\n",
      "Epoch:13/20... Training Step:3709... Training loss:2.1304... 0.2063 sec/batch\n",
      "Epoch:13/20... Training Step:3710... Training loss:2.1064... 0.1933 sec/batch\n",
      "Epoch:13/20... Training Step:3711... Training loss:2.0938... 0.2035 sec/batch\n",
      "Epoch:13/20... Training Step:3712... Training loss:2.1382... 0.2003 sec/batch\n",
      "Epoch:13/20... Training Step:3713... Training loss:2.0986... 0.2188 sec/batch\n",
      "Epoch:13/20... Training Step:3714... Training loss:2.1105... 0.1929 sec/batch\n",
      "Epoch:13/20... Training Step:3715... Training loss:2.1298... 0.2037 sec/batch\n",
      "Epoch:13/20... Training Step:3716... Training loss:2.1383... 0.2051 sec/batch\n",
      "Epoch:13/20... Training Step:3717... Training loss:2.1084... 0.2027 sec/batch\n",
      "Epoch:13/20... Training Step:3718... Training loss:2.1339... 0.1983 sec/batch\n",
      "Epoch:13/20... Training Step:3719... Training loss:2.0876... 0.1933 sec/batch\n",
      "Epoch:13/20... Training Step:3720... Training loss:2.1197... 0.1950 sec/batch\n",
      "Epoch:13/20... Training Step:3721... Training loss:2.1113... 0.2014 sec/batch\n",
      "Epoch:13/20... Training Step:3722... Training loss:2.1485... 0.1960 sec/batch\n",
      "Epoch:13/20... Training Step:3723... Training loss:2.1738... 0.2026 sec/batch\n",
      "Epoch:13/20... Training Step:3724... Training loss:2.1262... 0.1991 sec/batch\n",
      "Epoch:13/20... Training Step:3725... Training loss:2.1484... 0.1972 sec/batch\n",
      "Epoch:13/20... Training Step:3726... Training loss:2.1503... 0.1920 sec/batch\n",
      "Epoch:13/20... Training Step:3727... Training loss:2.1385... 0.1922 sec/batch\n",
      "Epoch:13/20... Training Step:3728... Training loss:2.1322... 0.1946 sec/batch\n",
      "Epoch:13/20... Training Step:3729... Training loss:2.1151... 0.1919 sec/batch\n",
      "Epoch:13/20... Training Step:3730... Training loss:2.1317... 0.2055 sec/batch\n",
      "Epoch:13/20... Training Step:3731... Training loss:2.1198... 0.2000 sec/batch\n",
      "Epoch:13/20... Training Step:3732... Training loss:2.1541... 0.1936 sec/batch\n",
      "Epoch:13/20... Training Step:3733... Training loss:2.1180... 0.1948 sec/batch\n",
      "Epoch:13/20... Training Step:3734... Training loss:2.1234... 0.1918 sec/batch\n",
      "Epoch:13/20... Training Step:3735... Training loss:2.1558... 0.1958 sec/batch\n",
      "Epoch:13/20... Training Step:3736... Training loss:2.1406... 0.1928 sec/batch\n",
      "Epoch:13/20... Training Step:3737... Training loss:2.1100... 0.1926 sec/batch\n",
      "Epoch:13/20... Training Step:3738... Training loss:2.1505... 0.2184 sec/batch\n",
      "Epoch:13/20... Training Step:3739... Training loss:2.1349... 0.2071 sec/batch\n",
      "Epoch:13/20... Training Step:3740... Training loss:2.1480... 0.1924 sec/batch\n",
      "Epoch:13/20... Training Step:3741... Training loss:2.1608... 0.2060 sec/batch\n",
      "Epoch:13/20... Training Step:3742... Training loss:2.1245... 0.2083 sec/batch\n",
      "Epoch:13/20... Training Step:3743... Training loss:2.0952... 0.1921 sec/batch\n",
      "Epoch:13/20... Training Step:3744... Training loss:2.0796... 0.2074 sec/batch\n",
      "Epoch:13/20... Training Step:3745... Training loss:2.0914... 0.1924 sec/batch\n",
      "Epoch:13/20... Training Step:3746... Training loss:2.1041... 0.2017 sec/batch\n",
      "Epoch:13/20... Training Step:3747... Training loss:2.1460... 0.1985 sec/batch\n",
      "Epoch:13/20... Training Step:3748... Training loss:2.1071... 0.2104 sec/batch\n",
      "Epoch:13/20... Training Step:3749... Training loss:2.1029... 0.2070 sec/batch\n",
      "Epoch:13/20... Training Step:3750... Training loss:2.1085... 0.1911 sec/batch\n",
      "Epoch:13/20... Training Step:3751... Training loss:2.0991... 0.2017 sec/batch\n",
      "Epoch:13/20... Training Step:3752... Training loss:2.1316... 0.1917 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13/20... Training Step:3753... Training loss:2.1553... 0.1942 sec/batch\n",
      "Epoch:13/20... Training Step:3754... Training loss:2.0961... 0.1943 sec/batch\n",
      "Epoch:13/20... Training Step:3755... Training loss:2.1440... 0.2088 sec/batch\n",
      "Epoch:13/20... Training Step:3756... Training loss:2.1409... 0.1910 sec/batch\n",
      "Epoch:13/20... Training Step:3757... Training loss:2.1330... 0.2060 sec/batch\n",
      "Epoch:13/20... Training Step:3758... Training loss:2.1319... 0.2078 sec/batch\n",
      "Epoch:13/20... Training Step:3759... Training loss:2.1065... 0.1922 sec/batch\n",
      "Epoch:13/20... Training Step:3760... Training loss:2.1401... 0.1937 sec/batch\n",
      "Epoch:13/20... Training Step:3761... Training loss:2.1258... 0.1932 sec/batch\n",
      "Epoch:13/20... Training Step:3762... Training loss:2.1339... 0.1922 sec/batch\n",
      "Epoch:13/20... Training Step:3763... Training loss:2.1439... 0.1917 sec/batch\n",
      "Epoch:13/20... Training Step:3764... Training loss:2.1620... 0.1944 sec/batch\n",
      "Epoch:13/20... Training Step:3765... Training loss:2.1736... 0.1930 sec/batch\n",
      "Epoch:13/20... Training Step:3766... Training loss:2.1456... 0.1999 sec/batch\n",
      "Epoch:13/20... Training Step:3767... Training loss:2.1133... 0.2096 sec/batch\n",
      "Epoch:13/20... Training Step:3768... Training loss:2.1156... 0.2020 sec/batch\n",
      "Epoch:13/20... Training Step:3769... Training loss:2.1009... 0.1920 sec/batch\n",
      "Epoch:13/20... Training Step:3770... Training loss:2.1082... 0.1927 sec/batch\n",
      "Epoch:14/20... Training Step:3771... Training loss:2.2450... 0.1922 sec/batch\n",
      "Epoch:14/20... Training Step:3772... Training loss:2.1250... 0.1974 sec/batch\n",
      "Epoch:14/20... Training Step:3773... Training loss:2.1292... 0.2115 sec/batch\n",
      "Epoch:14/20... Training Step:3774... Training loss:2.1560... 0.1915 sec/batch\n",
      "Epoch:14/20... Training Step:3775... Training loss:2.1319... 0.2076 sec/batch\n",
      "Epoch:14/20... Training Step:3776... Training loss:2.1334... 0.1922 sec/batch\n",
      "Epoch:14/20... Training Step:3777... Training loss:2.1226... 0.1940 sec/batch\n",
      "Epoch:14/20... Training Step:3778... Training loss:2.1322... 0.2077 sec/batch\n",
      "Epoch:14/20... Training Step:3779... Training loss:2.1294... 0.2034 sec/batch\n",
      "Epoch:14/20... Training Step:3780... Training loss:2.1283... 0.2110 sec/batch\n",
      "Epoch:14/20... Training Step:3781... Training loss:2.1551... 0.1915 sec/batch\n",
      "Epoch:14/20... Training Step:3782... Training loss:2.1594... 0.1959 sec/batch\n",
      "Epoch:14/20... Training Step:3783... Training loss:2.1529... 0.1934 sec/batch\n",
      "Epoch:14/20... Training Step:3784... Training loss:2.1479... 0.1937 sec/batch\n",
      "Epoch:14/20... Training Step:3785... Training loss:2.1155... 0.1949 sec/batch\n",
      "Epoch:14/20... Training Step:3786... Training loss:2.1478... 0.1956 sec/batch\n",
      "Epoch:14/20... Training Step:3787... Training loss:2.1485... 0.1973 sec/batch\n",
      "Epoch:14/20... Training Step:3788... Training loss:2.1495... 0.1938 sec/batch\n",
      "Epoch:14/20... Training Step:3789... Training loss:2.1546... 0.2004 sec/batch\n",
      "Epoch:14/20... Training Step:3790... Training loss:2.1540... 0.2094 sec/batch\n",
      "Epoch:14/20... Training Step:3791... Training loss:2.1478... 0.2029 sec/batch\n",
      "Epoch:14/20... Training Step:3792... Training loss:2.1246... 0.2091 sec/batch\n",
      "Epoch:14/20... Training Step:3793... Training loss:2.1219... 0.2067 sec/batch\n",
      "Epoch:14/20... Training Step:3794... Training loss:2.1612... 0.2023 sec/batch\n",
      "Epoch:14/20... Training Step:3795... Training loss:2.1139... 0.2145 sec/batch\n",
      "Epoch:14/20... Training Step:3796... Training loss:2.1182... 0.2028 sec/batch\n",
      "Epoch:14/20... Training Step:3797... Training loss:2.1407... 0.2045 sec/batch\n",
      "Epoch:14/20... Training Step:3798... Training loss:2.1522... 0.2578 sec/batch\n",
      "Epoch:14/20... Training Step:3799... Training loss:2.1530... 0.2026 sec/batch\n",
      "Epoch:14/20... Training Step:3800... Training loss:2.1578... 0.1925 sec/batch\n",
      "Epoch:14/20... Training Step:3801... Training loss:2.1403... 0.1998 sec/batch\n",
      "Epoch:14/20... Training Step:3802... Training loss:2.1149... 0.2029 sec/batch\n",
      "Epoch:14/20... Training Step:3803... Training loss:2.1275... 0.1920 sec/batch\n",
      "Epoch:14/20... Training Step:3804... Training loss:2.1425... 0.2071 sec/batch\n",
      "Epoch:14/20... Training Step:3805... Training loss:2.1512... 0.2037 sec/batch\n",
      "Epoch:14/20... Training Step:3806... Training loss:2.1255... 0.2170 sec/batch\n",
      "Epoch:14/20... Training Step:3807... Training loss:2.1419... 0.2342 sec/batch\n",
      "Epoch:14/20... Training Step:3808... Training loss:2.1426... 0.1992 sec/batch\n",
      "Epoch:14/20... Training Step:3809... Training loss:2.1241... 0.1930 sec/batch\n",
      "Epoch:14/20... Training Step:3810... Training loss:2.0930... 0.1924 sec/batch\n",
      "Epoch:14/20... Training Step:3811... Training loss:2.1103... 0.2204 sec/batch\n",
      "Epoch:14/20... Training Step:3812... Training loss:2.1249... 0.1991 sec/batch\n",
      "Epoch:14/20... Training Step:3813... Training loss:2.1650... 0.2056 sec/batch\n",
      "Epoch:14/20... Training Step:3814... Training loss:2.1276... 0.2107 sec/batch\n",
      "Epoch:14/20... Training Step:3815... Training loss:2.1513... 0.2146 sec/batch\n",
      "Epoch:14/20... Training Step:3816... Training loss:2.1567... 0.1929 sec/batch\n",
      "Epoch:14/20... Training Step:3817... Training loss:2.1520... 0.1960 sec/batch\n",
      "Epoch:14/20... Training Step:3818... Training loss:2.1272... 0.1949 sec/batch\n",
      "Epoch:14/20... Training Step:3819... Training loss:2.0913... 0.1930 sec/batch\n",
      "Epoch:14/20... Training Step:3820... Training loss:2.1624... 0.1947 sec/batch\n",
      "Epoch:14/20... Training Step:3821... Training loss:2.1089... 0.2012 sec/batch\n",
      "Epoch:14/20... Training Step:3822... Training loss:2.1162... 0.2015 sec/batch\n",
      "Epoch:14/20... Training Step:3823... Training loss:2.1339... 0.2032 sec/batch\n",
      "Epoch:14/20... Training Step:3824... Training loss:2.0796... 0.2358 sec/batch\n",
      "Epoch:14/20... Training Step:3825... Training loss:2.1121... 0.1924 sec/batch\n",
      "Epoch:14/20... Training Step:3826... Training loss:2.1137... 0.1965 sec/batch\n",
      "Epoch:14/20... Training Step:3827... Training loss:2.1081... 0.1924 sec/batch\n",
      "Epoch:14/20... Training Step:3828... Training loss:2.1190... 0.1994 sec/batch\n",
      "Epoch:14/20... Training Step:3829... Training loss:2.1367... 0.1921 sec/batch\n",
      "Epoch:14/20... Training Step:3830... Training loss:2.1275... 0.1939 sec/batch\n",
      "Epoch:14/20... Training Step:3831... Training loss:2.1291... 0.2069 sec/batch\n",
      "Epoch:14/20... Training Step:3832... Training loss:2.1227... 0.2011 sec/batch\n",
      "Epoch:14/20... Training Step:3833... Training loss:2.0955... 0.1927 sec/batch\n",
      "Epoch:14/20... Training Step:3834... Training loss:2.1240... 0.1967 sec/batch\n",
      "Epoch:14/20... Training Step:3835... Training loss:2.1223... 0.1918 sec/batch\n",
      "Epoch:14/20... Training Step:3836... Training loss:2.0723... 0.2041 sec/batch\n",
      "Epoch:14/20... Training Step:3837... Training loss:2.0810... 0.1941 sec/batch\n",
      "Epoch:14/20... Training Step:3838... Training loss:2.1037... 0.2125 sec/batch\n",
      "Epoch:14/20... Training Step:3839... Training loss:2.1148... 0.2048 sec/batch\n",
      "Epoch:14/20... Training Step:3840... Training loss:2.0735... 0.2146 sec/batch\n",
      "Epoch:14/20... Training Step:3841... Training loss:2.0754... 0.1959 sec/batch\n",
      "Epoch:14/20... Training Step:3842... Training loss:2.0831... 0.1963 sec/batch\n",
      "Epoch:14/20... Training Step:3843... Training loss:2.1096... 0.1985 sec/batch\n",
      "Epoch:14/20... Training Step:3844... Training loss:2.0882... 0.2011 sec/batch\n",
      "Epoch:14/20... Training Step:3845... Training loss:2.0544... 0.1938 sec/batch\n",
      "Epoch:14/20... Training Step:3846... Training loss:2.0763... 0.1923 sec/batch\n",
      "Epoch:14/20... Training Step:3847... Training loss:2.0778... 0.2097 sec/batch\n",
      "Epoch:14/20... Training Step:3848... Training loss:2.0768... 0.2168 sec/batch\n",
      "Epoch:14/20... Training Step:3849... Training loss:2.1020... 0.1982 sec/batch\n",
      "Epoch:14/20... Training Step:3850... Training loss:2.0815... 0.1965 sec/batch\n",
      "Epoch:14/20... Training Step:3851... Training loss:2.0558... 0.1954 sec/batch\n",
      "Epoch:14/20... Training Step:3852... Training loss:2.0736... 0.1931 sec/batch\n",
      "Epoch:14/20... Training Step:3853... Training loss:2.1019... 0.1959 sec/batch\n",
      "Epoch:14/20... Training Step:3854... Training loss:2.1251... 0.1988 sec/batch\n",
      "Epoch:14/20... Training Step:3855... Training loss:2.1222... 0.1951 sec/batch\n",
      "Epoch:14/20... Training Step:3856... Training loss:2.0876... 0.1951 sec/batch\n",
      "Epoch:14/20... Training Step:3857... Training loss:2.1025... 0.1966 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14/20... Training Step:3858... Training loss:2.1134... 0.1982 sec/batch\n",
      "Epoch:14/20... Training Step:3859... Training loss:2.0985... 0.2132 sec/batch\n",
      "Epoch:14/20... Training Step:3860... Training loss:2.1139... 0.1991 sec/batch\n",
      "Epoch:14/20... Training Step:3861... Training loss:2.1091... 0.1987 sec/batch\n",
      "Epoch:14/20... Training Step:3862... Training loss:2.1067... 0.1955 sec/batch\n",
      "Epoch:14/20... Training Step:3863... Training loss:2.1222... 0.1922 sec/batch\n",
      "Epoch:14/20... Training Step:3864... Training loss:2.1080... 0.1932 sec/batch\n",
      "Epoch:14/20... Training Step:3865... Training loss:2.0897... 0.2107 sec/batch\n",
      "Epoch:14/20... Training Step:3866... Training loss:2.1232... 0.1983 sec/batch\n",
      "Epoch:14/20... Training Step:3867... Training loss:2.1137... 0.2028 sec/batch\n",
      "Epoch:14/20... Training Step:3868... Training loss:2.1383... 0.1936 sec/batch\n",
      "Epoch:14/20... Training Step:3869... Training loss:2.0930... 0.1951 sec/batch\n",
      "Epoch:14/20... Training Step:3870... Training loss:2.1253... 0.1933 sec/batch\n",
      "Epoch:14/20... Training Step:3871... Training loss:2.1181... 0.2034 sec/batch\n",
      "Epoch:14/20... Training Step:3872... Training loss:2.1174... 0.1936 sec/batch\n",
      "Epoch:14/20... Training Step:3873... Training loss:2.1282... 0.2009 sec/batch\n",
      "Epoch:14/20... Training Step:3874... Training loss:2.1113... 0.1918 sec/batch\n",
      "Epoch:14/20... Training Step:3875... Training loss:2.1535... 0.1963 sec/batch\n",
      "Epoch:14/20... Training Step:3876... Training loss:2.1290... 0.1914 sec/batch\n",
      "Epoch:14/20... Training Step:3877... Training loss:2.1497... 0.2012 sec/batch\n",
      "Epoch:14/20... Training Step:3878... Training loss:2.1010... 0.1969 sec/batch\n",
      "Epoch:14/20... Training Step:3879... Training loss:2.1186... 0.1945 sec/batch\n",
      "Epoch:14/20... Training Step:3880... Training loss:2.1205... 0.2046 sec/batch\n",
      "Epoch:14/20... Training Step:3881... Training loss:2.1105... 0.1932 sec/batch\n",
      "Epoch:14/20... Training Step:3882... Training loss:2.0711... 0.2004 sec/batch\n",
      "Epoch:14/20... Training Step:3883... Training loss:2.0710... 0.1988 sec/batch\n",
      "Epoch:14/20... Training Step:3884... Training loss:2.1192... 0.2042 sec/batch\n",
      "Epoch:14/20... Training Step:3885... Training loss:2.0888... 0.2128 sec/batch\n",
      "Epoch:14/20... Training Step:3886... Training loss:2.1354... 0.1986 sec/batch\n",
      "Epoch:14/20... Training Step:3887... Training loss:2.1343... 0.2184 sec/batch\n",
      "Epoch:14/20... Training Step:3888... Training loss:2.1126... 0.2034 sec/batch\n",
      "Epoch:14/20... Training Step:3889... Training loss:2.1244... 0.1945 sec/batch\n",
      "Epoch:14/20... Training Step:3890... Training loss:2.1236... 0.2080 sec/batch\n",
      "Epoch:14/20... Training Step:3891... Training loss:2.1194... 0.1931 sec/batch\n",
      "Epoch:14/20... Training Step:3892... Training loss:2.1450... 0.2068 sec/batch\n",
      "Epoch:14/20... Training Step:3893... Training loss:2.1092... 0.1980 sec/batch\n",
      "Epoch:14/20... Training Step:3894... Training loss:2.1472... 0.1977 sec/batch\n",
      "Epoch:14/20... Training Step:3895... Training loss:2.1051... 0.2114 sec/batch\n",
      "Epoch:14/20... Training Step:3896... Training loss:2.1334... 0.1944 sec/batch\n",
      "Epoch:14/20... Training Step:3897... Training loss:2.1469... 0.1924 sec/batch\n",
      "Epoch:14/20... Training Step:3898... Training loss:2.1587... 0.1963 sec/batch\n",
      "Epoch:14/20... Training Step:3899... Training loss:2.1614... 0.1952 sec/batch\n",
      "Epoch:14/20... Training Step:3900... Training loss:2.1969... 0.2054 sec/batch\n",
      "Epoch:14/20... Training Step:3901... Training loss:2.1459... 0.1916 sec/batch\n",
      "Epoch:14/20... Training Step:3902... Training loss:2.1356... 0.1922 sec/batch\n",
      "Epoch:14/20... Training Step:3903... Training loss:2.1555... 0.2013 sec/batch\n",
      "Epoch:14/20... Training Step:3904... Training loss:2.1691... 0.1931 sec/batch\n",
      "Epoch:14/20... Training Step:3905... Training loss:2.1399... 0.2059 sec/batch\n",
      "Epoch:14/20... Training Step:3906... Training loss:2.1238... 0.2017 sec/batch\n",
      "Epoch:14/20... Training Step:3907... Training loss:2.1209... 0.1918 sec/batch\n",
      "Epoch:14/20... Training Step:3908... Training loss:2.1271... 0.1955 sec/batch\n",
      "Epoch:14/20... Training Step:3909... Training loss:2.1252... 0.2118 sec/batch\n",
      "Epoch:14/20... Training Step:3910... Training loss:2.1197... 0.1934 sec/batch\n",
      "Epoch:14/20... Training Step:3911... Training loss:2.1160... 0.1939 sec/batch\n",
      "Epoch:14/20... Training Step:3912... Training loss:2.1903... 0.2065 sec/batch\n",
      "Epoch:14/20... Training Step:3913... Training loss:2.1344... 0.1914 sec/batch\n",
      "Epoch:14/20... Training Step:3914... Training loss:2.1102... 0.1970 sec/batch\n",
      "Epoch:14/20... Training Step:3915... Training loss:2.1230... 0.1917 sec/batch\n",
      "Epoch:14/20... Training Step:3916... Training loss:2.1400... 0.1945 sec/batch\n",
      "Epoch:14/20... Training Step:3917... Training loss:2.0966... 0.2115 sec/batch\n",
      "Epoch:14/20... Training Step:3918... Training loss:2.1130... 0.1968 sec/batch\n",
      "Epoch:14/20... Training Step:3919... Training loss:2.1049... 0.1949 sec/batch\n",
      "Epoch:14/20... Training Step:3920... Training loss:2.1230... 0.1939 sec/batch\n",
      "Epoch:14/20... Training Step:3921... Training loss:2.0890... 0.1922 sec/batch\n",
      "Epoch:14/20... Training Step:3922... Training loss:2.1032... 0.1985 sec/batch\n",
      "Epoch:14/20... Training Step:3923... Training loss:2.1325... 0.1921 sec/batch\n",
      "Epoch:14/20... Training Step:3924... Training loss:2.1003... 0.1952 sec/batch\n",
      "Epoch:14/20... Training Step:3925... Training loss:2.1051... 0.2142 sec/batch\n",
      "Epoch:14/20... Training Step:3926... Training loss:2.1476... 0.2024 sec/batch\n",
      "Epoch:14/20... Training Step:3927... Training loss:2.1214... 0.1912 sec/batch\n",
      "Epoch:14/20... Training Step:3928... Training loss:2.1077... 0.1920 sec/batch\n",
      "Epoch:14/20... Training Step:3929... Training loss:2.1004... 0.2026 sec/batch\n",
      "Epoch:14/20... Training Step:3930... Training loss:2.1002... 0.1966 sec/batch\n",
      "Epoch:14/20... Training Step:3931... Training loss:2.0908... 0.1973 sec/batch\n",
      "Epoch:14/20... Training Step:3932... Training loss:2.1003... 0.1944 sec/batch\n",
      "Epoch:14/20... Training Step:3933... Training loss:2.0831... 0.2042 sec/batch\n",
      "Epoch:14/20... Training Step:3934... Training loss:2.1417... 0.1915 sec/batch\n",
      "Epoch:14/20... Training Step:3935... Training loss:2.1369... 0.2057 sec/batch\n",
      "Epoch:14/20... Training Step:3936... Training loss:2.1579... 0.1926 sec/batch\n",
      "Epoch:14/20... Training Step:3937... Training loss:2.1596... 0.2081 sec/batch\n",
      "Epoch:14/20... Training Step:3938... Training loss:2.1519... 0.1917 sec/batch\n",
      "Epoch:14/20... Training Step:3939... Training loss:2.0962... 0.2056 sec/batch\n",
      "Epoch:14/20... Training Step:3940... Training loss:2.1012... 0.2019 sec/batch\n",
      "Epoch:14/20... Training Step:3941... Training loss:2.1246... 0.1997 sec/batch\n",
      "Epoch:14/20... Training Step:3942... Training loss:2.0975... 0.1957 sec/batch\n",
      "Epoch:14/20... Training Step:3943... Training loss:2.0980... 0.2040 sec/batch\n",
      "Epoch:14/20... Training Step:3944... Training loss:2.1236... 0.1958 sec/batch\n",
      "Epoch:14/20... Training Step:3945... Training loss:2.1180... 0.1941 sec/batch\n",
      "Epoch:14/20... Training Step:3946... Training loss:2.1148... 0.1927 sec/batch\n",
      "Epoch:14/20... Training Step:3947... Training loss:2.0939... 0.2120 sec/batch\n",
      "Epoch:14/20... Training Step:3948... Training loss:2.0805... 0.2004 sec/batch\n",
      "Epoch:14/20... Training Step:3949... Training loss:2.0800... 0.2082 sec/batch\n",
      "Epoch:14/20... Training Step:3950... Training loss:2.1172... 0.1921 sec/batch\n",
      "Epoch:14/20... Training Step:3951... Training loss:2.1092... 0.1919 sec/batch\n",
      "Epoch:14/20... Training Step:3952... Training loss:2.1281... 0.1915 sec/batch\n",
      "Epoch:14/20... Training Step:3953... Training loss:2.1245... 0.2071 sec/batch\n",
      "Epoch:14/20... Training Step:3954... Training loss:2.1087... 0.2048 sec/batch\n",
      "Epoch:14/20... Training Step:3955... Training loss:2.1147... 0.1922 sec/batch\n",
      "Epoch:14/20... Training Step:3956... Training loss:2.0850... 0.1992 sec/batch\n",
      "Epoch:14/20... Training Step:3957... Training loss:2.1360... 0.1930 sec/batch\n",
      "Epoch:14/20... Training Step:3958... Training loss:2.1259... 0.2033 sec/batch\n",
      "Epoch:14/20... Training Step:3959... Training loss:2.1236... 0.1966 sec/batch\n",
      "Epoch:14/20... Training Step:3960... Training loss:2.1185... 0.1945 sec/batch\n",
      "Epoch:14/20... Training Step:3961... Training loss:2.0974... 0.2016 sec/batch\n",
      "Epoch:14/20... Training Step:3962... Training loss:2.1247... 0.2015 sec/batch\n",
      "Epoch:14/20... Training Step:3963... Training loss:2.1514... 0.1999 sec/batch\n",
      "Epoch:14/20... Training Step:3964... Training loss:2.1225... 0.1968 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14/20... Training Step:3965... Training loss:2.1383... 0.1925 sec/batch\n",
      "Epoch:14/20... Training Step:3966... Training loss:2.1245... 0.1947 sec/batch\n",
      "Epoch:14/20... Training Step:3967... Training loss:2.1055... 0.1940 sec/batch\n",
      "Epoch:14/20... Training Step:3968... Training loss:2.1269... 0.1919 sec/batch\n",
      "Epoch:14/20... Training Step:3969... Training loss:2.1113... 0.1967 sec/batch\n",
      "Epoch:14/20... Training Step:3970... Training loss:2.1117... 0.2033 sec/batch\n",
      "Epoch:14/20... Training Step:3971... Training loss:2.1366... 0.2001 sec/batch\n",
      "Epoch:14/20... Training Step:3972... Training loss:2.1341... 0.2165 sec/batch\n",
      "Epoch:14/20... Training Step:3973... Training loss:2.1447... 0.1926 sec/batch\n",
      "Epoch:14/20... Training Step:3974... Training loss:2.0973... 0.2040 sec/batch\n",
      "Epoch:14/20... Training Step:3975... Training loss:2.0680... 0.2079 sec/batch\n",
      "Epoch:14/20... Training Step:3976... Training loss:2.1036... 0.1988 sec/batch\n",
      "Epoch:14/20... Training Step:3977... Training loss:2.1004... 0.2016 sec/batch\n",
      "Epoch:14/20... Training Step:3978... Training loss:2.0950... 0.1922 sec/batch\n",
      "Epoch:14/20... Training Step:3979... Training loss:2.0736... 0.2204 sec/batch\n",
      "Epoch:14/20... Training Step:3980... Training loss:2.0841... 0.2145 sec/batch\n",
      "Epoch:14/20... Training Step:3981... Training loss:2.0917... 0.1931 sec/batch\n",
      "Epoch:14/20... Training Step:3982... Training loss:2.0693... 0.1977 sec/batch\n",
      "Epoch:14/20... Training Step:3983... Training loss:2.0560... 0.2091 sec/batch\n",
      "Epoch:14/20... Training Step:3984... Training loss:2.1258... 0.2196 sec/batch\n",
      "Epoch:14/20... Training Step:3985... Training loss:2.0692... 0.2029 sec/batch\n",
      "Epoch:14/20... Training Step:3986... Training loss:2.0697... 0.1947 sec/batch\n",
      "Epoch:14/20... Training Step:3987... Training loss:2.0724... 0.2303 sec/batch\n",
      "Epoch:14/20... Training Step:3988... Training loss:2.0607... 0.1928 sec/batch\n",
      "Epoch:14/20... Training Step:3989... Training loss:2.0908... 0.1971 sec/batch\n",
      "Epoch:14/20... Training Step:3990... Training loss:2.1020... 0.1946 sec/batch\n",
      "Epoch:14/20... Training Step:3991... Training loss:2.1110... 0.1953 sec/batch\n",
      "Epoch:14/20... Training Step:3992... Training loss:2.1210... 0.1911 sec/batch\n",
      "Epoch:14/20... Training Step:3993... Training loss:2.0950... 0.2009 sec/batch\n",
      "Epoch:14/20... Training Step:3994... Training loss:2.0713... 0.1910 sec/batch\n",
      "Epoch:14/20... Training Step:3995... Training loss:2.1153... 0.1967 sec/batch\n",
      "Epoch:14/20... Training Step:3996... Training loss:2.0695... 0.2052 sec/batch\n",
      "Epoch:14/20... Training Step:3997... Training loss:2.0833... 0.2016 sec/batch\n",
      "Epoch:14/20... Training Step:3998... Training loss:2.0946... 0.2032 sec/batch\n",
      "Epoch:14/20... Training Step:3999... Training loss:2.0903... 0.2169 sec/batch\n",
      "Epoch:14/20... Training Step:4000... Training loss:2.0834... 0.1918 sec/batch\n",
      "Epoch:14/20... Training Step:4001... Training loss:2.0608... 0.1907 sec/batch\n",
      "Epoch:14/20... Training Step:4002... Training loss:2.1111... 0.2088 sec/batch\n",
      "Epoch:14/20... Training Step:4003... Training loss:2.0736... 0.1927 sec/batch\n",
      "Epoch:14/20... Training Step:4004... Training loss:2.0834... 0.1983 sec/batch\n",
      "Epoch:14/20... Training Step:4005... Training loss:2.1021... 0.1929 sec/batch\n",
      "Epoch:14/20... Training Step:4006... Training loss:2.1128... 0.2032 sec/batch\n",
      "Epoch:14/20... Training Step:4007... Training loss:2.0779... 0.2030 sec/batch\n",
      "Epoch:14/20... Training Step:4008... Training loss:2.1033... 0.1955 sec/batch\n",
      "Epoch:14/20... Training Step:4009... Training loss:2.0507... 0.2387 sec/batch\n",
      "Epoch:14/20... Training Step:4010... Training loss:2.0772... 0.1927 sec/batch\n",
      "Epoch:14/20... Training Step:4011... Training loss:2.0999... 0.1930 sec/batch\n",
      "Epoch:14/20... Training Step:4012... Training loss:2.1177... 0.1917 sec/batch\n",
      "Epoch:14/20... Training Step:4013... Training loss:2.1395... 0.1991 sec/batch\n",
      "Epoch:14/20... Training Step:4014... Training loss:2.0971... 0.1970 sec/batch\n",
      "Epoch:14/20... Training Step:4015... Training loss:2.1160... 0.1925 sec/batch\n",
      "Epoch:14/20... Training Step:4016... Training loss:2.1169... 0.1953 sec/batch\n",
      "Epoch:14/20... Training Step:4017... Training loss:2.1036... 0.2032 sec/batch\n",
      "Epoch:14/20... Training Step:4018... Training loss:2.0955... 0.1940 sec/batch\n",
      "Epoch:14/20... Training Step:4019... Training loss:2.0884... 0.2000 sec/batch\n",
      "Epoch:14/20... Training Step:4020... Training loss:2.1106... 0.1927 sec/batch\n",
      "Epoch:14/20... Training Step:4021... Training loss:2.0950... 0.2170 sec/batch\n",
      "Epoch:14/20... Training Step:4022... Training loss:2.1223... 0.1932 sec/batch\n",
      "Epoch:14/20... Training Step:4023... Training loss:2.0886... 0.2108 sec/batch\n",
      "Epoch:14/20... Training Step:4024... Training loss:2.1006... 0.1991 sec/batch\n",
      "Epoch:14/20... Training Step:4025... Training loss:2.1321... 0.2008 sec/batch\n",
      "Epoch:14/20... Training Step:4026... Training loss:2.1112... 0.1923 sec/batch\n",
      "Epoch:14/20... Training Step:4027... Training loss:2.0813... 0.2044 sec/batch\n",
      "Epoch:14/20... Training Step:4028... Training loss:2.1309... 0.1964 sec/batch\n",
      "Epoch:14/20... Training Step:4029... Training loss:2.1113... 0.1948 sec/batch\n",
      "Epoch:14/20... Training Step:4030... Training loss:2.1148... 0.1939 sec/batch\n",
      "Epoch:14/20... Training Step:4031... Training loss:2.1253... 0.1944 sec/batch\n",
      "Epoch:14/20... Training Step:4032... Training loss:2.0833... 0.1949 sec/batch\n",
      "Epoch:14/20... Training Step:4033... Training loss:2.0773... 0.2033 sec/batch\n",
      "Epoch:14/20... Training Step:4034... Training loss:2.0547... 0.1935 sec/batch\n",
      "Epoch:14/20... Training Step:4035... Training loss:2.0586... 0.2025 sec/batch\n",
      "Epoch:14/20... Training Step:4036... Training loss:2.0661... 0.2038 sec/batch\n",
      "Epoch:14/20... Training Step:4037... Training loss:2.1247... 0.1913 sec/batch\n",
      "Epoch:14/20... Training Step:4038... Training loss:2.0839... 0.2050 sec/batch\n",
      "Epoch:14/20... Training Step:4039... Training loss:2.0704... 0.2076 sec/batch\n",
      "Epoch:14/20... Training Step:4040... Training loss:2.0724... 0.1931 sec/batch\n",
      "Epoch:14/20... Training Step:4041... Training loss:2.0740... 0.2105 sec/batch\n",
      "Epoch:14/20... Training Step:4042... Training loss:2.1124... 0.2034 sec/batch\n",
      "Epoch:14/20... Training Step:4043... Training loss:2.1298... 0.1919 sec/batch\n",
      "Epoch:14/20... Training Step:4044... Training loss:2.0642... 0.1929 sec/batch\n",
      "Epoch:14/20... Training Step:4045... Training loss:2.1122... 0.2119 sec/batch\n",
      "Epoch:14/20... Training Step:4046... Training loss:2.1092... 0.1968 sec/batch\n",
      "Epoch:14/20... Training Step:4047... Training loss:2.1065... 0.1987 sec/batch\n",
      "Epoch:14/20... Training Step:4048... Training loss:2.1151... 0.1952 sec/batch\n",
      "Epoch:14/20... Training Step:4049... Training loss:2.0911... 0.1968 sec/batch\n",
      "Epoch:14/20... Training Step:4050... Training loss:2.1051... 0.2040 sec/batch\n",
      "Epoch:14/20... Training Step:4051... Training loss:2.0893... 0.2123 sec/batch\n",
      "Epoch:14/20... Training Step:4052... Training loss:2.0994... 0.1934 sec/batch\n",
      "Epoch:14/20... Training Step:4053... Training loss:2.1134... 0.1944 sec/batch\n",
      "Epoch:14/20... Training Step:4054... Training loss:2.1243... 0.2048 sec/batch\n",
      "Epoch:14/20... Training Step:4055... Training loss:2.1402... 0.1911 sec/batch\n",
      "Epoch:14/20... Training Step:4056... Training loss:2.1109... 0.1948 sec/batch\n",
      "Epoch:14/20... Training Step:4057... Training loss:2.0875... 0.1923 sec/batch\n",
      "Epoch:14/20... Training Step:4058... Training loss:2.0859... 0.2072 sec/batch\n",
      "Epoch:14/20... Training Step:4059... Training loss:2.0833... 0.1920 sec/batch\n",
      "Epoch:14/20... Training Step:4060... Training loss:2.0820... 0.1984 sec/batch\n",
      "Epoch:15/20... Training Step:4061... Training loss:2.2177... 0.2011 sec/batch\n",
      "Epoch:15/20... Training Step:4062... Training loss:2.0984... 0.2159 sec/batch\n",
      "Epoch:15/20... Training Step:4063... Training loss:2.1037... 0.1979 sec/batch\n",
      "Epoch:15/20... Training Step:4064... Training loss:2.1248... 0.1952 sec/batch\n",
      "Epoch:15/20... Training Step:4065... Training loss:2.1000... 0.2113 sec/batch\n",
      "Epoch:15/20... Training Step:4066... Training loss:2.0946... 0.2045 sec/batch\n",
      "Epoch:15/20... Training Step:4067... Training loss:2.1003... 0.1958 sec/batch\n",
      "Epoch:15/20... Training Step:4068... Training loss:2.1081... 0.1932 sec/batch\n",
      "Epoch:15/20... Training Step:4069... Training loss:2.1002... 0.2009 sec/batch\n",
      "Epoch:15/20... Training Step:4070... Training loss:2.0983... 0.2171 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15/20... Training Step:4071... Training loss:2.1274... 0.2110 sec/batch\n",
      "Epoch:15/20... Training Step:4072... Training loss:2.1199... 0.1923 sec/batch\n",
      "Epoch:15/20... Training Step:4073... Training loss:2.1234... 0.2082 sec/batch\n",
      "Epoch:15/20... Training Step:4074... Training loss:2.1178... 0.1923 sec/batch\n",
      "Epoch:15/20... Training Step:4075... Training loss:2.0923... 0.2130 sec/batch\n",
      "Epoch:15/20... Training Step:4076... Training loss:2.1191... 0.2057 sec/batch\n",
      "Epoch:15/20... Training Step:4077... Training loss:2.1101... 0.1931 sec/batch\n",
      "Epoch:15/20... Training Step:4078... Training loss:2.1356... 0.1949 sec/batch\n",
      "Epoch:15/20... Training Step:4079... Training loss:2.1173... 0.1965 sec/batch\n",
      "Epoch:15/20... Training Step:4080... Training loss:2.1285... 0.1929 sec/batch\n",
      "Epoch:15/20... Training Step:4081... Training loss:2.1203... 0.1952 sec/batch\n",
      "Epoch:15/20... Training Step:4082... Training loss:2.0941... 0.1954 sec/batch\n",
      "Epoch:15/20... Training Step:4083... Training loss:2.0895... 0.2058 sec/batch\n",
      "Epoch:15/20... Training Step:4084... Training loss:2.1374... 0.2176 sec/batch\n",
      "Epoch:15/20... Training Step:4085... Training loss:2.0981... 0.1932 sec/batch\n",
      "Epoch:15/20... Training Step:4086... Training loss:2.0910... 0.1925 sec/batch\n",
      "Epoch:15/20... Training Step:4087... Training loss:2.1149... 0.2075 sec/batch\n",
      "Epoch:15/20... Training Step:4088... Training loss:2.1139... 0.1928 sec/batch\n",
      "Epoch:15/20... Training Step:4089... Training loss:2.1253... 0.2003 sec/batch\n",
      "Epoch:15/20... Training Step:4090... Training loss:2.1337... 0.1926 sec/batch\n",
      "Epoch:15/20... Training Step:4091... Training loss:2.1015... 0.1987 sec/batch\n",
      "Epoch:15/20... Training Step:4092... Training loss:2.0836... 0.1984 sec/batch\n",
      "Epoch:15/20... Training Step:4093... Training loss:2.0942... 0.2199 sec/batch\n",
      "Epoch:15/20... Training Step:4094... Training loss:2.1041... 0.1938 sec/batch\n",
      "Epoch:15/20... Training Step:4095... Training loss:2.1285... 0.1952 sec/batch\n",
      "Epoch:15/20... Training Step:4096... Training loss:2.1084... 0.1917 sec/batch\n",
      "Epoch:15/20... Training Step:4097... Training loss:2.1243... 0.2134 sec/batch\n",
      "Epoch:15/20... Training Step:4098... Training loss:2.1081... 0.1959 sec/batch\n",
      "Epoch:15/20... Training Step:4099... Training loss:2.0937... 0.1982 sec/batch\n",
      "Epoch:15/20... Training Step:4100... Training loss:2.0655... 0.1920 sec/batch\n",
      "Epoch:15/20... Training Step:4101... Training loss:2.0916... 0.1911 sec/batch\n",
      "Epoch:15/20... Training Step:4102... Training loss:2.1076... 0.1934 sec/batch\n",
      "Epoch:15/20... Training Step:4103... Training loss:2.1268... 0.1971 sec/batch\n",
      "Epoch:15/20... Training Step:4104... Training loss:2.1010... 0.1963 sec/batch\n",
      "Epoch:15/20... Training Step:4105... Training loss:2.1300... 0.1939 sec/batch\n",
      "Epoch:15/20... Training Step:4106... Training loss:2.1329... 0.2135 sec/batch\n",
      "Epoch:15/20... Training Step:4107... Training loss:2.1312... 0.2034 sec/batch\n",
      "Epoch:15/20... Training Step:4108... Training loss:2.1047... 0.1909 sec/batch\n",
      "Epoch:15/20... Training Step:4109... Training loss:2.0623... 0.1964 sec/batch\n",
      "Epoch:15/20... Training Step:4110... Training loss:2.1303... 0.1977 sec/batch\n",
      "Epoch:15/20... Training Step:4111... Training loss:2.0865... 0.1961 sec/batch\n",
      "Epoch:15/20... Training Step:4112... Training loss:2.0943... 0.2017 sec/batch\n",
      "Epoch:15/20... Training Step:4113... Training loss:2.0990... 0.1941 sec/batch\n",
      "Epoch:15/20... Training Step:4114... Training loss:2.0553... 0.1949 sec/batch\n",
      "Epoch:15/20... Training Step:4115... Training loss:2.0859... 0.1915 sec/batch\n",
      "Epoch:15/20... Training Step:4116... Training loss:2.0905... 0.1918 sec/batch\n",
      "Epoch:15/20... Training Step:4117... Training loss:2.0802... 0.2172 sec/batch\n",
      "Epoch:15/20... Training Step:4118... Training loss:2.0921... 0.2063 sec/batch\n",
      "Epoch:15/20... Training Step:4119... Training loss:2.1083... 0.1948 sec/batch\n",
      "Epoch:15/20... Training Step:4120... Training loss:2.0979... 0.2212 sec/batch\n",
      "Epoch:15/20... Training Step:4121... Training loss:2.1002... 0.2022 sec/batch\n",
      "Epoch:15/20... Training Step:4122... Training loss:2.0969... 0.2083 sec/batch\n",
      "Epoch:15/20... Training Step:4123... Training loss:2.0749... 0.1919 sec/batch\n",
      "Epoch:15/20... Training Step:4124... Training loss:2.0912... 0.1921 sec/batch\n",
      "Epoch:15/20... Training Step:4125... Training loss:2.0803... 0.2154 sec/batch\n",
      "Epoch:15/20... Training Step:4126... Training loss:2.0453... 0.1968 sec/batch\n",
      "Epoch:15/20... Training Step:4127... Training loss:2.0560... 0.1981 sec/batch\n",
      "Epoch:15/20... Training Step:4128... Training loss:2.0706... 0.2020 sec/batch\n",
      "Epoch:15/20... Training Step:4129... Training loss:2.0814... 0.1982 sec/batch\n",
      "Epoch:15/20... Training Step:4130... Training loss:2.0548... 0.2159 sec/batch\n",
      "Epoch:15/20... Training Step:4131... Training loss:2.0362... 0.2000 sec/batch\n",
      "Epoch:15/20... Training Step:4132... Training loss:2.0560... 0.1924 sec/batch\n",
      "Epoch:15/20... Training Step:4133... Training loss:2.0791... 0.1926 sec/batch\n",
      "Epoch:15/20... Training Step:4134... Training loss:2.0454... 0.2178 sec/batch\n",
      "Epoch:15/20... Training Step:4135... Training loss:2.0231... 0.1970 sec/batch\n",
      "Epoch:15/20... Training Step:4136... Training loss:2.0458... 0.1966 sec/batch\n",
      "Epoch:15/20... Training Step:4137... Training loss:2.0431... 0.2065 sec/batch\n",
      "Epoch:15/20... Training Step:4138... Training loss:2.0466... 0.1920 sec/batch\n",
      "Epoch:15/20... Training Step:4139... Training loss:2.0680... 0.1947 sec/batch\n",
      "Epoch:15/20... Training Step:4140... Training loss:2.0526... 0.2110 sec/batch\n",
      "Epoch:15/20... Training Step:4141... Training loss:2.0332... 0.1929 sec/batch\n",
      "Epoch:15/20... Training Step:4142... Training loss:2.0418... 0.2077 sec/batch\n",
      "Epoch:15/20... Training Step:4143... Training loss:2.0682... 0.1928 sec/batch\n",
      "Epoch:15/20... Training Step:4144... Training loss:2.0977... 0.2057 sec/batch\n",
      "Epoch:15/20... Training Step:4145... Training loss:2.1006... 0.2000 sec/batch\n",
      "Epoch:15/20... Training Step:4146... Training loss:2.0487... 0.1980 sec/batch\n",
      "Epoch:15/20... Training Step:4147... Training loss:2.0828... 0.1936 sec/batch\n",
      "Epoch:15/20... Training Step:4148... Training loss:2.0830... 0.2038 sec/batch\n",
      "Epoch:15/20... Training Step:4149... Training loss:2.0745... 0.2108 sec/batch\n",
      "Epoch:15/20... Training Step:4150... Training loss:2.0842... 0.2160 sec/batch\n",
      "Epoch:15/20... Training Step:4151... Training loss:2.0815... 0.2039 sec/batch\n",
      "Epoch:15/20... Training Step:4152... Training loss:2.0719... 0.1973 sec/batch\n",
      "Epoch:15/20... Training Step:4153... Training loss:2.0851... 0.1924 sec/batch\n",
      "Epoch:15/20... Training Step:4154... Training loss:2.0749... 0.1945 sec/batch\n",
      "Epoch:15/20... Training Step:4155... Training loss:2.0501... 0.2119 sec/batch\n",
      "Epoch:15/20... Training Step:4156... Training loss:2.0799... 0.1915 sec/batch\n",
      "Epoch:15/20... Training Step:4157... Training loss:2.0848... 0.2006 sec/batch\n",
      "Epoch:15/20... Training Step:4158... Training loss:2.1062... 0.1924 sec/batch\n",
      "Epoch:15/20... Training Step:4159... Training loss:2.0621... 0.1937 sec/batch\n",
      "Epoch:15/20... Training Step:4160... Training loss:2.0930... 0.1943 sec/batch\n",
      "Epoch:15/20... Training Step:4161... Training loss:2.0801... 0.1963 sec/batch\n",
      "Epoch:15/20... Training Step:4162... Training loss:2.0879... 0.2150 sec/batch\n",
      "Epoch:15/20... Training Step:4163... Training loss:2.1002... 0.1915 sec/batch\n",
      "Epoch:15/20... Training Step:4164... Training loss:2.0874... 0.1947 sec/batch\n",
      "Epoch:15/20... Training Step:4165... Training loss:2.1250... 0.2018 sec/batch\n",
      "Epoch:15/20... Training Step:4166... Training loss:2.1005... 0.1942 sec/batch\n",
      "Epoch:15/20... Training Step:4167... Training loss:2.1110... 0.1927 sec/batch\n",
      "Epoch:15/20... Training Step:4168... Training loss:2.0663... 0.1997 sec/batch\n",
      "Epoch:15/20... Training Step:4169... Training loss:2.1067... 0.2174 sec/batch\n",
      "Epoch:15/20... Training Step:4170... Training loss:2.0858... 0.1924 sec/batch\n",
      "Epoch:15/20... Training Step:4171... Training loss:2.0735... 0.1939 sec/batch\n",
      "Epoch:15/20... Training Step:4172... Training loss:2.0521... 0.1974 sec/batch\n",
      "Epoch:15/20... Training Step:4173... Training loss:2.0423... 0.2042 sec/batch\n",
      "Epoch:15/20... Training Step:4174... Training loss:2.0993... 0.2033 sec/batch\n",
      "Epoch:15/20... Training Step:4175... Training loss:2.0620... 0.1911 sec/batch\n",
      "Epoch:15/20... Training Step:4176... Training loss:2.1105... 0.2045 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15/20... Training Step:4177... Training loss:2.1043... 0.2167 sec/batch\n",
      "Epoch:15/20... Training Step:4178... Training loss:2.0757... 0.1946 sec/batch\n",
      "Epoch:15/20... Training Step:4179... Training loss:2.0980... 0.1936 sec/batch\n",
      "Epoch:15/20... Training Step:4180... Training loss:2.0897... 0.2021 sec/batch\n",
      "Epoch:15/20... Training Step:4181... Training loss:2.0986... 0.2127 sec/batch\n",
      "Epoch:15/20... Training Step:4182... Training loss:2.1070... 0.1918 sec/batch\n",
      "Epoch:15/20... Training Step:4183... Training loss:2.0830... 0.1929 sec/batch\n",
      "Epoch:15/20... Training Step:4184... Training loss:2.1128... 0.1923 sec/batch\n",
      "Epoch:15/20... Training Step:4185... Training loss:2.0807... 0.1944 sec/batch\n",
      "Epoch:15/20... Training Step:4186... Training loss:2.1014... 0.1929 sec/batch\n",
      "Epoch:15/20... Training Step:4187... Training loss:2.1206... 0.1920 sec/batch\n",
      "Epoch:15/20... Training Step:4188... Training loss:2.1221... 0.1916 sec/batch\n",
      "Epoch:15/20... Training Step:4189... Training loss:2.1378... 0.2183 sec/batch\n",
      "Epoch:15/20... Training Step:4190... Training loss:2.1623... 0.1989 sec/batch\n",
      "Epoch:15/20... Training Step:4191... Training loss:2.1112... 0.2132 sec/batch\n",
      "Epoch:15/20... Training Step:4192... Training loss:2.1096... 0.1970 sec/batch\n",
      "Epoch:15/20... Training Step:4193... Training loss:2.1281... 0.1935 sec/batch\n",
      "Epoch:15/20... Training Step:4194... Training loss:2.1390... 0.1947 sec/batch\n",
      "Epoch:15/20... Training Step:4195... Training loss:2.1070... 0.1980 sec/batch\n",
      "Epoch:15/20... Training Step:4196... Training loss:2.0901... 0.2041 sec/batch\n",
      "Epoch:15/20... Training Step:4197... Training loss:2.0979... 0.1979 sec/batch\n",
      "Epoch:15/20... Training Step:4198... Training loss:2.0926... 0.1914 sec/batch\n",
      "Epoch:15/20... Training Step:4199... Training loss:2.0929... 0.1922 sec/batch\n",
      "Epoch:15/20... Training Step:4200... Training loss:2.0909... 0.2012 sec/batch\n",
      "Epoch:15/20... Training Step:4201... Training loss:2.0908... 0.1921 sec/batch\n",
      "Epoch:15/20... Training Step:4202... Training loss:2.1740... 0.2029 sec/batch\n",
      "Epoch:15/20... Training Step:4203... Training loss:2.1041... 0.1965 sec/batch\n",
      "Epoch:15/20... Training Step:4204... Training loss:2.0810... 0.2006 sec/batch\n",
      "Epoch:15/20... Training Step:4205... Training loss:2.1011... 0.1934 sec/batch\n",
      "Epoch:15/20... Training Step:4206... Training loss:2.1097... 0.1932 sec/batch\n",
      "Epoch:15/20... Training Step:4207... Training loss:2.0647... 0.2227 sec/batch\n",
      "Epoch:15/20... Training Step:4208... Training loss:2.0781... 0.2052 sec/batch\n",
      "Epoch:15/20... Training Step:4209... Training loss:2.0893... 0.2014 sec/batch\n",
      "Epoch:15/20... Training Step:4210... Training loss:2.0961... 0.1927 sec/batch\n",
      "Epoch:15/20... Training Step:4211... Training loss:2.0528... 0.1994 sec/batch\n",
      "Epoch:15/20... Training Step:4212... Training loss:2.0675... 0.1917 sec/batch\n",
      "Epoch:15/20... Training Step:4213... Training loss:2.0952... 0.2003 sec/batch\n",
      "Epoch:15/20... Training Step:4214... Training loss:2.0593... 0.1932 sec/batch\n",
      "Epoch:15/20... Training Step:4215... Training loss:2.0785... 0.1998 sec/batch\n",
      "Epoch:15/20... Training Step:4216... Training loss:2.1110... 0.1943 sec/batch\n",
      "Epoch:15/20... Training Step:4217... Training loss:2.0811... 0.1945 sec/batch\n",
      "Epoch:15/20... Training Step:4218... Training loss:2.0816... 0.2097 sec/batch\n",
      "Epoch:15/20... Training Step:4219... Training loss:2.0751... 0.2020 sec/batch\n",
      "Epoch:15/20... Training Step:4220... Training loss:2.0692... 0.2082 sec/batch\n",
      "Epoch:15/20... Training Step:4221... Training loss:2.0538... 0.1977 sec/batch\n",
      "Epoch:15/20... Training Step:4222... Training loss:2.0709... 0.1944 sec/batch\n",
      "Epoch:15/20... Training Step:4223... Training loss:2.0545... 0.2083 sec/batch\n",
      "Epoch:15/20... Training Step:4224... Training loss:2.1085... 0.1914 sec/batch\n",
      "Epoch:15/20... Training Step:4225... Training loss:2.1041... 0.1961 sec/batch\n",
      "Epoch:15/20... Training Step:4226... Training loss:2.1314... 0.2087 sec/batch\n",
      "Epoch:15/20... Training Step:4227... Training loss:2.1330... 0.2049 sec/batch\n",
      "Epoch:15/20... Training Step:4228... Training loss:2.1245... 0.1929 sec/batch\n",
      "Epoch:15/20... Training Step:4229... Training loss:2.0719... 0.1930 sec/batch\n",
      "Epoch:15/20... Training Step:4230... Training loss:2.0670... 0.1927 sec/batch\n",
      "Epoch:15/20... Training Step:4231... Training loss:2.0962... 0.1962 sec/batch\n",
      "Epoch:15/20... Training Step:4232... Training loss:2.0688... 0.2056 sec/batch\n",
      "Epoch:15/20... Training Step:4233... Training loss:2.0609... 0.1960 sec/batch\n",
      "Epoch:15/20... Training Step:4234... Training loss:2.0990... 0.1936 sec/batch\n",
      "Epoch:15/20... Training Step:4235... Training loss:2.0877... 0.2028 sec/batch\n",
      "Epoch:15/20... Training Step:4236... Training loss:2.0836... 0.2105 sec/batch\n",
      "Epoch:15/20... Training Step:4237... Training loss:2.0635... 0.2021 sec/batch\n",
      "Epoch:15/20... Training Step:4238... Training loss:2.0680... 0.1929 sec/batch\n",
      "Epoch:15/20... Training Step:4239... Training loss:2.0626... 0.1970 sec/batch\n",
      "Epoch:15/20... Training Step:4240... Training loss:2.0925... 0.1923 sec/batch\n",
      "Epoch:15/20... Training Step:4241... Training loss:2.0822... 0.2052 sec/batch\n",
      "Epoch:15/20... Training Step:4242... Training loss:2.0893... 0.1954 sec/batch\n",
      "Epoch:15/20... Training Step:4243... Training loss:2.0891... 0.2000 sec/batch\n",
      "Epoch:15/20... Training Step:4244... Training loss:2.0758... 0.1958 sec/batch\n",
      "Epoch:15/20... Training Step:4245... Training loss:2.0907... 0.2021 sec/batch\n",
      "Epoch:15/20... Training Step:4246... Training loss:2.0558... 0.2148 sec/batch\n",
      "Epoch:15/20... Training Step:4247... Training loss:2.1135... 0.1985 sec/batch\n",
      "Epoch:15/20... Training Step:4248... Training loss:2.0897... 0.1980 sec/batch\n",
      "Epoch:15/20... Training Step:4249... Training loss:2.0987... 0.2074 sec/batch\n",
      "Epoch:15/20... Training Step:4250... Training loss:2.0919... 0.1934 sec/batch\n",
      "Epoch:15/20... Training Step:4251... Training loss:2.0688... 0.2046 sec/batch\n",
      "Epoch:15/20... Training Step:4252... Training loss:2.0945... 0.2092 sec/batch\n",
      "Epoch:15/20... Training Step:4253... Training loss:2.1304... 0.2194 sec/batch\n",
      "Epoch:15/20... Training Step:4254... Training loss:2.0909... 0.2138 sec/batch\n",
      "Epoch:15/20... Training Step:4255... Training loss:2.1017... 0.1927 sec/batch\n",
      "Epoch:15/20... Training Step:4256... Training loss:2.0982... 0.2101 sec/batch\n",
      "Epoch:15/20... Training Step:4257... Training loss:2.0774... 0.1936 sec/batch\n",
      "Epoch:15/20... Training Step:4258... Training loss:2.1051... 0.1918 sec/batch\n",
      "Epoch:15/20... Training Step:4259... Training loss:2.0817... 0.1957 sec/batch\n",
      "Epoch:15/20... Training Step:4260... Training loss:2.0799... 0.1957 sec/batch\n",
      "Epoch:15/20... Training Step:4261... Training loss:2.1054... 0.2105 sec/batch\n",
      "Epoch:15/20... Training Step:4262... Training loss:2.1063... 0.1922 sec/batch\n",
      "Epoch:15/20... Training Step:4263... Training loss:2.1099... 0.2044 sec/batch\n",
      "Epoch:15/20... Training Step:4264... Training loss:2.0616... 0.1915 sec/batch\n",
      "Epoch:15/20... Training Step:4265... Training loss:2.0336... 0.1919 sec/batch\n",
      "Epoch:15/20... Training Step:4266... Training loss:2.0737... 0.2145 sec/batch\n",
      "Epoch:15/20... Training Step:4267... Training loss:2.0584... 0.1969 sec/batch\n",
      "Epoch:15/20... Training Step:4268... Training loss:2.0683... 0.2027 sec/batch\n",
      "Epoch:15/20... Training Step:4269... Training loss:2.0515... 0.2013 sec/batch\n",
      "Epoch:15/20... Training Step:4270... Training loss:2.0489... 0.1986 sec/batch\n",
      "Epoch:15/20... Training Step:4271... Training loss:2.0592... 0.2056 sec/batch\n",
      "Epoch:15/20... Training Step:4272... Training loss:2.0424... 0.1917 sec/batch\n",
      "Epoch:15/20... Training Step:4273... Training loss:2.0254... 0.2082 sec/batch\n",
      "Epoch:15/20... Training Step:4274... Training loss:2.1014... 0.1924 sec/batch\n",
      "Epoch:15/20... Training Step:4275... Training loss:2.0332... 0.1994 sec/batch\n",
      "Epoch:15/20... Training Step:4276... Training loss:2.0369... 0.2037 sec/batch\n",
      "Epoch:15/20... Training Step:4277... Training loss:2.0437... 0.2022 sec/batch\n",
      "Epoch:15/20... Training Step:4278... Training loss:2.0374... 0.2032 sec/batch\n",
      "Epoch:15/20... Training Step:4279... Training loss:2.0642... 0.1933 sec/batch\n",
      "Epoch:15/20... Training Step:4280... Training loss:2.0708... 0.1944 sec/batch\n",
      "Epoch:15/20... Training Step:4281... Training loss:2.0891... 0.2069 sec/batch\n",
      "Epoch:15/20... Training Step:4282... Training loss:2.1003... 0.2066 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15/20... Training Step:4283... Training loss:2.0620... 0.2051 sec/batch\n",
      "Epoch:15/20... Training Step:4284... Training loss:2.0386... 0.1931 sec/batch\n",
      "Epoch:15/20... Training Step:4285... Training loss:2.0905... 0.1935 sec/batch\n",
      "Epoch:15/20... Training Step:4286... Training loss:2.0451... 0.1931 sec/batch\n",
      "Epoch:15/20... Training Step:4287... Training loss:2.0546... 0.1940 sec/batch\n",
      "Epoch:15/20... Training Step:4288... Training loss:2.0622... 0.1915 sec/batch\n",
      "Epoch:15/20... Training Step:4289... Training loss:2.0698... 0.2068 sec/batch\n",
      "Epoch:15/20... Training Step:4290... Training loss:2.0631... 0.2014 sec/batch\n",
      "Epoch:15/20... Training Step:4291... Training loss:2.0352... 0.2055 sec/batch\n",
      "Epoch:15/20... Training Step:4292... Training loss:2.0790... 0.1917 sec/batch\n",
      "Epoch:15/20... Training Step:4293... Training loss:2.0498... 0.1963 sec/batch\n",
      "Epoch:15/20... Training Step:4294... Training loss:2.0592... 0.1924 sec/batch\n",
      "Epoch:15/20... Training Step:4295... Training loss:2.0675... 0.2079 sec/batch\n",
      "Epoch:15/20... Training Step:4296... Training loss:2.0894... 0.1973 sec/batch\n",
      "Epoch:15/20... Training Step:4297... Training loss:2.0497... 0.2095 sec/batch\n",
      "Epoch:15/20... Training Step:4298... Training loss:2.0814... 0.1966 sec/batch\n",
      "Epoch:15/20... Training Step:4299... Training loss:2.0319... 0.2065 sec/batch\n",
      "Epoch:15/20... Training Step:4300... Training loss:2.0630... 0.2016 sec/batch\n",
      "Epoch:15/20... Training Step:4301... Training loss:2.0646... 0.2152 sec/batch\n",
      "Epoch:15/20... Training Step:4302... Training loss:2.0928... 0.1964 sec/batch\n",
      "Epoch:15/20... Training Step:4303... Training loss:2.1186... 0.1949 sec/batch\n",
      "Epoch:15/20... Training Step:4304... Training loss:2.0649... 0.2114 sec/batch\n",
      "Epoch:15/20... Training Step:4305... Training loss:2.0891... 0.1951 sec/batch\n",
      "Epoch:15/20... Training Step:4306... Training loss:2.0802... 0.1999 sec/batch\n",
      "Epoch:15/20... Training Step:4307... Training loss:2.0867... 0.1916 sec/batch\n",
      "Epoch:15/20... Training Step:4308... Training loss:2.0785... 0.1910 sec/batch\n",
      "Epoch:15/20... Training Step:4309... Training loss:2.0632... 0.1962 sec/batch\n",
      "Epoch:15/20... Training Step:4310... Training loss:2.0832... 0.2019 sec/batch\n",
      "Epoch:15/20... Training Step:4311... Training loss:2.0658... 0.1926 sec/batch\n",
      "Epoch:15/20... Training Step:4312... Training loss:2.1053... 0.1917 sec/batch\n",
      "Epoch:15/20... Training Step:4313... Training loss:2.0562... 0.2042 sec/batch\n",
      "Epoch:15/20... Training Step:4314... Training loss:2.0718... 0.2070 sec/batch\n",
      "Epoch:15/20... Training Step:4315... Training loss:2.1062... 0.2002 sec/batch\n",
      "Epoch:15/20... Training Step:4316... Training loss:2.0857... 0.1946 sec/batch\n",
      "Epoch:15/20... Training Step:4317... Training loss:2.0553... 0.1970 sec/batch\n",
      "Epoch:15/20... Training Step:4318... Training loss:2.0956... 0.1993 sec/batch\n",
      "Epoch:15/20... Training Step:4319... Training loss:2.0793... 0.1950 sec/batch\n",
      "Epoch:15/20... Training Step:4320... Training loss:2.0874... 0.1943 sec/batch\n",
      "Epoch:15/20... Training Step:4321... Training loss:2.0977... 0.2103 sec/batch\n",
      "Epoch:15/20... Training Step:4322... Training loss:2.0474... 0.2044 sec/batch\n",
      "Epoch:15/20... Training Step:4323... Training loss:2.0457... 0.2067 sec/batch\n",
      "Epoch:15/20... Training Step:4324... Training loss:2.0231... 0.2099 sec/batch\n",
      "Epoch:15/20... Training Step:4325... Training loss:2.0354... 0.1943 sec/batch\n",
      "Epoch:15/20... Training Step:4326... Training loss:2.0433... 0.1933 sec/batch\n",
      "Epoch:15/20... Training Step:4327... Training loss:2.0955... 0.1959 sec/batch\n",
      "Epoch:15/20... Training Step:4328... Training loss:2.0525... 0.1976 sec/batch\n",
      "Epoch:15/20... Training Step:4329... Training loss:2.0416... 0.1934 sec/batch\n",
      "Epoch:15/20... Training Step:4330... Training loss:2.0467... 0.2202 sec/batch\n",
      "Epoch:15/20... Training Step:4331... Training loss:2.0337... 0.2105 sec/batch\n",
      "Epoch:15/20... Training Step:4332... Training loss:2.0809... 0.1923 sec/batch\n",
      "Epoch:15/20... Training Step:4333... Training loss:2.1040... 0.2032 sec/batch\n",
      "Epoch:15/20... Training Step:4334... Training loss:2.0303... 0.1997 sec/batch\n",
      "Epoch:15/20... Training Step:4335... Training loss:2.0882... 0.2147 sec/batch\n",
      "Epoch:15/20... Training Step:4336... Training loss:2.0839... 0.1963 sec/batch\n",
      "Epoch:15/20... Training Step:4337... Training loss:2.0776... 0.1931 sec/batch\n",
      "Epoch:15/20... Training Step:4338... Training loss:2.0819... 0.1952 sec/batch\n",
      "Epoch:15/20... Training Step:4339... Training loss:2.0607... 0.2095 sec/batch\n",
      "Epoch:15/20... Training Step:4340... Training loss:2.0846... 0.2212 sec/batch\n",
      "Epoch:15/20... Training Step:4341... Training loss:2.0768... 0.1920 sec/batch\n",
      "Epoch:15/20... Training Step:4342... Training loss:2.0783... 0.1942 sec/batch\n",
      "Epoch:15/20... Training Step:4343... Training loss:2.0813... 0.1916 sec/batch\n",
      "Epoch:15/20... Training Step:4344... Training loss:2.0872... 0.1964 sec/batch\n",
      "Epoch:15/20... Training Step:4345... Training loss:2.1172... 0.1939 sec/batch\n",
      "Epoch:15/20... Training Step:4346... Training loss:2.0837... 0.1959 sec/batch\n",
      "Epoch:15/20... Training Step:4347... Training loss:2.0560... 0.1921 sec/batch\n",
      "Epoch:15/20... Training Step:4348... Training loss:2.0530... 0.2071 sec/batch\n",
      "Epoch:15/20... Training Step:4349... Training loss:2.0566... 0.2019 sec/batch\n",
      "Epoch:15/20... Training Step:4350... Training loss:2.0546... 0.1925 sec/batch\n",
      "Epoch:16/20... Training Step:4351... Training loss:2.1868... 0.1932 sec/batch\n",
      "Epoch:16/20... Training Step:4352... Training loss:2.0691... 0.1909 sec/batch\n",
      "Epoch:16/20... Training Step:4353... Training loss:2.0694... 0.1932 sec/batch\n",
      "Epoch:16/20... Training Step:4354... Training loss:2.0917... 0.2144 sec/batch\n",
      "Epoch:16/20... Training Step:4355... Training loss:2.0752... 0.2095 sec/batch\n",
      "Epoch:16/20... Training Step:4356... Training loss:2.0687... 0.2045 sec/batch\n",
      "Epoch:16/20... Training Step:4357... Training loss:2.0644... 0.1965 sec/batch\n",
      "Epoch:16/20... Training Step:4358... Training loss:2.0808... 0.1923 sec/batch\n",
      "Epoch:16/20... Training Step:4359... Training loss:2.0756... 0.2082 sec/batch\n",
      "Epoch:16/20... Training Step:4360... Training loss:2.0744... 0.2091 sec/batch\n",
      "Epoch:16/20... Training Step:4361... Training loss:2.1020... 0.1946 sec/batch\n",
      "Epoch:16/20... Training Step:4362... Training loss:2.0883... 0.2061 sec/batch\n",
      "Epoch:16/20... Training Step:4363... Training loss:2.0919... 0.1920 sec/batch\n",
      "Epoch:16/20... Training Step:4364... Training loss:2.0906... 0.2052 sec/batch\n",
      "Epoch:16/20... Training Step:4365... Training loss:2.0537... 0.1990 sec/batch\n",
      "Epoch:16/20... Training Step:4366... Training loss:2.0843... 0.1933 sec/batch\n",
      "Epoch:16/20... Training Step:4367... Training loss:2.0902... 0.2024 sec/batch\n",
      "Epoch:16/20... Training Step:4368... Training loss:2.0973... 0.1976 sec/batch\n",
      "Epoch:16/20... Training Step:4369... Training loss:2.0951... 0.2130 sec/batch\n",
      "Epoch:16/20... Training Step:4370... Training loss:2.1003... 0.1909 sec/batch\n",
      "Epoch:16/20... Training Step:4371... Training loss:2.0995... 0.1919 sec/batch\n",
      "Epoch:16/20... Training Step:4372... Training loss:2.0566... 0.1977 sec/batch\n",
      "Epoch:16/20... Training Step:4373... Training loss:2.0605... 0.1936 sec/batch\n",
      "Epoch:16/20... Training Step:4374... Training loss:2.1041... 0.2016 sec/batch\n",
      "Epoch:16/20... Training Step:4375... Training loss:2.0628... 0.1925 sec/batch\n",
      "Epoch:16/20... Training Step:4376... Training loss:2.0650... 0.2003 sec/batch\n",
      "Epoch:16/20... Training Step:4377... Training loss:2.0914... 0.1913 sec/batch\n",
      "Epoch:16/20... Training Step:4378... Training loss:2.0887... 0.2115 sec/batch\n",
      "Epoch:16/20... Training Step:4379... Training loss:2.0930... 0.1912 sec/batch\n",
      "Epoch:16/20... Training Step:4380... Training loss:2.1023... 0.1952 sec/batch\n",
      "Epoch:16/20... Training Step:4381... Training loss:2.0779... 0.1934 sec/batch\n",
      "Epoch:16/20... Training Step:4382... Training loss:2.0591... 0.1946 sec/batch\n",
      "Epoch:16/20... Training Step:4383... Training loss:2.0659... 0.2160 sec/batch\n",
      "Epoch:16/20... Training Step:4384... Training loss:2.0794... 0.2029 sec/batch\n",
      "Epoch:16/20... Training Step:4385... Training loss:2.1011... 0.1911 sec/batch\n",
      "Epoch:16/20... Training Step:4386... Training loss:2.0779... 0.2027 sec/batch\n",
      "Epoch:16/20... Training Step:4387... Training loss:2.0938... 0.2051 sec/batch\n",
      "Epoch:16/20... Training Step:4388... Training loss:2.0842... 0.1935 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16/20... Training Step:4389... Training loss:2.0613... 0.1939 sec/batch\n",
      "Epoch:16/20... Training Step:4390... Training loss:2.0401... 0.2042 sec/batch\n",
      "Epoch:16/20... Training Step:4391... Training loss:2.0677... 0.1931 sec/batch\n",
      "Epoch:16/20... Training Step:4392... Training loss:2.0813... 0.1939 sec/batch\n",
      "Epoch:16/20... Training Step:4393... Training loss:2.1004... 0.2014 sec/batch\n",
      "Epoch:16/20... Training Step:4394... Training loss:2.0662... 0.2149 sec/batch\n",
      "Epoch:16/20... Training Step:4395... Training loss:2.0980... 0.2141 sec/batch\n",
      "Epoch:16/20... Training Step:4396... Training loss:2.1027... 0.2037 sec/batch\n",
      "Epoch:16/20... Training Step:4397... Training loss:2.1007... 0.2018 sec/batch\n",
      "Epoch:16/20... Training Step:4398... Training loss:2.0724... 0.1959 sec/batch\n",
      "Epoch:16/20... Training Step:4399... Training loss:2.0361... 0.2004 sec/batch\n",
      "Epoch:16/20... Training Step:4400... Training loss:2.1140... 0.2080 sec/batch\n",
      "Epoch:16/20... Training Step:4401... Training loss:2.0589... 0.1937 sec/batch\n",
      "Epoch:16/20... Training Step:4402... Training loss:2.0669... 0.1931 sec/batch\n",
      "Epoch:16/20... Training Step:4403... Training loss:2.0640... 0.1988 sec/batch\n",
      "Epoch:16/20... Training Step:4404... Training loss:2.0283... 0.1922 sec/batch\n",
      "Epoch:16/20... Training Step:4405... Training loss:2.0466... 0.1942 sec/batch\n",
      "Epoch:16/20... Training Step:4406... Training loss:2.0451... 0.1919 sec/batch\n",
      "Epoch:16/20... Training Step:4407... Training loss:2.0477... 0.2180 sec/batch\n",
      "Epoch:16/20... Training Step:4408... Training loss:2.0558... 0.1930 sec/batch\n",
      "Epoch:16/20... Training Step:4409... Training loss:2.0764... 0.2017 sec/batch\n",
      "Epoch:16/20... Training Step:4410... Training loss:2.0760... 0.2039 sec/batch\n",
      "Epoch:16/20... Training Step:4411... Training loss:2.0681... 0.1925 sec/batch\n",
      "Epoch:16/20... Training Step:4412... Training loss:2.0658... 0.1957 sec/batch\n",
      "Epoch:16/20... Training Step:4413... Training loss:2.0479... 0.2218 sec/batch\n",
      "Epoch:16/20... Training Step:4414... Training loss:2.0653... 0.2052 sec/batch\n",
      "Epoch:16/20... Training Step:4415... Training loss:2.0507... 0.1908 sec/batch\n",
      "Epoch:16/20... Training Step:4416... Training loss:2.0244... 0.1947 sec/batch\n",
      "Epoch:16/20... Training Step:4417... Training loss:2.0255... 0.2029 sec/batch\n",
      "Epoch:16/20... Training Step:4418... Training loss:2.0447... 0.2034 sec/batch\n",
      "Epoch:16/20... Training Step:4419... Training loss:2.0449... 0.1969 sec/batch\n",
      "Epoch:16/20... Training Step:4420... Training loss:2.0265... 0.1942 sec/batch\n",
      "Epoch:16/20... Training Step:4421... Training loss:2.0049... 0.1931 sec/batch\n",
      "Epoch:16/20... Training Step:4422... Training loss:2.0289... 0.1930 sec/batch\n",
      "Epoch:16/20... Training Step:4423... Training loss:2.0468... 0.2123 sec/batch\n",
      "Epoch:16/20... Training Step:4424... Training loss:2.0322... 0.2078 sec/batch\n",
      "Epoch:16/20... Training Step:4425... Training loss:1.9899... 0.1919 sec/batch\n",
      "Epoch:16/20... Training Step:4426... Training loss:2.0175... 0.1944 sec/batch\n",
      "Epoch:16/20... Training Step:4427... Training loss:2.0129... 0.1915 sec/batch\n",
      "Epoch:16/20... Training Step:4428... Training loss:2.0143... 0.2091 sec/batch\n",
      "Epoch:16/20... Training Step:4429... Training loss:2.0425... 0.2165 sec/batch\n",
      "Epoch:16/20... Training Step:4430... Training loss:2.0293... 0.1968 sec/batch\n",
      "Epoch:16/20... Training Step:4431... Training loss:1.9954... 0.2171 sec/batch\n",
      "Epoch:16/20... Training Step:4432... Training loss:2.0117... 0.1923 sec/batch\n",
      "Epoch:16/20... Training Step:4433... Training loss:2.0422... 0.1952 sec/batch\n",
      "Epoch:16/20... Training Step:4434... Training loss:2.0687... 0.2085 sec/batch\n",
      "Epoch:16/20... Training Step:4435... Training loss:2.0664... 0.1943 sec/batch\n",
      "Epoch:16/20... Training Step:4436... Training loss:2.0264... 0.1995 sec/batch\n",
      "Epoch:16/20... Training Step:4437... Training loss:2.0499... 0.1919 sec/batch\n",
      "Epoch:16/20... Training Step:4438... Training loss:2.0568... 0.1947 sec/batch\n",
      "Epoch:16/20... Training Step:4439... Training loss:2.0392... 0.1977 sec/batch\n",
      "Epoch:16/20... Training Step:4440... Training loss:2.0600... 0.1999 sec/batch\n",
      "Epoch:16/20... Training Step:4441... Training loss:2.0550... 0.1998 sec/batch\n",
      "Epoch:16/20... Training Step:4442... Training loss:2.0492... 0.1986 sec/batch\n",
      "Epoch:16/20... Training Step:4443... Training loss:2.0668... 0.2084 sec/batch\n",
      "Epoch:16/20... Training Step:4444... Training loss:2.0482... 0.1950 sec/batch\n",
      "Epoch:16/20... Training Step:4445... Training loss:2.0276... 0.1941 sec/batch\n",
      "Epoch:16/20... Training Step:4446... Training loss:2.0558... 0.2034 sec/batch\n",
      "Epoch:16/20... Training Step:4447... Training loss:2.0507... 0.1932 sec/batch\n",
      "Epoch:16/20... Training Step:4448... Training loss:2.0852... 0.1931 sec/batch\n",
      "Epoch:16/20... Training Step:4449... Training loss:2.0268... 0.1946 sec/batch\n",
      "Epoch:16/20... Training Step:4450... Training loss:2.0745... 0.1990 sec/batch\n",
      "Epoch:16/20... Training Step:4451... Training loss:2.0563... 0.1929 sec/batch\n",
      "Epoch:16/20... Training Step:4452... Training loss:2.0731... 0.1999 sec/batch\n",
      "Epoch:16/20... Training Step:4453... Training loss:2.0755... 0.1921 sec/batch\n",
      "Epoch:16/20... Training Step:4454... Training loss:2.0571... 0.1963 sec/batch\n",
      "Epoch:16/20... Training Step:4455... Training loss:2.1090... 0.1926 sec/batch\n",
      "Epoch:16/20... Training Step:4456... Training loss:2.0732... 0.2146 sec/batch\n",
      "Epoch:16/20... Training Step:4457... Training loss:2.0788... 0.2083 sec/batch\n",
      "Epoch:16/20... Training Step:4458... Training loss:2.0465... 0.2202 sec/batch\n",
      "Epoch:16/20... Training Step:4459... Training loss:2.0637... 0.1932 sec/batch\n",
      "Epoch:16/20... Training Step:4460... Training loss:2.0557... 0.1946 sec/batch\n",
      "Epoch:16/20... Training Step:4461... Training loss:2.0573... 0.1983 sec/batch\n",
      "Epoch:16/20... Training Step:4462... Training loss:2.0306... 0.1965 sec/batch\n",
      "Epoch:16/20... Training Step:4463... Training loss:2.0142... 0.2022 sec/batch\n",
      "Epoch:16/20... Training Step:4464... Training loss:2.0734... 0.2215 sec/batch\n",
      "Epoch:16/20... Training Step:4465... Training loss:2.0399... 0.1985 sec/batch\n",
      "Epoch:16/20... Training Step:4466... Training loss:2.0743... 0.2043 sec/batch\n",
      "Epoch:16/20... Training Step:4467... Training loss:2.0810... 0.1931 sec/batch\n",
      "Epoch:16/20... Training Step:4468... Training loss:2.0510... 0.1921 sec/batch\n",
      "Epoch:16/20... Training Step:4469... Training loss:2.0621... 0.1953 sec/batch\n",
      "Epoch:16/20... Training Step:4470... Training loss:2.0676... 0.1978 sec/batch\n",
      "Epoch:16/20... Training Step:4471... Training loss:2.0678... 0.2165 sec/batch\n",
      "Epoch:16/20... Training Step:4472... Training loss:2.0832... 0.1934 sec/batch\n",
      "Epoch:16/20... Training Step:4473... Training loss:2.0418... 0.2115 sec/batch\n",
      "Epoch:16/20... Training Step:4474... Training loss:2.0846... 0.2013 sec/batch\n",
      "Epoch:16/20... Training Step:4475... Training loss:2.0562... 0.1919 sec/batch\n",
      "Epoch:16/20... Training Step:4476... Training loss:2.0822... 0.1958 sec/batch\n",
      "Epoch:16/20... Training Step:4477... Training loss:2.0996... 0.2044 sec/batch\n",
      "Epoch:16/20... Training Step:4478... Training loss:2.0983... 0.2076 sec/batch\n",
      "Epoch:16/20... Training Step:4479... Training loss:2.1081... 0.1945 sec/batch\n",
      "Epoch:16/20... Training Step:4480... Training loss:2.1342... 0.1933 sec/batch\n",
      "Epoch:16/20... Training Step:4481... Training loss:2.1006... 0.1988 sec/batch\n",
      "Epoch:16/20... Training Step:4482... Training loss:2.0814... 0.2002 sec/batch\n",
      "Epoch:16/20... Training Step:4483... Training loss:2.0984... 0.1926 sec/batch\n",
      "Epoch:16/20... Training Step:4484... Training loss:2.1044... 0.2099 sec/batch\n",
      "Epoch:16/20... Training Step:4485... Training loss:2.0822... 0.1929 sec/batch\n",
      "Epoch:16/20... Training Step:4486... Training loss:2.0604... 0.1991 sec/batch\n",
      "Epoch:16/20... Training Step:4487... Training loss:2.0674... 0.1911 sec/batch\n",
      "Epoch:16/20... Training Step:4488... Training loss:2.0780... 0.1934 sec/batch\n",
      "Epoch:16/20... Training Step:4489... Training loss:2.0635... 0.1963 sec/batch\n",
      "Epoch:16/20... Training Step:4490... Training loss:2.0649... 0.1956 sec/batch\n",
      "Epoch:16/20... Training Step:4491... Training loss:2.0668... 0.1976 sec/batch\n",
      "Epoch:16/20... Training Step:4492... Training loss:2.1281... 0.2009 sec/batch\n",
      "Epoch:16/20... Training Step:4493... Training loss:2.0745... 0.2136 sec/batch\n",
      "Epoch:16/20... Training Step:4494... Training loss:2.0465... 0.2047 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16/20... Training Step:4495... Training loss:2.0668... 0.2054 sec/batch\n",
      "Epoch:16/20... Training Step:4496... Training loss:2.0802... 0.1943 sec/batch\n",
      "Epoch:16/20... Training Step:4497... Training loss:2.0436... 0.1943 sec/batch\n",
      "Epoch:16/20... Training Step:4498... Training loss:2.0563... 0.1919 sec/batch\n",
      "Epoch:16/20... Training Step:4499... Training loss:2.0572... 0.1959 sec/batch\n",
      "Epoch:16/20... Training Step:4500... Training loss:2.0626... 0.1918 sec/batch\n",
      "Epoch:16/20... Training Step:4501... Training loss:2.0342... 0.1947 sec/batch\n",
      "Epoch:16/20... Training Step:4502... Training loss:2.0401... 0.2049 sec/batch\n",
      "Epoch:16/20... Training Step:4503... Training loss:2.0650... 0.2027 sec/batch\n",
      "Epoch:16/20... Training Step:4504... Training loss:2.0303... 0.1931 sec/batch\n",
      "Epoch:16/20... Training Step:4505... Training loss:2.0488... 0.1935 sec/batch\n",
      "Epoch:16/20... Training Step:4506... Training loss:2.0821... 0.1922 sec/batch\n",
      "Epoch:16/20... Training Step:4507... Training loss:2.0526... 0.1934 sec/batch\n",
      "Epoch:16/20... Training Step:4508... Training loss:2.0618... 0.1927 sec/batch\n",
      "Epoch:16/20... Training Step:4509... Training loss:2.0446... 0.2071 sec/batch\n",
      "Epoch:16/20... Training Step:4510... Training loss:2.0488... 0.2129 sec/batch\n",
      "Epoch:16/20... Training Step:4511... Training loss:2.0310... 0.1922 sec/batch\n",
      "Epoch:16/20... Training Step:4512... Training loss:2.0387... 0.1936 sec/batch\n",
      "Epoch:16/20... Training Step:4513... Training loss:2.0188... 0.1920 sec/batch\n",
      "Epoch:16/20... Training Step:4514... Training loss:2.0855... 0.1937 sec/batch\n",
      "Epoch:16/20... Training Step:4515... Training loss:2.0781... 0.1909 sec/batch\n",
      "Epoch:16/20... Training Step:4516... Training loss:2.1042... 0.2135 sec/batch\n",
      "Epoch:16/20... Training Step:4517... Training loss:2.1140... 0.1914 sec/batch\n",
      "Epoch:16/20... Training Step:4518... Training loss:2.1085... 0.1961 sec/batch\n",
      "Epoch:16/20... Training Step:4519... Training loss:2.0454... 0.2172 sec/batch\n",
      "Epoch:16/20... Training Step:4520... Training loss:2.0397... 0.2057 sec/batch\n",
      "Epoch:16/20... Training Step:4521... Training loss:2.0622... 0.1983 sec/batch\n",
      "Epoch:16/20... Training Step:4522... Training loss:2.0375... 0.2060 sec/batch\n",
      "Epoch:16/20... Training Step:4523... Training loss:2.0364... 0.1934 sec/batch\n",
      "Epoch:16/20... Training Step:4524... Training loss:2.0670... 0.1928 sec/batch\n",
      "Epoch:16/20... Training Step:4525... Training loss:2.0556... 0.1912 sec/batch\n",
      "Epoch:16/20... Training Step:4526... Training loss:2.0542... 0.1947 sec/batch\n",
      "Epoch:16/20... Training Step:4527... Training loss:2.0361... 0.1921 sec/batch\n",
      "Epoch:16/20... Training Step:4528... Training loss:2.0414... 0.2170 sec/batch\n",
      "Epoch:16/20... Training Step:4529... Training loss:2.0294... 0.2022 sec/batch\n",
      "Epoch:16/20... Training Step:4530... Training loss:2.0599... 0.2087 sec/batch\n",
      "Epoch:16/20... Training Step:4531... Training loss:2.0548... 0.2096 sec/batch\n",
      "Epoch:16/20... Training Step:4532... Training loss:2.0631... 0.2257 sec/batch\n",
      "Epoch:16/20... Training Step:4533... Training loss:2.0649... 0.1919 sec/batch\n",
      "Epoch:16/20... Training Step:4534... Training loss:2.0546... 0.1966 sec/batch\n",
      "Epoch:16/20... Training Step:4535... Training loss:2.0585... 0.1922 sec/batch\n",
      "Epoch:16/20... Training Step:4536... Training loss:2.0126... 0.1904 sec/batch\n",
      "Epoch:16/20... Training Step:4537... Training loss:2.0787... 0.1928 sec/batch\n",
      "Epoch:16/20... Training Step:4538... Training loss:2.0780... 0.1953 sec/batch\n",
      "Epoch:16/20... Training Step:4539... Training loss:2.0658... 0.1922 sec/batch\n",
      "Epoch:16/20... Training Step:4540... Training loss:2.0650... 0.2182 sec/batch\n",
      "Epoch:16/20... Training Step:4541... Training loss:2.0431... 0.2204 sec/batch\n",
      "Epoch:16/20... Training Step:4542... Training loss:2.0681... 0.1917 sec/batch\n",
      "Epoch:16/20... Training Step:4543... Training loss:2.1029... 0.1954 sec/batch\n",
      "Epoch:16/20... Training Step:4544... Training loss:2.0730... 0.2022 sec/batch\n",
      "Epoch:16/20... Training Step:4545... Training loss:2.0789... 0.1955 sec/batch\n",
      "Epoch:16/20... Training Step:4546... Training loss:2.0700... 0.2023 sec/batch\n",
      "Epoch:16/20... Training Step:4547... Training loss:2.0507... 0.1910 sec/batch\n",
      "Epoch:16/20... Training Step:4548... Training loss:2.0803... 0.1942 sec/batch\n",
      "Epoch:16/20... Training Step:4549... Training loss:2.0620... 0.2220 sec/batch\n",
      "Epoch:16/20... Training Step:4550... Training loss:2.0597... 0.2388 sec/batch\n",
      "Epoch:16/20... Training Step:4551... Training loss:2.0854... 0.1955 sec/batch\n",
      "Epoch:16/20... Training Step:4552... Training loss:2.0754... 0.2280 sec/batch\n",
      "Epoch:16/20... Training Step:4553... Training loss:2.0737... 0.1928 sec/batch\n",
      "Epoch:16/20... Training Step:4554... Training loss:2.0441... 0.1930 sec/batch\n",
      "Epoch:16/20... Training Step:4555... Training loss:2.0014... 0.1946 sec/batch\n",
      "Epoch:16/20... Training Step:4556... Training loss:2.0451... 0.2032 sec/batch\n",
      "Epoch:16/20... Training Step:4557... Training loss:2.0388... 0.2057 sec/batch\n",
      "Epoch:16/20... Training Step:4558... Training loss:2.0407... 0.2064 sec/batch\n",
      "Epoch:16/20... Training Step:4559... Training loss:2.0303... 0.2177 sec/batch\n",
      "Epoch:16/20... Training Step:4560... Training loss:2.0196... 0.1918 sec/batch\n",
      "Epoch:16/20... Training Step:4561... Training loss:2.0253... 0.2088 sec/batch\n",
      "Epoch:16/20... Training Step:4562... Training loss:2.0137... 0.1935 sec/batch\n",
      "Epoch:16/20... Training Step:4563... Training loss:2.0000... 0.1977 sec/batch\n",
      "Epoch:16/20... Training Step:4564... Training loss:2.0773... 0.1926 sec/batch\n",
      "Epoch:16/20... Training Step:4565... Training loss:2.0053... 0.2006 sec/batch\n",
      "Epoch:16/20... Training Step:4566... Training loss:2.0147... 0.1919 sec/batch\n",
      "Epoch:16/20... Training Step:4567... Training loss:2.0209... 0.1971 sec/batch\n",
      "Epoch:16/20... Training Step:4568... Training loss:2.0092... 0.1939 sec/batch\n",
      "Epoch:16/20... Training Step:4569... Training loss:2.0435... 0.2095 sec/batch\n",
      "Epoch:16/20... Training Step:4570... Training loss:2.0471... 0.2127 sec/batch\n",
      "Epoch:16/20... Training Step:4571... Training loss:2.0595... 0.1973 sec/batch\n",
      "Epoch:16/20... Training Step:4572... Training loss:2.0794... 0.2035 sec/batch\n",
      "Epoch:16/20... Training Step:4573... Training loss:2.0421... 0.1999 sec/batch\n",
      "Epoch:16/20... Training Step:4574... Training loss:2.0122... 0.2057 sec/batch\n",
      "Epoch:16/20... Training Step:4575... Training loss:2.0550... 0.2014 sec/batch\n",
      "Epoch:16/20... Training Step:4576... Training loss:2.0166... 0.2118 sec/batch\n",
      "Epoch:16/20... Training Step:4577... Training loss:2.0285... 0.1908 sec/batch\n",
      "Epoch:16/20... Training Step:4578... Training loss:2.0354... 0.1938 sec/batch\n",
      "Epoch:16/20... Training Step:4579... Training loss:2.0424... 0.1922 sec/batch\n",
      "Epoch:16/20... Training Step:4580... Training loss:2.0303... 0.1999 sec/batch\n",
      "Epoch:16/20... Training Step:4581... Training loss:2.0070... 0.1985 sec/batch\n",
      "Epoch:16/20... Training Step:4582... Training loss:2.0541... 0.1978 sec/batch\n",
      "Epoch:16/20... Training Step:4583... Training loss:2.0195... 0.2095 sec/batch\n",
      "Epoch:16/20... Training Step:4584... Training loss:2.0322... 0.1930 sec/batch\n",
      "Epoch:16/20... Training Step:4585... Training loss:2.0413... 0.2087 sec/batch\n",
      "Epoch:16/20... Training Step:4586... Training loss:2.0656... 0.1921 sec/batch\n",
      "Epoch:16/20... Training Step:4587... Training loss:2.0207... 0.1921 sec/batch\n",
      "Epoch:16/20... Training Step:4588... Training loss:2.0537... 0.1940 sec/batch\n",
      "Epoch:16/20... Training Step:4589... Training loss:2.0037... 0.1935 sec/batch\n",
      "Epoch:16/20... Training Step:4590... Training loss:2.0106... 0.1964 sec/batch\n",
      "Epoch:16/20... Training Step:4591... Training loss:2.0254... 0.1995 sec/batch\n",
      "Epoch:16/20... Training Step:4592... Training loss:2.0686... 0.1930 sec/batch\n",
      "Epoch:16/20... Training Step:4593... Training loss:2.0884... 0.2092 sec/batch\n",
      "Epoch:16/20... Training Step:4594... Training loss:2.0408... 0.1947 sec/batch\n",
      "Epoch:16/20... Training Step:4595... Training loss:2.0664... 0.2055 sec/batch\n",
      "Epoch:16/20... Training Step:4596... Training loss:2.0685... 0.1915 sec/batch\n",
      "Epoch:16/20... Training Step:4597... Training loss:2.0550... 0.1961 sec/batch\n",
      "Epoch:16/20... Training Step:4598... Training loss:2.0493... 0.1998 sec/batch\n",
      "Epoch:16/20... Training Step:4599... Training loss:2.0405... 0.1962 sec/batch\n",
      "Epoch:16/20... Training Step:4600... Training loss:2.0578... 0.2118 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16/20... Training Step:4601... Training loss:2.0491... 0.1922 sec/batch\n",
      "Epoch:16/20... Training Step:4602... Training loss:2.0798... 0.2039 sec/batch\n",
      "Epoch:16/20... Training Step:4603... Training loss:2.0333... 0.1990 sec/batch\n",
      "Epoch:16/20... Training Step:4604... Training loss:2.0369... 0.2071 sec/batch\n",
      "Epoch:16/20... Training Step:4605... Training loss:2.0840... 0.1917 sec/batch\n",
      "Epoch:16/20... Training Step:4606... Training loss:2.0638... 0.1967 sec/batch\n",
      "Epoch:16/20... Training Step:4607... Training loss:2.0379... 0.2020 sec/batch\n",
      "Epoch:16/20... Training Step:4608... Training loss:2.0600... 0.2062 sec/batch\n",
      "Epoch:16/20... Training Step:4609... Training loss:2.0507... 0.1927 sec/batch\n",
      "Epoch:16/20... Training Step:4610... Training loss:2.0590... 0.1977 sec/batch\n",
      "Epoch:16/20... Training Step:4611... Training loss:2.0684... 0.1942 sec/batch\n",
      "Epoch:16/20... Training Step:4612... Training loss:2.0353... 0.1930 sec/batch\n",
      "Epoch:16/20... Training Step:4613... Training loss:2.0259... 0.2180 sec/batch\n",
      "Epoch:16/20... Training Step:4614... Training loss:1.9972... 0.2023 sec/batch\n",
      "Epoch:16/20... Training Step:4615... Training loss:2.0025... 0.1979 sec/batch\n",
      "Epoch:16/20... Training Step:4616... Training loss:2.0224... 0.1999 sec/batch\n",
      "Epoch:16/20... Training Step:4617... Training loss:2.0807... 0.2076 sec/batch\n",
      "Epoch:16/20... Training Step:4618... Training loss:2.0305... 0.1932 sec/batch\n",
      "Epoch:16/20... Training Step:4619... Training loss:2.0085... 0.1955 sec/batch\n",
      "Epoch:16/20... Training Step:4620... Training loss:2.0192... 0.2063 sec/batch\n",
      "Epoch:16/20... Training Step:4621... Training loss:2.0148... 0.1927 sec/batch\n",
      "Epoch:16/20... Training Step:4622... Training loss:2.0644... 0.1939 sec/batch\n",
      "Epoch:16/20... Training Step:4623... Training loss:2.0801... 0.1910 sec/batch\n",
      "Epoch:16/20... Training Step:4624... Training loss:2.0014... 0.1950 sec/batch\n",
      "Epoch:16/20... Training Step:4625... Training loss:2.0637... 0.2079 sec/batch\n",
      "Epoch:16/20... Training Step:4626... Training loss:2.0470... 0.2014 sec/batch\n",
      "Epoch:16/20... Training Step:4627... Training loss:2.0520... 0.2070 sec/batch\n",
      "Epoch:16/20... Training Step:4628... Training loss:2.0561... 0.1920 sec/batch\n",
      "Epoch:16/20... Training Step:4629... Training loss:2.0241... 0.2096 sec/batch\n",
      "Epoch:16/20... Training Step:4630... Training loss:2.0670... 0.2034 sec/batch\n",
      "Epoch:16/20... Training Step:4631... Training loss:2.0407... 0.1926 sec/batch\n",
      "Epoch:16/20... Training Step:4632... Training loss:2.0533... 0.1938 sec/batch\n",
      "Epoch:16/20... Training Step:4633... Training loss:2.0541... 0.1914 sec/batch\n",
      "Epoch:16/20... Training Step:4634... Training loss:2.0652... 0.1956 sec/batch\n",
      "Epoch:16/20... Training Step:4635... Training loss:2.0940... 0.1921 sec/batch\n",
      "Epoch:16/20... Training Step:4636... Training loss:2.0618... 0.1921 sec/batch\n",
      "Epoch:16/20... Training Step:4637... Training loss:2.0228... 0.1929 sec/batch\n",
      "Epoch:16/20... Training Step:4638... Training loss:2.0296... 0.1934 sec/batch\n",
      "Epoch:16/20... Training Step:4639... Training loss:2.0319... 0.2052 sec/batch\n",
      "Epoch:16/20... Training Step:4640... Training loss:2.0213... 0.2030 sec/batch\n",
      "Epoch:17/20... Training Step:4641... Training loss:2.1452... 0.1930 sec/batch\n",
      "Epoch:17/20... Training Step:4642... Training loss:2.0458... 0.2037 sec/batch\n",
      "Epoch:17/20... Training Step:4643... Training loss:2.0457... 0.1928 sec/batch\n",
      "Epoch:17/20... Training Step:4644... Training loss:2.0710... 0.1945 sec/batch\n",
      "Epoch:17/20... Training Step:4645... Training loss:2.0505... 0.2072 sec/batch\n",
      "Epoch:17/20... Training Step:4646... Training loss:2.0330... 0.2022 sec/batch\n",
      "Epoch:17/20... Training Step:4647... Training loss:2.0443... 0.2064 sec/batch\n",
      "Epoch:17/20... Training Step:4648... Training loss:2.0593... 0.1934 sec/batch\n",
      "Epoch:17/20... Training Step:4649... Training loss:2.0487... 0.1931 sec/batch\n",
      "Epoch:17/20... Training Step:4650... Training loss:2.0520... 0.2008 sec/batch\n",
      "Epoch:17/20... Training Step:4651... Training loss:2.0748... 0.2035 sec/batch\n",
      "Epoch:17/20... Training Step:4652... Training loss:2.0608... 0.1986 sec/batch\n",
      "Epoch:17/20... Training Step:4653... Training loss:2.0668... 0.2118 sec/batch\n",
      "Epoch:17/20... Training Step:4654... Training loss:2.0654... 0.1925 sec/batch\n",
      "Epoch:17/20... Training Step:4655... Training loss:2.0365... 0.1972 sec/batch\n",
      "Epoch:17/20... Training Step:4656... Training loss:2.0708... 0.2230 sec/batch\n",
      "Epoch:17/20... Training Step:4657... Training loss:2.0668... 0.1909 sec/batch\n",
      "Epoch:17/20... Training Step:4658... Training loss:2.0757... 0.1925 sec/batch\n",
      "Epoch:17/20... Training Step:4659... Training loss:2.0619... 0.1925 sec/batch\n",
      "Epoch:17/20... Training Step:4660... Training loss:2.0631... 0.1948 sec/batch\n",
      "Epoch:17/20... Training Step:4661... Training loss:2.0755... 0.1995 sec/batch\n",
      "Epoch:17/20... Training Step:4662... Training loss:2.0369... 0.2037 sec/batch\n",
      "Epoch:17/20... Training Step:4663... Training loss:2.0398... 0.2006 sec/batch\n",
      "Epoch:17/20... Training Step:4664... Training loss:2.0762... 0.1974 sec/batch\n",
      "Epoch:17/20... Training Step:4665... Training loss:2.0357... 0.1943 sec/batch\n",
      "Epoch:17/20... Training Step:4666... Training loss:2.0384... 0.1927 sec/batch\n",
      "Epoch:17/20... Training Step:4667... Training loss:2.0665... 0.1970 sec/batch\n",
      "Epoch:17/20... Training Step:4668... Training loss:2.0578... 0.1922 sec/batch\n",
      "Epoch:17/20... Training Step:4669... Training loss:2.0711... 0.1916 sec/batch\n",
      "Epoch:17/20... Training Step:4670... Training loss:2.0779... 0.1945 sec/batch\n",
      "Epoch:17/20... Training Step:4671... Training loss:2.0495... 0.1945 sec/batch\n",
      "Epoch:17/20... Training Step:4672... Training loss:2.0300... 0.2040 sec/batch\n",
      "Epoch:17/20... Training Step:4673... Training loss:2.0415... 0.2016 sec/batch\n",
      "Epoch:17/20... Training Step:4674... Training loss:2.0528... 0.2088 sec/batch\n",
      "Epoch:17/20... Training Step:4675... Training loss:2.0691... 0.1998 sec/batch\n",
      "Epoch:17/20... Training Step:4676... Training loss:2.0532... 0.1945 sec/batch\n",
      "Epoch:17/20... Training Step:4677... Training loss:2.0714... 0.1980 sec/batch\n",
      "Epoch:17/20... Training Step:4678... Training loss:2.0733... 0.2105 sec/batch\n",
      "Epoch:17/20... Training Step:4679... Training loss:2.0438... 0.1973 sec/batch\n",
      "Epoch:17/20... Training Step:4680... Training loss:2.0123... 0.2034 sec/batch\n",
      "Epoch:17/20... Training Step:4681... Training loss:2.0401... 0.1925 sec/batch\n",
      "Epoch:17/20... Training Step:4682... Training loss:2.0458... 0.1926 sec/batch\n",
      "Epoch:17/20... Training Step:4683... Training loss:2.0686... 0.2096 sec/batch\n",
      "Epoch:17/20... Training Step:4684... Training loss:2.0555... 0.2126 sec/batch\n",
      "Epoch:17/20... Training Step:4685... Training loss:2.0756... 0.2002 sec/batch\n",
      "Epoch:17/20... Training Step:4686... Training loss:2.0712... 0.2088 sec/batch\n",
      "Epoch:17/20... Training Step:4687... Training loss:2.0729... 0.1966 sec/batch\n",
      "Epoch:17/20... Training Step:4688... Training loss:2.0508... 0.2080 sec/batch\n",
      "Epoch:17/20... Training Step:4689... Training loss:2.0118... 0.2084 sec/batch\n",
      "Epoch:17/20... Training Step:4690... Training loss:2.0846... 0.2110 sec/batch\n",
      "Epoch:17/20... Training Step:4691... Training loss:2.0328... 0.1925 sec/batch\n",
      "Epoch:17/20... Training Step:4692... Training loss:2.0491... 0.2021 sec/batch\n",
      "Epoch:17/20... Training Step:4693... Training loss:2.0476... 0.1990 sec/batch\n",
      "Epoch:17/20... Training Step:4694... Training loss:2.0073... 0.1972 sec/batch\n",
      "Epoch:17/20... Training Step:4695... Training loss:2.0141... 0.2012 sec/batch\n",
      "Epoch:17/20... Training Step:4696... Training loss:2.0262... 0.1956 sec/batch\n",
      "Epoch:17/20... Training Step:4697... Training loss:2.0255... 0.1923 sec/batch\n",
      "Epoch:17/20... Training Step:4698... Training loss:2.0437... 0.1947 sec/batch\n",
      "Epoch:17/20... Training Step:4699... Training loss:2.0527... 0.1982 sec/batch\n",
      "Epoch:17/20... Training Step:4700... Training loss:2.0455... 0.2024 sec/batch\n",
      "Epoch:17/20... Training Step:4701... Training loss:2.0384... 0.2035 sec/batch\n",
      "Epoch:17/20... Training Step:4702... Training loss:2.0333... 0.1968 sec/batch\n",
      "Epoch:17/20... Training Step:4703... Training loss:2.0038... 0.1922 sec/batch\n",
      "Epoch:17/20... Training Step:4704... Training loss:2.0321... 0.1964 sec/batch\n",
      "Epoch:17/20... Training Step:4705... Training loss:2.0266... 0.1928 sec/batch\n",
      "Epoch:17/20... Training Step:4706... Training loss:1.9981... 0.1918 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:17/20... Training Step:4707... Training loss:1.9943... 0.1932 sec/batch\n",
      "Epoch:17/20... Training Step:4708... Training loss:2.0280... 0.2090 sec/batch\n",
      "Epoch:17/20... Training Step:4709... Training loss:2.0229... 0.2015 sec/batch\n",
      "Epoch:17/20... Training Step:4710... Training loss:1.9969... 0.2170 sec/batch\n",
      "Epoch:17/20... Training Step:4711... Training loss:1.9919... 0.2003 sec/batch\n",
      "Epoch:17/20... Training Step:4712... Training loss:2.0056... 0.2167 sec/batch\n",
      "Epoch:17/20... Training Step:4713... Training loss:2.0119... 0.1912 sec/batch\n",
      "Epoch:17/20... Training Step:4714... Training loss:2.0016... 0.2091 sec/batch\n",
      "Epoch:17/20... Training Step:4715... Training loss:1.9685... 0.1932 sec/batch\n",
      "Epoch:17/20... Training Step:4716... Training loss:1.9970... 0.1929 sec/batch\n",
      "Epoch:17/20... Training Step:4717... Training loss:1.9875... 0.2069 sec/batch\n",
      "Epoch:17/20... Training Step:4718... Training loss:1.9927... 0.1950 sec/batch\n",
      "Epoch:17/20... Training Step:4719... Training loss:2.0150... 0.1959 sec/batch\n",
      "Epoch:17/20... Training Step:4720... Training loss:2.0006... 0.1925 sec/batch\n",
      "Epoch:17/20... Training Step:4721... Training loss:1.9732... 0.1928 sec/batch\n",
      "Epoch:17/20... Training Step:4722... Training loss:1.9883... 0.2112 sec/batch\n",
      "Epoch:17/20... Training Step:4723... Training loss:2.0234... 0.1913 sec/batch\n",
      "Epoch:17/20... Training Step:4724... Training loss:2.0483... 0.2033 sec/batch\n",
      "Epoch:17/20... Training Step:4725... Training loss:2.0434... 0.1968 sec/batch\n",
      "Epoch:17/20... Training Step:4726... Training loss:1.9953... 0.2016 sec/batch\n",
      "Epoch:17/20... Training Step:4727... Training loss:2.0255... 0.1918 sec/batch\n",
      "Epoch:17/20... Training Step:4728... Training loss:2.0322... 0.2041 sec/batch\n",
      "Epoch:17/20... Training Step:4729... Training loss:2.0198... 0.1921 sec/batch\n",
      "Epoch:17/20... Training Step:4730... Training loss:2.0360... 0.1946 sec/batch\n",
      "Epoch:17/20... Training Step:4731... Training loss:2.0301... 0.2042 sec/batch\n",
      "Epoch:17/20... Training Step:4732... Training loss:2.0204... 0.1917 sec/batch\n",
      "Epoch:17/20... Training Step:4733... Training loss:2.0440... 0.2066 sec/batch\n",
      "Epoch:17/20... Training Step:4734... Training loss:2.0224... 0.1912 sec/batch\n",
      "Epoch:17/20... Training Step:4735... Training loss:2.0094... 0.2176 sec/batch\n",
      "Epoch:17/20... Training Step:4736... Training loss:2.0408... 0.2040 sec/batch\n",
      "Epoch:17/20... Training Step:4737... Training loss:2.0338... 0.1972 sec/batch\n",
      "Epoch:17/20... Training Step:4738... Training loss:2.0509... 0.2027 sec/batch\n",
      "Epoch:17/20... Training Step:4739... Training loss:1.9990... 0.2077 sec/batch\n",
      "Epoch:17/20... Training Step:4740... Training loss:2.0396... 0.1965 sec/batch\n",
      "Epoch:17/20... Training Step:4741... Training loss:2.0322... 0.2032 sec/batch\n",
      "Epoch:17/20... Training Step:4742... Training loss:2.0306... 0.1920 sec/batch\n",
      "Epoch:17/20... Training Step:4743... Training loss:2.0516... 0.1995 sec/batch\n",
      "Epoch:17/20... Training Step:4744... Training loss:2.0454... 0.1947 sec/batch\n",
      "Epoch:17/20... Training Step:4745... Training loss:2.0874... 0.1937 sec/batch\n",
      "Epoch:17/20... Training Step:4746... Training loss:2.0531... 0.1964 sec/batch\n",
      "Epoch:17/20... Training Step:4747... Training loss:2.0571... 0.1927 sec/batch\n",
      "Epoch:17/20... Training Step:4748... Training loss:2.0221... 0.1924 sec/batch\n",
      "Epoch:17/20... Training Step:4749... Training loss:2.0367... 0.1924 sec/batch\n",
      "Epoch:17/20... Training Step:4750... Training loss:2.0423... 0.1920 sec/batch\n",
      "Epoch:17/20... Training Step:4751... Training loss:2.0311... 0.1974 sec/batch\n",
      "Epoch:17/20... Training Step:4752... Training loss:2.0056... 0.2055 sec/batch\n",
      "Epoch:17/20... Training Step:4753... Training loss:1.9948... 0.2122 sec/batch\n",
      "Epoch:17/20... Training Step:4754... Training loss:2.0413... 0.2029 sec/batch\n",
      "Epoch:17/20... Training Step:4755... Training loss:2.0146... 0.1923 sec/batch\n",
      "Epoch:17/20... Training Step:4756... Training loss:2.0556... 0.1953 sec/batch\n",
      "Epoch:17/20... Training Step:4757... Training loss:2.0510... 0.1930 sec/batch\n",
      "Epoch:17/20... Training Step:4758... Training loss:2.0354... 0.2090 sec/batch\n",
      "Epoch:17/20... Training Step:4759... Training loss:2.0407... 0.1917 sec/batch\n",
      "Epoch:17/20... Training Step:4760... Training loss:2.0297... 0.1985 sec/batch\n",
      "Epoch:17/20... Training Step:4761... Training loss:2.0403... 0.2013 sec/batch\n",
      "Epoch:17/20... Training Step:4762... Training loss:2.0573... 0.2116 sec/batch\n",
      "Epoch:17/20... Training Step:4763... Training loss:2.0264... 0.1971 sec/batch\n",
      "Epoch:17/20... Training Step:4764... Training loss:2.0537... 0.1923 sec/batch\n",
      "Epoch:17/20... Training Step:4765... Training loss:2.0265... 0.2014 sec/batch\n",
      "Epoch:17/20... Training Step:4766... Training loss:2.0637... 0.2064 sec/batch\n",
      "Epoch:17/20... Training Step:4767... Training loss:2.0709... 0.2028 sec/batch\n",
      "Epoch:17/20... Training Step:4768... Training loss:2.0726... 0.1916 sec/batch\n",
      "Epoch:17/20... Training Step:4769... Training loss:2.0781... 0.1922 sec/batch\n",
      "Epoch:17/20... Training Step:4770... Training loss:2.1103... 0.1939 sec/batch\n",
      "Epoch:17/20... Training Step:4771... Training loss:2.0710... 0.1943 sec/batch\n",
      "Epoch:17/20... Training Step:4772... Training loss:2.0629... 0.1930 sec/batch\n",
      "Epoch:17/20... Training Step:4773... Training loss:2.0819... 0.1943 sec/batch\n",
      "Epoch:17/20... Training Step:4774... Training loss:2.0815... 0.1996 sec/batch\n",
      "Epoch:17/20... Training Step:4775... Training loss:2.0640... 0.1999 sec/batch\n",
      "Epoch:17/20... Training Step:4776... Training loss:2.0351... 0.1973 sec/batch\n",
      "Epoch:17/20... Training Step:4777... Training loss:2.0450... 0.2032 sec/batch\n",
      "Epoch:17/20... Training Step:4778... Training loss:2.0476... 0.1921 sec/batch\n",
      "Epoch:17/20... Training Step:4779... Training loss:2.0565... 0.1971 sec/batch\n",
      "Epoch:17/20... Training Step:4780... Training loss:2.0489... 0.2079 sec/batch\n",
      "Epoch:17/20... Training Step:4781... Training loss:2.0442... 0.1925 sec/batch\n",
      "Epoch:17/20... Training Step:4782... Training loss:2.1010... 0.2121 sec/batch\n",
      "Epoch:17/20... Training Step:4783... Training loss:2.0466... 0.2075 sec/batch\n",
      "Epoch:17/20... Training Step:4784... Training loss:2.0209... 0.1934 sec/batch\n",
      "Epoch:17/20... Training Step:4785... Training loss:2.0503... 0.2055 sec/batch\n",
      "Epoch:17/20... Training Step:4786... Training loss:2.0658... 0.1949 sec/batch\n",
      "Epoch:17/20... Training Step:4787... Training loss:2.0092... 0.1950 sec/batch\n",
      "Epoch:17/20... Training Step:4788... Training loss:2.0206... 0.1943 sec/batch\n",
      "Epoch:17/20... Training Step:4789... Training loss:2.0279... 0.2021 sec/batch\n",
      "Epoch:17/20... Training Step:4790... Training loss:2.0390... 0.1990 sec/batch\n",
      "Epoch:17/20... Training Step:4791... Training loss:2.0038... 0.2076 sec/batch\n",
      "Epoch:17/20... Training Step:4792... Training loss:2.0184... 0.2172 sec/batch\n",
      "Epoch:17/20... Training Step:4793... Training loss:2.0447... 0.2102 sec/batch\n",
      "Epoch:17/20... Training Step:4794... Training loss:2.0041... 0.2183 sec/batch\n",
      "Epoch:17/20... Training Step:4795... Training loss:2.0241... 0.2140 sec/batch\n",
      "Epoch:17/20... Training Step:4796... Training loss:2.0571... 0.1968 sec/batch\n",
      "Epoch:17/20... Training Step:4797... Training loss:2.0244... 0.2148 sec/batch\n",
      "Epoch:17/20... Training Step:4798... Training loss:2.0267... 0.2053 sec/batch\n",
      "Epoch:17/20... Training Step:4799... Training loss:2.0192... 0.1949 sec/batch\n",
      "Epoch:17/20... Training Step:4800... Training loss:2.0083... 0.1948 sec/batch\n",
      "Epoch:17/20... Training Step:4801... Training loss:2.0112... 0.1927 sec/batch\n",
      "Epoch:17/20... Training Step:4802... Training loss:2.0169... 0.1964 sec/batch\n",
      "Epoch:17/20... Training Step:4803... Training loss:2.0033... 0.1974 sec/batch\n",
      "Epoch:17/20... Training Step:4804... Training loss:2.0692... 0.2094 sec/batch\n",
      "Epoch:17/20... Training Step:4805... Training loss:2.0495... 0.2178 sec/batch\n",
      "Epoch:17/20... Training Step:4806... Training loss:2.0814... 0.2035 sec/batch\n",
      "Epoch:17/20... Training Step:4807... Training loss:2.0752... 0.2016 sec/batch\n",
      "Epoch:17/20... Training Step:4808... Training loss:2.0789... 0.1936 sec/batch\n",
      "Epoch:17/20... Training Step:4809... Training loss:2.0260... 0.1989 sec/batch\n",
      "Epoch:17/20... Training Step:4810... Training loss:2.0120... 0.2018 sec/batch\n",
      "Epoch:17/20... Training Step:4811... Training loss:2.0455... 0.2048 sec/batch\n",
      "Epoch:17/20... Training Step:4812... Training loss:2.0121... 0.1992 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:17/20... Training Step:4813... Training loss:2.0150... 0.1990 sec/batch\n",
      "Epoch:17/20... Training Step:4814... Training loss:2.0338... 0.1959 sec/batch\n",
      "Epoch:17/20... Training Step:4815... Training loss:2.0321... 0.1951 sec/batch\n",
      "Epoch:17/20... Training Step:4816... Training loss:2.0328... 0.1941 sec/batch\n",
      "Epoch:17/20... Training Step:4817... Training loss:2.0145... 0.2009 sec/batch\n",
      "Epoch:17/20... Training Step:4818... Training loss:2.0163... 0.2022 sec/batch\n",
      "Epoch:17/20... Training Step:4819... Training loss:2.0148... 0.1930 sec/batch\n",
      "Epoch:17/20... Training Step:4820... Training loss:2.0429... 0.2041 sec/batch\n",
      "Epoch:17/20... Training Step:4821... Training loss:2.0292... 0.2012 sec/batch\n",
      "Epoch:17/20... Training Step:4822... Training loss:2.0488... 0.1957 sec/batch\n",
      "Epoch:17/20... Training Step:4823... Training loss:2.0339... 0.1945 sec/batch\n",
      "Epoch:17/20... Training Step:4824... Training loss:2.0209... 0.2065 sec/batch\n",
      "Epoch:17/20... Training Step:4825... Training loss:2.0335... 0.1930 sec/batch\n",
      "Epoch:17/20... Training Step:4826... Training loss:2.0010... 0.2047 sec/batch\n",
      "Epoch:17/20... Training Step:4827... Training loss:2.0623... 0.2162 sec/batch\n",
      "Epoch:17/20... Training Step:4828... Training loss:2.0379... 0.1912 sec/batch\n",
      "Epoch:17/20... Training Step:4829... Training loss:2.0383... 0.1969 sec/batch\n",
      "Epoch:17/20... Training Step:4830... Training loss:2.0400... 0.1909 sec/batch\n",
      "Epoch:17/20... Training Step:4831... Training loss:2.0124... 0.1956 sec/batch\n",
      "Epoch:17/20... Training Step:4832... Training loss:2.0385... 0.1948 sec/batch\n",
      "Epoch:17/20... Training Step:4833... Training loss:2.0869... 0.1989 sec/batch\n",
      "Epoch:17/20... Training Step:4834... Training loss:2.0446... 0.1919 sec/batch\n",
      "Epoch:17/20... Training Step:4835... Training loss:2.0633... 0.1998 sec/batch\n",
      "Epoch:17/20... Training Step:4836... Training loss:2.0428... 0.1930 sec/batch\n",
      "Epoch:17/20... Training Step:4837... Training loss:2.0287... 0.1938 sec/batch\n",
      "Epoch:17/20... Training Step:4838... Training loss:2.0635... 0.2083 sec/batch\n",
      "Epoch:17/20... Training Step:4839... Training loss:2.0407... 0.2043 sec/batch\n",
      "Epoch:17/20... Training Step:4840... Training loss:2.0380... 0.2328 sec/batch\n",
      "Epoch:17/20... Training Step:4841... Training loss:2.0641... 0.2190 sec/batch\n",
      "Epoch:17/20... Training Step:4842... Training loss:2.0600... 0.1928 sec/batch\n",
      "Epoch:17/20... Training Step:4843... Training loss:2.0567... 0.2055 sec/batch\n",
      "Epoch:17/20... Training Step:4844... Training loss:2.0150... 0.1923 sec/batch\n",
      "Epoch:17/20... Training Step:4845... Training loss:1.9826... 0.2046 sec/batch\n",
      "Epoch:17/20... Training Step:4846... Training loss:2.0197... 0.1931 sec/batch\n",
      "Epoch:17/20... Training Step:4847... Training loss:2.0174... 0.2031 sec/batch\n",
      "Epoch:17/20... Training Step:4848... Training loss:2.0281... 0.2015 sec/batch\n",
      "Epoch:17/20... Training Step:4849... Training loss:2.0023... 0.2014 sec/batch\n",
      "Epoch:17/20... Training Step:4850... Training loss:2.0052... 0.1911 sec/batch\n",
      "Epoch:17/20... Training Step:4851... Training loss:2.0114... 0.2017 sec/batch\n",
      "Epoch:17/20... Training Step:4852... Training loss:1.9851... 0.1968 sec/batch\n",
      "Epoch:17/20... Training Step:4853... Training loss:1.9785... 0.2021 sec/batch\n",
      "Epoch:17/20... Training Step:4854... Training loss:2.0502... 0.1945 sec/batch\n",
      "Epoch:17/20... Training Step:4855... Training loss:1.9776... 0.2086 sec/batch\n",
      "Epoch:17/20... Training Step:4856... Training loss:1.9871... 0.2047 sec/batch\n",
      "Epoch:17/20... Training Step:4857... Training loss:1.9976... 0.1979 sec/batch\n",
      "Epoch:17/20... Training Step:4858... Training loss:1.9825... 0.2040 sec/batch\n",
      "Epoch:17/20... Training Step:4859... Training loss:2.0031... 0.1918 sec/batch\n",
      "Epoch:17/20... Training Step:4860... Training loss:2.0216... 0.1993 sec/batch\n",
      "Epoch:17/20... Training Step:4861... Training loss:2.0319... 0.1980 sec/batch\n",
      "Epoch:17/20... Training Step:4862... Training loss:2.0443... 0.1979 sec/batch\n",
      "Epoch:17/20... Training Step:4863... Training loss:2.0127... 0.1931 sec/batch\n",
      "Epoch:17/20... Training Step:4864... Training loss:1.9861... 0.1982 sec/batch\n",
      "Epoch:17/20... Training Step:4865... Training loss:2.0339... 0.1998 sec/batch\n",
      "Epoch:17/20... Training Step:4866... Training loss:1.9970... 0.2174 sec/batch\n",
      "Epoch:17/20... Training Step:4867... Training loss:2.0034... 0.1985 sec/batch\n",
      "Epoch:17/20... Training Step:4868... Training loss:2.0137... 0.2048 sec/batch\n",
      "Epoch:17/20... Training Step:4869... Training loss:2.0149... 0.1921 sec/batch\n",
      "Epoch:17/20... Training Step:4870... Training loss:2.0138... 0.2014 sec/batch\n",
      "Epoch:17/20... Training Step:4871... Training loss:1.9836... 0.2024 sec/batch\n",
      "Epoch:17/20... Training Step:4872... Training loss:2.0248... 0.2055 sec/batch\n",
      "Epoch:17/20... Training Step:4873... Training loss:1.9967... 0.1924 sec/batch\n",
      "Epoch:17/20... Training Step:4874... Training loss:1.9994... 0.2285 sec/batch\n",
      "Epoch:17/20... Training Step:4875... Training loss:2.0162... 0.1941 sec/batch\n",
      "Epoch:17/20... Training Step:4876... Training loss:2.0311... 0.2040 sec/batch\n",
      "Epoch:17/20... Training Step:4877... Training loss:1.9999... 0.1975 sec/batch\n",
      "Epoch:17/20... Training Step:4878... Training loss:2.0321... 0.1970 sec/batch\n",
      "Epoch:17/20... Training Step:4879... Training loss:1.9788... 0.1944 sec/batch\n",
      "Epoch:17/20... Training Step:4880... Training loss:1.9968... 0.1942 sec/batch\n",
      "Epoch:17/20... Training Step:4881... Training loss:2.0246... 0.1926 sec/batch\n",
      "Epoch:17/20... Training Step:4882... Training loss:2.0350... 0.1925 sec/batch\n",
      "Epoch:17/20... Training Step:4883... Training loss:2.0644... 0.2112 sec/batch\n",
      "Epoch:17/20... Training Step:4884... Training loss:2.0138... 0.2037 sec/batch\n",
      "Epoch:17/20... Training Step:4885... Training loss:2.0377... 0.2035 sec/batch\n",
      "Epoch:17/20... Training Step:4886... Training loss:2.0364... 0.2121 sec/batch\n",
      "Epoch:17/20... Training Step:4887... Training loss:2.0270... 0.1909 sec/batch\n",
      "Epoch:17/20... Training Step:4888... Training loss:2.0249... 0.2101 sec/batch\n",
      "Epoch:17/20... Training Step:4889... Training loss:2.0005... 0.2259 sec/batch\n",
      "Epoch:17/20... Training Step:4890... Training loss:2.0350... 0.2012 sec/batch\n",
      "Epoch:17/20... Training Step:4891... Training loss:2.0187... 0.2110 sec/batch\n",
      "Epoch:17/20... Training Step:4892... Training loss:2.0530... 0.2050 sec/batch\n",
      "Epoch:17/20... Training Step:4893... Training loss:2.0159... 0.2085 sec/batch\n",
      "Epoch:17/20... Training Step:4894... Training loss:2.0188... 0.1931 sec/batch\n",
      "Epoch:17/20... Training Step:4895... Training loss:2.0532... 0.1922 sec/batch\n",
      "Epoch:17/20... Training Step:4896... Training loss:2.0364... 0.1971 sec/batch\n",
      "Epoch:17/20... Training Step:4897... Training loss:2.0005... 0.2062 sec/batch\n",
      "Epoch:17/20... Training Step:4898... Training loss:2.0375... 0.1929 sec/batch\n",
      "Epoch:17/20... Training Step:4899... Training loss:2.0227... 0.2136 sec/batch\n",
      "Epoch:17/20... Training Step:4900... Training loss:2.0441... 0.2091 sec/batch\n",
      "Epoch:17/20... Training Step:4901... Training loss:2.0475... 0.1988 sec/batch\n",
      "Epoch:17/20... Training Step:4902... Training loss:2.0047... 0.2101 sec/batch\n",
      "Epoch:17/20... Training Step:4903... Training loss:1.9875... 0.1930 sec/batch\n",
      "Epoch:17/20... Training Step:4904... Training loss:1.9640... 0.2032 sec/batch\n",
      "Epoch:17/20... Training Step:4905... Training loss:1.9823... 0.2103 sec/batch\n",
      "Epoch:17/20... Training Step:4906... Training loss:1.9865... 0.1919 sec/batch\n",
      "Epoch:17/20... Training Step:4907... Training loss:2.0483... 0.1996 sec/batch\n",
      "Epoch:17/20... Training Step:4908... Training loss:1.9968... 0.1999 sec/batch\n",
      "Epoch:17/20... Training Step:4909... Training loss:1.9834... 0.1988 sec/batch\n",
      "Epoch:17/20... Training Step:4910... Training loss:1.9901... 0.1950 sec/batch\n",
      "Epoch:17/20... Training Step:4911... Training loss:1.9861... 0.2174 sec/batch\n",
      "Epoch:17/20... Training Step:4912... Training loss:2.0281... 0.1919 sec/batch\n",
      "Epoch:17/20... Training Step:4913... Training loss:2.0547... 0.1940 sec/batch\n",
      "Epoch:17/20... Training Step:4914... Training loss:1.9818... 0.1991 sec/batch\n",
      "Epoch:17/20... Training Step:4915... Training loss:2.0348... 0.1969 sec/batch\n",
      "Epoch:17/20... Training Step:4916... Training loss:2.0304... 0.1939 sec/batch\n",
      "Epoch:17/20... Training Step:4917... Training loss:2.0205... 0.1912 sec/batch\n",
      "Epoch:17/20... Training Step:4918... Training loss:2.0309... 0.1995 sec/batch\n",
      "Epoch:17/20... Training Step:4919... Training loss:1.9997... 0.1925 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:17/20... Training Step:4920... Training loss:2.0323... 0.1991 sec/batch\n",
      "Epoch:17/20... Training Step:4921... Training loss:2.0209... 0.1944 sec/batch\n",
      "Epoch:17/20... Training Step:4922... Training loss:2.0316... 0.1949 sec/batch\n",
      "Epoch:17/20... Training Step:4923... Training loss:2.0301... 0.2143 sec/batch\n",
      "Epoch:17/20... Training Step:4924... Training loss:2.0421... 0.2077 sec/batch\n",
      "Epoch:17/20... Training Step:4925... Training loss:2.0602... 0.1918 sec/batch\n",
      "Epoch:17/20... Training Step:4926... Training loss:2.0328... 0.1923 sec/batch\n",
      "Epoch:17/20... Training Step:4927... Training loss:1.9973... 0.1982 sec/batch\n",
      "Epoch:17/20... Training Step:4928... Training loss:2.0027... 0.2086 sec/batch\n",
      "Epoch:17/20... Training Step:4929... Training loss:2.0022... 0.1924 sec/batch\n",
      "Epoch:17/20... Training Step:4930... Training loss:1.9982... 0.1954 sec/batch\n",
      "Epoch:18/20... Training Step:4931... Training loss:2.1210... 0.1920 sec/batch\n",
      "Epoch:18/20... Training Step:4932... Training loss:2.0114... 0.1984 sec/batch\n",
      "Epoch:18/20... Training Step:4933... Training loss:2.0189... 0.1911 sec/batch\n",
      "Epoch:18/20... Training Step:4934... Training loss:2.0412... 0.1964 sec/batch\n",
      "Epoch:18/20... Training Step:4935... Training loss:2.0137... 0.1929 sec/batch\n",
      "Epoch:18/20... Training Step:4936... Training loss:2.0104... 0.1995 sec/batch\n",
      "Epoch:18/20... Training Step:4937... Training loss:2.0188... 0.1927 sec/batch\n",
      "Epoch:18/20... Training Step:4938... Training loss:2.0153... 0.2041 sec/batch\n",
      "Epoch:18/20... Training Step:4939... Training loss:2.0274... 0.2022 sec/batch\n",
      "Epoch:18/20... Training Step:4940... Training loss:2.0262... 0.2041 sec/batch\n",
      "Epoch:18/20... Training Step:4941... Training loss:2.0484... 0.1999 sec/batch\n",
      "Epoch:18/20... Training Step:4942... Training loss:2.0349... 0.2131 sec/batch\n",
      "Epoch:18/20... Training Step:4943... Training loss:2.0377... 0.1980 sec/batch\n",
      "Epoch:18/20... Training Step:4944... Training loss:2.0415... 0.2010 sec/batch\n",
      "Epoch:18/20... Training Step:4945... Training loss:2.0063... 0.2006 sec/batch\n",
      "Epoch:18/20... Training Step:4946... Training loss:2.0357... 0.2161 sec/batch\n",
      "Epoch:18/20... Training Step:4947... Training loss:2.0324... 0.2095 sec/batch\n",
      "Epoch:18/20... Training Step:4948... Training loss:2.0443... 0.2050 sec/batch\n",
      "Epoch:18/20... Training Step:4949... Training loss:2.0411... 0.1925 sec/batch\n",
      "Epoch:18/20... Training Step:4950... Training loss:2.0437... 0.1947 sec/batch\n",
      "Epoch:18/20... Training Step:4951... Training loss:2.0456... 0.1914 sec/batch\n",
      "Epoch:18/20... Training Step:4952... Training loss:2.0133... 0.2028 sec/batch\n",
      "Epoch:18/20... Training Step:4953... Training loss:2.0177... 0.2048 sec/batch\n",
      "Epoch:18/20... Training Step:4954... Training loss:2.0583... 0.1981 sec/batch\n",
      "Epoch:18/20... Training Step:4955... Training loss:2.0136... 0.2019 sec/batch\n",
      "Epoch:18/20... Training Step:4956... Training loss:2.0152... 0.1946 sec/batch\n",
      "Epoch:18/20... Training Step:4957... Training loss:2.0431... 0.1956 sec/batch\n",
      "Epoch:18/20... Training Step:4958... Training loss:2.0405... 0.2058 sec/batch\n",
      "Epoch:18/20... Training Step:4959... Training loss:2.0352... 0.2056 sec/batch\n",
      "Epoch:18/20... Training Step:4960... Training loss:2.0471... 0.1923 sec/batch\n",
      "Epoch:18/20... Training Step:4961... Training loss:2.0284... 0.2080 sec/batch\n",
      "Epoch:18/20... Training Step:4962... Training loss:2.0045... 0.2137 sec/batch\n",
      "Epoch:18/20... Training Step:4963... Training loss:2.0180... 0.1961 sec/batch\n",
      "Epoch:18/20... Training Step:4964... Training loss:2.0247... 0.2057 sec/batch\n",
      "Epoch:18/20... Training Step:4965... Training loss:2.0507... 0.1930 sec/batch\n",
      "Epoch:18/20... Training Step:4966... Training loss:2.0306... 0.2329 sec/batch\n",
      "Epoch:18/20... Training Step:4967... Training loss:2.0504... 0.2133 sec/batch\n",
      "Epoch:18/20... Training Step:4968... Training loss:2.0289... 0.2129 sec/batch\n",
      "Epoch:18/20... Training Step:4969... Training loss:2.0053... 0.1935 sec/batch\n",
      "Epoch:18/20... Training Step:4970... Training loss:1.9966... 0.1940 sec/batch\n",
      "Epoch:18/20... Training Step:4971... Training loss:2.0152... 0.1962 sec/batch\n",
      "Epoch:18/20... Training Step:4972... Training loss:2.0260... 0.2147 sec/batch\n",
      "Epoch:18/20... Training Step:4973... Training loss:2.0591... 0.2013 sec/batch\n",
      "Epoch:18/20... Training Step:4974... Training loss:2.0252... 0.2038 sec/batch\n",
      "Epoch:18/20... Training Step:4975... Training loss:2.0521... 0.2033 sec/batch\n",
      "Epoch:18/20... Training Step:4976... Training loss:2.0514... 0.2144 sec/batch\n",
      "Epoch:18/20... Training Step:4977... Training loss:2.0585... 0.1924 sec/batch\n",
      "Epoch:18/20... Training Step:4978... Training loss:2.0210... 0.2240 sec/batch\n",
      "Epoch:18/20... Training Step:4979... Training loss:1.9874... 0.1924 sec/batch\n",
      "Epoch:18/20... Training Step:4980... Training loss:2.0551... 0.1944 sec/batch\n",
      "Epoch:18/20... Training Step:4981... Training loss:2.0011... 0.1933 sec/batch\n",
      "Epoch:18/20... Training Step:4982... Training loss:2.0192... 0.1938 sec/batch\n",
      "Epoch:18/20... Training Step:4983... Training loss:2.0146... 0.2060 sec/batch\n",
      "Epoch:18/20... Training Step:4984... Training loss:1.9734... 0.1998 sec/batch\n",
      "Epoch:18/20... Training Step:4985... Training loss:2.0083... 0.2045 sec/batch\n",
      "Epoch:18/20... Training Step:4986... Training loss:1.9963... 0.1907 sec/batch\n",
      "Epoch:18/20... Training Step:4987... Training loss:1.9969... 0.2087 sec/batch\n",
      "Epoch:18/20... Training Step:4988... Training loss:2.0123... 0.1924 sec/batch\n",
      "Epoch:18/20... Training Step:4989... Training loss:2.0210... 0.1929 sec/batch\n",
      "Epoch:18/20... Training Step:4990... Training loss:2.0182... 0.2011 sec/batch\n",
      "Epoch:18/20... Training Step:4991... Training loss:2.0238... 0.1932 sec/batch\n",
      "Epoch:18/20... Training Step:4992... Training loss:2.0114... 0.1932 sec/batch\n",
      "Epoch:18/20... Training Step:4993... Training loss:1.9888... 0.2182 sec/batch\n",
      "Epoch:18/20... Training Step:4994... Training loss:2.0135... 0.2060 sec/batch\n",
      "Epoch:18/20... Training Step:4995... Training loss:2.0069... 0.2071 sec/batch\n",
      "Epoch:18/20... Training Step:4996... Training loss:1.9709... 0.1934 sec/batch\n",
      "Epoch:18/20... Training Step:4997... Training loss:1.9764... 0.2061 sec/batch\n",
      "Epoch:18/20... Training Step:4998... Training loss:1.9919... 0.1950 sec/batch\n",
      "Epoch:18/20... Training Step:4999... Training loss:1.9953... 0.2320 sec/batch\n",
      "Epoch:18/20... Training Step:5000... Training loss:1.9717... 0.2150 sec/batch\n",
      "Epoch:18/20... Training Step:5001... Training loss:1.9661... 0.1970 sec/batch\n",
      "Epoch:18/20... Training Step:5002... Training loss:1.9783... 0.1933 sec/batch\n",
      "Epoch:18/20... Training Step:5003... Training loss:1.9996... 0.1941 sec/batch\n",
      "Epoch:18/20... Training Step:5004... Training loss:1.9829... 0.2062 sec/batch\n",
      "Epoch:18/20... Training Step:5005... Training loss:1.9430... 0.2030 sec/batch\n",
      "Epoch:18/20... Training Step:5006... Training loss:1.9700... 0.1923 sec/batch\n",
      "Epoch:18/20... Training Step:5007... Training loss:1.9673... 0.2079 sec/batch\n",
      "Epoch:18/20... Training Step:5008... Training loss:1.9707... 0.2032 sec/batch\n",
      "Epoch:18/20... Training Step:5009... Training loss:1.9945... 0.1922 sec/batch\n",
      "Epoch:18/20... Training Step:5010... Training loss:1.9816... 0.2038 sec/batch\n",
      "Epoch:18/20... Training Step:5011... Training loss:1.9394... 0.1931 sec/batch\n",
      "Epoch:18/20... Training Step:5012... Training loss:1.9706... 0.2162 sec/batch\n",
      "Epoch:18/20... Training Step:5013... Training loss:1.9874... 0.2048 sec/batch\n",
      "Epoch:18/20... Training Step:5014... Training loss:2.0286... 0.2080 sec/batch\n",
      "Epoch:18/20... Training Step:5015... Training loss:2.0215... 0.2013 sec/batch\n",
      "Epoch:18/20... Training Step:5016... Training loss:1.9818... 0.1918 sec/batch\n",
      "Epoch:18/20... Training Step:5017... Training loss:2.0106... 0.1967 sec/batch\n",
      "Epoch:18/20... Training Step:5018... Training loss:2.0102... 0.1921 sec/batch\n",
      "Epoch:18/20... Training Step:5019... Training loss:2.0079... 0.2120 sec/batch\n",
      "Epoch:18/20... Training Step:5020... Training loss:2.0125... 0.1921 sec/batch\n",
      "Epoch:18/20... Training Step:5021... Training loss:2.0074... 0.1949 sec/batch\n",
      "Epoch:18/20... Training Step:5022... Training loss:1.9910... 0.1903 sec/batch\n",
      "Epoch:18/20... Training Step:5023... Training loss:2.0201... 0.2004 sec/batch\n",
      "Epoch:18/20... Training Step:5024... Training loss:2.0088... 0.1982 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:18/20... Training Step:5025... Training loss:1.9854... 0.2011 sec/batch\n",
      "Epoch:18/20... Training Step:5026... Training loss:2.0198... 0.2061 sec/batch\n",
      "Epoch:18/20... Training Step:5027... Training loss:2.0040... 0.1986 sec/batch\n",
      "Epoch:18/20... Training Step:5028... Training loss:2.0217... 0.2031 sec/batch\n",
      "Epoch:18/20... Training Step:5029... Training loss:1.9810... 0.1980 sec/batch\n",
      "Epoch:18/20... Training Step:5030... Training loss:2.0251... 0.1936 sec/batch\n",
      "Epoch:18/20... Training Step:5031... Training loss:2.0089... 0.1993 sec/batch\n",
      "Epoch:18/20... Training Step:5032... Training loss:2.0079... 0.1948 sec/batch\n",
      "Epoch:18/20... Training Step:5033... Training loss:2.0369... 0.2011 sec/batch\n",
      "Epoch:18/20... Training Step:5034... Training loss:2.0179... 0.1923 sec/batch\n",
      "Epoch:18/20... Training Step:5035... Training loss:2.0535... 0.1925 sec/batch\n",
      "Epoch:18/20... Training Step:5036... Training loss:2.0101... 0.1964 sec/batch\n",
      "Epoch:18/20... Training Step:5037... Training loss:2.0357... 0.1950 sec/batch\n",
      "Epoch:18/20... Training Step:5038... Training loss:1.9934... 0.1914 sec/batch\n",
      "Epoch:18/20... Training Step:5039... Training loss:2.0223... 0.2061 sec/batch\n",
      "Epoch:18/20... Training Step:5040... Training loss:2.0146... 0.1934 sec/batch\n",
      "Epoch:18/20... Training Step:5041... Training loss:2.0033... 0.2043 sec/batch\n",
      "Epoch:18/20... Training Step:5042... Training loss:1.9818... 0.2057 sec/batch\n",
      "Epoch:18/20... Training Step:5043... Training loss:1.9666... 0.1920 sec/batch\n",
      "Epoch:18/20... Training Step:5044... Training loss:2.0202... 0.1930 sec/batch\n",
      "Epoch:18/20... Training Step:5045... Training loss:1.9729... 0.1908 sec/batch\n",
      "Epoch:18/20... Training Step:5046... Training loss:2.0240... 0.2053 sec/batch\n",
      "Epoch:18/20... Training Step:5047... Training loss:2.0304... 0.2027 sec/batch\n",
      "Epoch:18/20... Training Step:5048... Training loss:2.0024... 0.2039 sec/batch\n",
      "Epoch:18/20... Training Step:5049... Training loss:2.0233... 0.2095 sec/batch\n",
      "Epoch:18/20... Training Step:5050... Training loss:2.0148... 0.1911 sec/batch\n",
      "Epoch:18/20... Training Step:5051... Training loss:2.0091... 0.1937 sec/batch\n",
      "Epoch:18/20... Training Step:5052... Training loss:2.0269... 0.2030 sec/batch\n",
      "Epoch:18/20... Training Step:5053... Training loss:1.9959... 0.1920 sec/batch\n",
      "Epoch:18/20... Training Step:5054... Training loss:2.0415... 0.2090 sec/batch\n",
      "Epoch:18/20... Training Step:5055... Training loss:1.9996... 0.1930 sec/batch\n",
      "Epoch:18/20... Training Step:5056... Training loss:2.0336... 0.1993 sec/batch\n",
      "Epoch:18/20... Training Step:5057... Training loss:2.0452... 0.1951 sec/batch\n",
      "Epoch:18/20... Training Step:5058... Training loss:2.0507... 0.1930 sec/batch\n",
      "Epoch:18/20... Training Step:5059... Training loss:2.0731... 0.1952 sec/batch\n",
      "Epoch:18/20... Training Step:5060... Training loss:2.0868... 0.2013 sec/batch\n",
      "Epoch:18/20... Training Step:5061... Training loss:2.0508... 0.1939 sec/batch\n",
      "Epoch:18/20... Training Step:5062... Training loss:2.0394... 0.1948 sec/batch\n",
      "Epoch:18/20... Training Step:5063... Training loss:2.0575... 0.2041 sec/batch\n",
      "Epoch:18/20... Training Step:5064... Training loss:2.0645... 0.2118 sec/batch\n",
      "Epoch:18/20... Training Step:5065... Training loss:2.0325... 0.2086 sec/batch\n",
      "Epoch:18/20... Training Step:5066... Training loss:2.0068... 0.2118 sec/batch\n",
      "Epoch:18/20... Training Step:5067... Training loss:2.0302... 0.1923 sec/batch\n",
      "Epoch:18/20... Training Step:5068... Training loss:2.0170... 0.2119 sec/batch\n",
      "Epoch:18/20... Training Step:5069... Training loss:2.0217... 0.2095 sec/batch\n",
      "Epoch:18/20... Training Step:5070... Training loss:2.0188... 0.2138 sec/batch\n",
      "Epoch:18/20... Training Step:5071... Training loss:2.0196... 0.2108 sec/batch\n",
      "Epoch:18/20... Training Step:5072... Training loss:2.0840... 0.1919 sec/batch\n",
      "Epoch:18/20... Training Step:5073... Training loss:2.0236... 0.2074 sec/batch\n",
      "Epoch:18/20... Training Step:5074... Training loss:2.0055... 0.1929 sec/batch\n",
      "Epoch:18/20... Training Step:5075... Training loss:2.0168... 0.1965 sec/batch\n",
      "Epoch:18/20... Training Step:5076... Training loss:2.0374... 0.1935 sec/batch\n",
      "Epoch:18/20... Training Step:5077... Training loss:1.9938... 0.2164 sec/batch\n",
      "Epoch:18/20... Training Step:5078... Training loss:1.9970... 0.1986 sec/batch\n",
      "Epoch:18/20... Training Step:5079... Training loss:1.9977... 0.2129 sec/batch\n",
      "Epoch:18/20... Training Step:5080... Training loss:2.0182... 0.2092 sec/batch\n",
      "Epoch:18/20... Training Step:5081... Training loss:1.9719... 0.1990 sec/batch\n",
      "Epoch:18/20... Training Step:5082... Training loss:1.9993... 0.1921 sec/batch\n",
      "Epoch:18/20... Training Step:5083... Training loss:2.0203... 0.1980 sec/batch\n",
      "Epoch:18/20... Training Step:5084... Training loss:1.9807... 0.2017 sec/batch\n",
      "Epoch:18/20... Training Step:5085... Training loss:1.9969... 0.2033 sec/batch\n",
      "Epoch:18/20... Training Step:5086... Training loss:2.0353... 0.1979 sec/batch\n",
      "Epoch:18/20... Training Step:5087... Training loss:2.0043... 0.2078 sec/batch\n",
      "Epoch:18/20... Training Step:5088... Training loss:1.9990... 0.1943 sec/batch\n",
      "Epoch:18/20... Training Step:5089... Training loss:2.0031... 0.2031 sec/batch\n",
      "Epoch:18/20... Training Step:5090... Training loss:1.9935... 0.1994 sec/batch\n",
      "Epoch:18/20... Training Step:5091... Training loss:1.9766... 0.2012 sec/batch\n",
      "Epoch:18/20... Training Step:5092... Training loss:1.9910... 0.2104 sec/batch\n",
      "Epoch:18/20... Training Step:5093... Training loss:1.9757... 0.1960 sec/batch\n",
      "Epoch:18/20... Training Step:5094... Training loss:2.0395... 0.1940 sec/batch\n",
      "Epoch:18/20... Training Step:5095... Training loss:2.0212... 0.2103 sec/batch\n",
      "Epoch:18/20... Training Step:5096... Training loss:2.0559... 0.2024 sec/batch\n",
      "Epoch:18/20... Training Step:5097... Training loss:2.0498... 0.2159 sec/batch\n",
      "Epoch:18/20... Training Step:5098... Training loss:2.0467... 0.1906 sec/batch\n",
      "Epoch:18/20... Training Step:5099... Training loss:2.0049... 0.1958 sec/batch\n",
      "Epoch:18/20... Training Step:5100... Training loss:1.9935... 0.2028 sec/batch\n",
      "Epoch:18/20... Training Step:5101... Training loss:2.0166... 0.1958 sec/batch\n",
      "Epoch:18/20... Training Step:5102... Training loss:1.9923... 0.1945 sec/batch\n",
      "Epoch:18/20... Training Step:5103... Training loss:1.9934... 0.1924 sec/batch\n",
      "Epoch:18/20... Training Step:5104... Training loss:2.0122... 0.2067 sec/batch\n",
      "Epoch:18/20... Training Step:5105... Training loss:2.0118... 0.1947 sec/batch\n",
      "Epoch:18/20... Training Step:5106... Training loss:2.0100... 0.1982 sec/batch\n",
      "Epoch:18/20... Training Step:5107... Training loss:1.9793... 0.2082 sec/batch\n",
      "Epoch:18/20... Training Step:5108... Training loss:1.9850... 0.1912 sec/batch\n",
      "Epoch:18/20... Training Step:5109... Training loss:1.9799... 0.1941 sec/batch\n",
      "Epoch:18/20... Training Step:5110... Training loss:2.0237... 0.1934 sec/batch\n",
      "Epoch:18/20... Training Step:5111... Training loss:2.0003... 0.2047 sec/batch\n",
      "Epoch:18/20... Training Step:5112... Training loss:2.0321... 0.1954 sec/batch\n",
      "Epoch:18/20... Training Step:5113... Training loss:2.0102... 0.1936 sec/batch\n",
      "Epoch:18/20... Training Step:5114... Training loss:2.0037... 0.2054 sec/batch\n",
      "Epoch:18/20... Training Step:5115... Training loss:2.0119... 0.2081 sec/batch\n",
      "Epoch:18/20... Training Step:5116... Training loss:1.9846... 0.1972 sec/batch\n",
      "Epoch:18/20... Training Step:5117... Training loss:2.0350... 0.2036 sec/batch\n",
      "Epoch:18/20... Training Step:5118... Training loss:2.0219... 0.1922 sec/batch\n",
      "Epoch:18/20... Training Step:5119... Training loss:2.0205... 0.1941 sec/batch\n",
      "Epoch:18/20... Training Step:5120... Training loss:2.0191... 0.1968 sec/batch\n",
      "Epoch:18/20... Training Step:5121... Training loss:1.9924... 0.1961 sec/batch\n",
      "Epoch:18/20... Training Step:5122... Training loss:2.0252... 0.1922 sec/batch\n",
      "Epoch:18/20... Training Step:5123... Training loss:2.0584... 0.1935 sec/batch\n",
      "Epoch:18/20... Training Step:5124... Training loss:2.0260... 0.2065 sec/batch\n",
      "Epoch:18/20... Training Step:5125... Training loss:2.0227... 0.1959 sec/batch\n",
      "Epoch:18/20... Training Step:5126... Training loss:2.0238... 0.1953 sec/batch\n",
      "Epoch:18/20... Training Step:5127... Training loss:2.0026... 0.2019 sec/batch\n",
      "Epoch:18/20... Training Step:5128... Training loss:2.0378... 0.2019 sec/batch\n",
      "Epoch:18/20... Training Step:5129... Training loss:1.9974... 0.1922 sec/batch\n",
      "Epoch:18/20... Training Step:5130... Training loss:2.0049... 0.2136 sec/batch\n",
      "Epoch:18/20... Training Step:5131... Training loss:2.0351... 0.1931 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:18/20... Training Step:5132... Training loss:2.0344... 0.1925 sec/batch\n",
      "Epoch:18/20... Training Step:5133... Training loss:2.0331... 0.1935 sec/batch\n",
      "Epoch:18/20... Training Step:5134... Training loss:1.9894... 0.1940 sec/batch\n",
      "Epoch:18/20... Training Step:5135... Training loss:1.9582... 0.1966 sec/batch\n",
      "Epoch:18/20... Training Step:5136... Training loss:1.9898... 0.2028 sec/batch\n",
      "Epoch:18/20... Training Step:5137... Training loss:1.9928... 0.2025 sec/batch\n",
      "Epoch:18/20... Training Step:5138... Training loss:1.9846... 0.2090 sec/batch\n",
      "Epoch:18/20... Training Step:5139... Training loss:1.9766... 0.2129 sec/batch\n",
      "Epoch:18/20... Training Step:5140... Training loss:1.9718... 0.2091 sec/batch\n",
      "Epoch:18/20... Training Step:5141... Training loss:1.9775... 0.2013 sec/batch\n",
      "Epoch:18/20... Training Step:5142... Training loss:1.9696... 0.1995 sec/batch\n",
      "Epoch:18/20... Training Step:5143... Training loss:1.9395... 0.1950 sec/batch\n",
      "Epoch:18/20... Training Step:5144... Training loss:2.0254... 0.1912 sec/batch\n",
      "Epoch:18/20... Training Step:5145... Training loss:1.9623... 0.1938 sec/batch\n",
      "Epoch:18/20... Training Step:5146... Training loss:1.9541... 0.1929 sec/batch\n",
      "Epoch:18/20... Training Step:5147... Training loss:1.9715... 0.2138 sec/batch\n",
      "Epoch:18/20... Training Step:5148... Training loss:1.9600... 0.2054 sec/batch\n",
      "Epoch:18/20... Training Step:5149... Training loss:1.9882... 0.2010 sec/batch\n",
      "Epoch:18/20... Training Step:5150... Training loss:1.9975... 0.1957 sec/batch\n",
      "Epoch:18/20... Training Step:5151... Training loss:2.0077... 0.2062 sec/batch\n",
      "Epoch:18/20... Training Step:5152... Training loss:2.0270... 0.1969 sec/batch\n",
      "Epoch:18/20... Training Step:5153... Training loss:1.9960... 0.1937 sec/batch\n",
      "Epoch:18/20... Training Step:5154... Training loss:1.9650... 0.2049 sec/batch\n",
      "Epoch:18/20... Training Step:5155... Training loss:2.0014... 0.1930 sec/batch\n",
      "Epoch:18/20... Training Step:5156... Training loss:1.9736... 0.1979 sec/batch\n",
      "Epoch:18/20... Training Step:5157... Training loss:1.9790... 0.2014 sec/batch\n",
      "Epoch:18/20... Training Step:5158... Training loss:1.9836... 0.1962 sec/batch\n",
      "Epoch:18/20... Training Step:5159... Training loss:1.9893... 0.2153 sec/batch\n",
      "Epoch:18/20... Training Step:5160... Training loss:1.9770... 0.2188 sec/batch\n",
      "Epoch:18/20... Training Step:5161... Training loss:1.9651... 0.1958 sec/batch\n",
      "Epoch:18/20... Training Step:5162... Training loss:2.0073... 0.1952 sec/batch\n",
      "Epoch:18/20... Training Step:5163... Training loss:1.9765... 0.2041 sec/batch\n",
      "Epoch:18/20... Training Step:5164... Training loss:1.9847... 0.2071 sec/batch\n",
      "Epoch:18/20... Training Step:5165... Training loss:1.9911... 0.1928 sec/batch\n",
      "Epoch:18/20... Training Step:5166... Training loss:2.0173... 0.2091 sec/batch\n",
      "Epoch:18/20... Training Step:5167... Training loss:1.9790... 0.1942 sec/batch\n",
      "Epoch:18/20... Training Step:5168... Training loss:2.0118... 0.2072 sec/batch\n",
      "Epoch:18/20... Training Step:5169... Training loss:1.9513... 0.1934 sec/batch\n",
      "Epoch:18/20... Training Step:5170... Training loss:1.9736... 0.2105 sec/batch\n",
      "Epoch:18/20... Training Step:5171... Training loss:1.9933... 0.1927 sec/batch\n",
      "Epoch:18/20... Training Step:5172... Training loss:2.0204... 0.1948 sec/batch\n",
      "Epoch:18/20... Training Step:5173... Training loss:2.0340... 0.2055 sec/batch\n",
      "Epoch:18/20... Training Step:5174... Training loss:1.9949... 0.2041 sec/batch\n",
      "Epoch:18/20... Training Step:5175... Training loss:2.0358... 0.2101 sec/batch\n",
      "Epoch:18/20... Training Step:5176... Training loss:2.0118... 0.2047 sec/batch\n",
      "Epoch:18/20... Training Step:5177... Training loss:2.0063... 0.2028 sec/batch\n",
      "Epoch:18/20... Training Step:5178... Training loss:2.0062... 0.2152 sec/batch\n",
      "Epoch:18/20... Training Step:5179... Training loss:1.9904... 0.1917 sec/batch\n",
      "Epoch:18/20... Training Step:5180... Training loss:2.0107... 0.1959 sec/batch\n",
      "Epoch:18/20... Training Step:5181... Training loss:1.9953... 0.2051 sec/batch\n",
      "Epoch:18/20... Training Step:5182... Training loss:2.0259... 0.1928 sec/batch\n",
      "Epoch:18/20... Training Step:5183... Training loss:1.9845... 0.2184 sec/batch\n",
      "Epoch:18/20... Training Step:5184... Training loss:1.9842... 0.2211 sec/batch\n",
      "Epoch:18/20... Training Step:5185... Training loss:2.0347... 0.1908 sec/batch\n",
      "Epoch:18/20... Training Step:5186... Training loss:2.0119... 0.1950 sec/batch\n",
      "Epoch:18/20... Training Step:5187... Training loss:1.9766... 0.2136 sec/batch\n",
      "Epoch:18/20... Training Step:5188... Training loss:2.0275... 0.1932 sec/batch\n",
      "Epoch:18/20... Training Step:5189... Training loss:2.0049... 0.1969 sec/batch\n",
      "Epoch:18/20... Training Step:5190... Training loss:2.0249... 0.1931 sec/batch\n",
      "Epoch:18/20... Training Step:5191... Training loss:2.0255... 0.1935 sec/batch\n",
      "Epoch:18/20... Training Step:5192... Training loss:1.9863... 0.2022 sec/batch\n",
      "Epoch:18/20... Training Step:5193... Training loss:1.9654... 0.1947 sec/batch\n",
      "Epoch:18/20... Training Step:5194... Training loss:1.9449... 0.2122 sec/batch\n",
      "Epoch:18/20... Training Step:5195... Training loss:1.9622... 0.1967 sec/batch\n",
      "Epoch:18/20... Training Step:5196... Training loss:1.9764... 0.1968 sec/batch\n",
      "Epoch:18/20... Training Step:5197... Training loss:2.0214... 0.2012 sec/batch\n",
      "Epoch:18/20... Training Step:5198... Training loss:1.9802... 0.2012 sec/batch\n",
      "Epoch:18/20... Training Step:5199... Training loss:1.9678... 0.1937 sec/batch\n",
      "Epoch:18/20... Training Step:5200... Training loss:1.9740... 0.2148 sec/batch\n",
      "Epoch:18/20... Training Step:5201... Training loss:1.9670... 0.1922 sec/batch\n",
      "Epoch:18/20... Training Step:5202... Training loss:2.0171... 0.2019 sec/batch\n",
      "Epoch:18/20... Training Step:5203... Training loss:2.0302... 0.2084 sec/batch\n",
      "Epoch:18/20... Training Step:5204... Training loss:1.9586... 0.1943 sec/batch\n",
      "Epoch:18/20... Training Step:5205... Training loss:2.0158... 0.2156 sec/batch\n",
      "Epoch:18/20... Training Step:5206... Training loss:2.0044... 0.1967 sec/batch\n",
      "Epoch:18/20... Training Step:5207... Training loss:2.0078... 0.2065 sec/batch\n",
      "Epoch:18/20... Training Step:5208... Training loss:2.0005... 0.1909 sec/batch\n",
      "Epoch:18/20... Training Step:5209... Training loss:1.9766... 0.1945 sec/batch\n",
      "Epoch:18/20... Training Step:5210... Training loss:2.0042... 0.1946 sec/batch\n",
      "Epoch:18/20... Training Step:5211... Training loss:1.9856... 0.1969 sec/batch\n",
      "Epoch:18/20... Training Step:5212... Training loss:1.9951... 0.1907 sec/batch\n",
      "Epoch:18/20... Training Step:5213... Training loss:2.0160... 0.1914 sec/batch\n",
      "Epoch:18/20... Training Step:5214... Training loss:2.0220... 0.1984 sec/batch\n",
      "Epoch:18/20... Training Step:5215... Training loss:2.0293... 0.2123 sec/batch\n",
      "Epoch:18/20... Training Step:5216... Training loss:2.0061... 0.1988 sec/batch\n",
      "Epoch:18/20... Training Step:5217... Training loss:1.9762... 0.2176 sec/batch\n",
      "Epoch:18/20... Training Step:5218... Training loss:1.9750... 0.2034 sec/batch\n",
      "Epoch:18/20... Training Step:5219... Training loss:1.9735... 0.2014 sec/batch\n",
      "Epoch:18/20... Training Step:5220... Training loss:1.9837... 0.2057 sec/batch\n",
      "Epoch:19/20... Training Step:5221... Training loss:2.0967... 0.1928 sec/batch\n",
      "Epoch:19/20... Training Step:5222... Training loss:1.9941... 0.1936 sec/batch\n",
      "Epoch:19/20... Training Step:5223... Training loss:1.9895... 0.1939 sec/batch\n",
      "Epoch:19/20... Training Step:5224... Training loss:2.0260... 0.2028 sec/batch\n",
      "Epoch:19/20... Training Step:5225... Training loss:2.0001... 0.2038 sec/batch\n",
      "Epoch:19/20... Training Step:5226... Training loss:1.9901... 0.1940 sec/batch\n",
      "Epoch:19/20... Training Step:5227... Training loss:1.9905... 0.1965 sec/batch\n",
      "Epoch:19/20... Training Step:5228... Training loss:2.0092... 0.1998 sec/batch\n",
      "Epoch:19/20... Training Step:5229... Training loss:2.0089... 0.2093 sec/batch\n",
      "Epoch:19/20... Training Step:5230... Training loss:2.0051... 0.1911 sec/batch\n",
      "Epoch:19/20... Training Step:5231... Training loss:2.0284... 0.1954 sec/batch\n",
      "Epoch:19/20... Training Step:5232... Training loss:2.0180... 0.1930 sec/batch\n",
      "Epoch:19/20... Training Step:5233... Training loss:2.0201... 0.1941 sec/batch\n",
      "Epoch:19/20... Training Step:5234... Training loss:2.0138... 0.1947 sec/batch\n",
      "Epoch:19/20... Training Step:5235... Training loss:1.9890... 0.2154 sec/batch\n",
      "Epoch:19/20... Training Step:5236... Training loss:2.0073... 0.1957 sec/batch\n",
      "Epoch:19/20... Training Step:5237... Training loss:2.0169... 0.1985 sec/batch\n",
      "Epoch:19/20... Training Step:5238... Training loss:2.0311... 0.1915 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:19/20... Training Step:5239... Training loss:2.0208... 0.1930 sec/batch\n",
      "Epoch:19/20... Training Step:5240... Training loss:2.0201... 0.2058 sec/batch\n",
      "Epoch:19/20... Training Step:5241... Training loss:2.0224... 0.1955 sec/batch\n",
      "Epoch:19/20... Training Step:5242... Training loss:1.9935... 0.2004 sec/batch\n",
      "Epoch:19/20... Training Step:5243... Training loss:1.9931... 0.1949 sec/batch\n",
      "Epoch:19/20... Training Step:5244... Training loss:2.0362... 0.1931 sec/batch\n",
      "Epoch:19/20... Training Step:5245... Training loss:1.9839... 0.2026 sec/batch\n",
      "Epoch:19/20... Training Step:5246... Training loss:1.9913... 0.2147 sec/batch\n",
      "Epoch:19/20... Training Step:5247... Training loss:2.0236... 0.1958 sec/batch\n",
      "Epoch:19/20... Training Step:5248... Training loss:2.0202... 0.1919 sec/batch\n",
      "Epoch:19/20... Training Step:5249... Training loss:2.0168... 0.1956 sec/batch\n",
      "Epoch:19/20... Training Step:5250... Training loss:2.0326... 0.1975 sec/batch\n",
      "Epoch:19/20... Training Step:5251... Training loss:2.0069... 0.2042 sec/batch\n",
      "Epoch:19/20... Training Step:5252... Training loss:1.9881... 0.2048 sec/batch\n",
      "Epoch:19/20... Training Step:5253... Training loss:1.9933... 0.1919 sec/batch\n",
      "Epoch:19/20... Training Step:5254... Training loss:2.0046... 0.1954 sec/batch\n",
      "Epoch:19/20... Training Step:5255... Training loss:2.0296... 0.2012 sec/batch\n",
      "Epoch:19/20... Training Step:5256... Training loss:2.0101... 0.2088 sec/batch\n",
      "Epoch:19/20... Training Step:5257... Training loss:2.0212... 0.2032 sec/batch\n",
      "Epoch:19/20... Training Step:5258... Training loss:2.0166... 0.2098 sec/batch\n",
      "Epoch:19/20... Training Step:5259... Training loss:2.0002... 0.1917 sec/batch\n",
      "Epoch:19/20... Training Step:5260... Training loss:1.9720... 0.2059 sec/batch\n",
      "Epoch:19/20... Training Step:5261... Training loss:1.9883... 0.1931 sec/batch\n",
      "Epoch:19/20... Training Step:5262... Training loss:2.0103... 0.2056 sec/batch\n",
      "Epoch:19/20... Training Step:5263... Training loss:2.0312... 0.2113 sec/batch\n",
      "Epoch:19/20... Training Step:5264... Training loss:2.0023... 0.2066 sec/batch\n",
      "Epoch:19/20... Training Step:5265... Training loss:2.0324... 0.1992 sec/batch\n",
      "Epoch:19/20... Training Step:5266... Training loss:2.0237... 0.2022 sec/batch\n",
      "Epoch:19/20... Training Step:5267... Training loss:2.0255... 0.1921 sec/batch\n",
      "Epoch:19/20... Training Step:5268... Training loss:2.0016... 0.1931 sec/batch\n",
      "Epoch:19/20... Training Step:5269... Training loss:1.9622... 0.2053 sec/batch\n",
      "Epoch:19/20... Training Step:5270... Training loss:2.0309... 0.1932 sec/batch\n",
      "Epoch:19/20... Training Step:5271... Training loss:1.9858... 0.2192 sec/batch\n",
      "Epoch:19/20... Training Step:5272... Training loss:1.9890... 0.1919 sec/batch\n",
      "Epoch:19/20... Training Step:5273... Training loss:1.9973... 0.1937 sec/batch\n",
      "Epoch:19/20... Training Step:5274... Training loss:1.9544... 0.1931 sec/batch\n",
      "Epoch:19/20... Training Step:5275... Training loss:1.9773... 0.1954 sec/batch\n",
      "Epoch:19/20... Training Step:5276... Training loss:1.9754... 0.2050 sec/batch\n",
      "Epoch:19/20... Training Step:5277... Training loss:1.9735... 0.2033 sec/batch\n",
      "Epoch:19/20... Training Step:5278... Training loss:1.9895... 0.1962 sec/batch\n",
      "Epoch:19/20... Training Step:5279... Training loss:2.0031... 0.2012 sec/batch\n",
      "Epoch:19/20... Training Step:5280... Training loss:1.9876... 0.1961 sec/batch\n",
      "Epoch:19/20... Training Step:5281... Training loss:2.0027... 0.1959 sec/batch\n",
      "Epoch:19/20... Training Step:5282... Training loss:1.9884... 0.2080 sec/batch\n",
      "Epoch:19/20... Training Step:5283... Training loss:1.9683... 0.1945 sec/batch\n",
      "Epoch:19/20... Training Step:5284... Training loss:1.9739... 0.1948 sec/batch\n",
      "Epoch:19/20... Training Step:5285... Training loss:1.9839... 0.1973 sec/batch\n",
      "Epoch:19/20... Training Step:5286... Training loss:1.9435... 0.2037 sec/batch\n",
      "Epoch:19/20... Training Step:5287... Training loss:1.9565... 0.1918 sec/batch\n",
      "Epoch:19/20... Training Step:5288... Training loss:1.9656... 0.2110 sec/batch\n",
      "Epoch:19/20... Training Step:5289... Training loss:1.9719... 0.1923 sec/batch\n",
      "Epoch:19/20... Training Step:5290... Training loss:1.9501... 0.2271 sec/batch\n",
      "Epoch:19/20... Training Step:5291... Training loss:1.9381... 0.1981 sec/batch\n",
      "Epoch:19/20... Training Step:5292... Training loss:1.9518... 0.2135 sec/batch\n",
      "Epoch:19/20... Training Step:5293... Training loss:1.9634... 0.1926 sec/batch\n",
      "Epoch:19/20... Training Step:5294... Training loss:1.9561... 0.1942 sec/batch\n",
      "Epoch:19/20... Training Step:5295... Training loss:1.9147... 0.2069 sec/batch\n",
      "Epoch:19/20... Training Step:5296... Training loss:1.9522... 0.2108 sec/batch\n",
      "Epoch:19/20... Training Step:5297... Training loss:1.9446... 0.1997 sec/batch\n",
      "Epoch:19/20... Training Step:5298... Training loss:1.9512... 0.1969 sec/batch\n",
      "Epoch:19/20... Training Step:5299... Training loss:1.9730... 0.2000 sec/batch\n",
      "Epoch:19/20... Training Step:5300... Training loss:1.9595... 0.1916 sec/batch\n",
      "Epoch:19/20... Training Step:5301... Training loss:1.9260... 0.2037 sec/batch\n",
      "Epoch:19/20... Training Step:5302... Training loss:1.9325... 0.1916 sec/batch\n",
      "Epoch:19/20... Training Step:5303... Training loss:1.9619... 0.2026 sec/batch\n",
      "Epoch:19/20... Training Step:5304... Training loss:2.0000... 0.2053 sec/batch\n",
      "Epoch:19/20... Training Step:5305... Training loss:2.0007... 0.1919 sec/batch\n",
      "Epoch:19/20... Training Step:5306... Training loss:1.9572... 0.1953 sec/batch\n",
      "Epoch:19/20... Training Step:5307... Training loss:1.9727... 0.2044 sec/batch\n",
      "Epoch:19/20... Training Step:5308... Training loss:1.9814... 0.1988 sec/batch\n",
      "Epoch:19/20... Training Step:5309... Training loss:1.9734... 0.2017 sec/batch\n",
      "Epoch:19/20... Training Step:5310... Training loss:1.9802... 0.2088 sec/batch\n",
      "Epoch:19/20... Training Step:5311... Training loss:1.9871... 0.1936 sec/batch\n",
      "Epoch:19/20... Training Step:5312... Training loss:1.9675... 0.2025 sec/batch\n",
      "Epoch:19/20... Training Step:5313... Training loss:1.9972... 0.1948 sec/batch\n",
      "Epoch:19/20... Training Step:5314... Training loss:1.9762... 0.1952 sec/batch\n",
      "Epoch:19/20... Training Step:5315... Training loss:1.9538... 0.1950 sec/batch\n",
      "Epoch:19/20... Training Step:5316... Training loss:1.9967... 0.1969 sec/batch\n",
      "Epoch:19/20... Training Step:5317... Training loss:1.9803... 0.1922 sec/batch\n",
      "Epoch:19/20... Training Step:5318... Training loss:2.0128... 0.2059 sec/batch\n",
      "Epoch:19/20... Training Step:5319... Training loss:1.9528... 0.1920 sec/batch\n",
      "Epoch:19/20... Training Step:5320... Training loss:1.9918... 0.1912 sec/batch\n",
      "Epoch:19/20... Training Step:5321... Training loss:1.9882... 0.2015 sec/batch\n",
      "Epoch:19/20... Training Step:5322... Training loss:1.9874... 0.2046 sec/batch\n",
      "Epoch:19/20... Training Step:5323... Training loss:2.0041... 0.2017 sec/batch\n",
      "Epoch:19/20... Training Step:5324... Training loss:1.9848... 0.2103 sec/batch\n",
      "Epoch:19/20... Training Step:5325... Training loss:2.0352... 0.2088 sec/batch\n",
      "Epoch:19/20... Training Step:5326... Training loss:1.9895... 0.2082 sec/batch\n",
      "Epoch:19/20... Training Step:5327... Training loss:2.0118... 0.1935 sec/batch\n",
      "Epoch:19/20... Training Step:5328... Training loss:1.9742... 0.1967 sec/batch\n",
      "Epoch:19/20... Training Step:5329... Training loss:2.0018... 0.1980 sec/batch\n",
      "Epoch:19/20... Training Step:5330... Training loss:1.9852... 0.1928 sec/batch\n",
      "Epoch:19/20... Training Step:5331... Training loss:1.9813... 0.2124 sec/batch\n",
      "Epoch:19/20... Training Step:5332... Training loss:1.9579... 0.2087 sec/batch\n",
      "Epoch:19/20... Training Step:5333... Training loss:1.9434... 0.2024 sec/batch\n",
      "Epoch:19/20... Training Step:5334... Training loss:1.9957... 0.2014 sec/batch\n",
      "Epoch:19/20... Training Step:5335... Training loss:1.9586... 0.1957 sec/batch\n",
      "Epoch:19/20... Training Step:5336... Training loss:2.0024... 0.2009 sec/batch\n",
      "Epoch:19/20... Training Step:5337... Training loss:2.0075... 0.1969 sec/batch\n",
      "Epoch:19/20... Training Step:5338... Training loss:1.9744... 0.2019 sec/batch\n",
      "Epoch:19/20... Training Step:5339... Training loss:1.9891... 0.2077 sec/batch\n",
      "Epoch:19/20... Training Step:5340... Training loss:1.9884... 0.1987 sec/batch\n",
      "Epoch:19/20... Training Step:5341... Training loss:1.9856... 0.1949 sec/batch\n",
      "Epoch:19/20... Training Step:5342... Training loss:2.0013... 0.1950 sec/batch\n",
      "Epoch:19/20... Training Step:5343... Training loss:1.9699... 0.1937 sec/batch\n",
      "Epoch:19/20... Training Step:5344... Training loss:2.0204... 0.1939 sec/batch\n",
      "Epoch:19/20... Training Step:5345... Training loss:1.9734... 0.1916 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:19/20... Training Step:5346... Training loss:2.0152... 0.2042 sec/batch\n",
      "Epoch:19/20... Training Step:5347... Training loss:2.0270... 0.1923 sec/batch\n",
      "Epoch:19/20... Training Step:5348... Training loss:2.0287... 0.1989 sec/batch\n",
      "Epoch:19/20... Training Step:5349... Training loss:2.0359... 0.2015 sec/batch\n",
      "Epoch:19/20... Training Step:5350... Training loss:2.0683... 0.1939 sec/batch\n",
      "Epoch:19/20... Training Step:5351... Training loss:2.0233... 0.1933 sec/batch\n",
      "Epoch:19/20... Training Step:5352... Training loss:2.0159... 0.1924 sec/batch\n",
      "Epoch:19/20... Training Step:5353... Training loss:2.0389... 0.2157 sec/batch\n",
      "Epoch:19/20... Training Step:5354... Training loss:2.0443... 0.1929 sec/batch\n",
      "Epoch:19/20... Training Step:5355... Training loss:2.0189... 0.1950 sec/batch\n",
      "Epoch:19/20... Training Step:5356... Training loss:1.9842... 0.2030 sec/batch\n",
      "Epoch:19/20... Training Step:5357... Training loss:1.9953... 0.2067 sec/batch\n",
      "Epoch:19/20... Training Step:5358... Training loss:1.9920... 0.1912 sec/batch\n",
      "Epoch:19/20... Training Step:5359... Training loss:2.0036... 0.2148 sec/batch\n",
      "Epoch:19/20... Training Step:5360... Training loss:1.9893... 0.1925 sec/batch\n",
      "Epoch:19/20... Training Step:5361... Training loss:1.9977... 0.1948 sec/batch\n",
      "Epoch:19/20... Training Step:5362... Training loss:2.0548... 0.1930 sec/batch\n",
      "Epoch:19/20... Training Step:5363... Training loss:2.0048... 0.1940 sec/batch\n",
      "Epoch:19/20... Training Step:5364... Training loss:1.9724... 0.1971 sec/batch\n",
      "Epoch:19/20... Training Step:5365... Training loss:1.9877... 0.2017 sec/batch\n",
      "Epoch:19/20... Training Step:5366... Training loss:2.0157... 0.2059 sec/batch\n",
      "Epoch:19/20... Training Step:5367... Training loss:1.9655... 0.2084 sec/batch\n",
      "Epoch:19/20... Training Step:5368... Training loss:1.9782... 0.2040 sec/batch\n",
      "Epoch:19/20... Training Step:5369... Training loss:1.9800... 0.1933 sec/batch\n",
      "Epoch:19/20... Training Step:5370... Training loss:1.9958... 0.1941 sec/batch\n",
      "Epoch:19/20... Training Step:5371... Training loss:1.9624... 0.2075 sec/batch\n",
      "Epoch:19/20... Training Step:5372... Training loss:1.9808... 0.2016 sec/batch\n",
      "Epoch:19/20... Training Step:5373... Training loss:2.0062... 0.2203 sec/batch\n",
      "Epoch:19/20... Training Step:5374... Training loss:1.9587... 0.2088 sec/batch\n",
      "Epoch:19/20... Training Step:5375... Training loss:1.9789... 0.1921 sec/batch\n",
      "Epoch:19/20... Training Step:5376... Training loss:2.0122... 0.2084 sec/batch\n",
      "Epoch:19/20... Training Step:5377... Training loss:1.9830... 0.1919 sec/batch\n",
      "Epoch:19/20... Training Step:5378... Training loss:1.9764... 0.2074 sec/batch\n",
      "Epoch:19/20... Training Step:5379... Training loss:1.9780... 0.2059 sec/batch\n",
      "Epoch:19/20... Training Step:5380... Training loss:1.9711... 0.1918 sec/batch\n",
      "Epoch:19/20... Training Step:5381... Training loss:1.9514... 0.1949 sec/batch\n",
      "Epoch:19/20... Training Step:5382... Training loss:1.9729... 0.2098 sec/batch\n",
      "Epoch:19/20... Training Step:5383... Training loss:1.9501... 0.1923 sec/batch\n",
      "Epoch:19/20... Training Step:5384... Training loss:2.0207... 0.2009 sec/batch\n",
      "Epoch:19/20... Training Step:5385... Training loss:2.0057... 0.2023 sec/batch\n",
      "Epoch:19/20... Training Step:5386... Training loss:2.0291... 0.1932 sec/batch\n",
      "Epoch:19/20... Training Step:5387... Training loss:2.0244... 0.2204 sec/batch\n",
      "Epoch:19/20... Training Step:5388... Training loss:2.0275... 0.1920 sec/batch\n",
      "Epoch:19/20... Training Step:5389... Training loss:1.9878... 0.1999 sec/batch\n",
      "Epoch:19/20... Training Step:5390... Training loss:1.9853... 0.2021 sec/batch\n",
      "Epoch:19/20... Training Step:5391... Training loss:1.9961... 0.2108 sec/batch\n",
      "Epoch:19/20... Training Step:5392... Training loss:1.9751... 0.1939 sec/batch\n",
      "Epoch:19/20... Training Step:5393... Training loss:1.9654... 0.2106 sec/batch\n",
      "Epoch:19/20... Training Step:5394... Training loss:1.9955... 0.2041 sec/batch\n",
      "Epoch:19/20... Training Step:5395... Training loss:1.9852... 0.1924 sec/batch\n",
      "Epoch:19/20... Training Step:5396... Training loss:1.9845... 0.1924 sec/batch\n",
      "Epoch:19/20... Training Step:5397... Training loss:1.9692... 0.2067 sec/batch\n",
      "Epoch:19/20... Training Step:5398... Training loss:1.9693... 0.1918 sec/batch\n",
      "Epoch:19/20... Training Step:5399... Training loss:1.9629... 0.1979 sec/batch\n",
      "Epoch:19/20... Training Step:5400... Training loss:1.9937... 0.1977 sec/batch\n",
      "Epoch:19/20... Training Step:5401... Training loss:1.9891... 0.1912 sec/batch\n",
      "Epoch:19/20... Training Step:5402... Training loss:1.9982... 0.1923 sec/batch\n",
      "Epoch:19/20... Training Step:5403... Training loss:1.9893... 0.2063 sec/batch\n",
      "Epoch:19/20... Training Step:5404... Training loss:1.9802... 0.1969 sec/batch\n",
      "Epoch:19/20... Training Step:5405... Training loss:1.9859... 0.2015 sec/batch\n",
      "Epoch:19/20... Training Step:5406... Training loss:1.9614... 0.1921 sec/batch\n",
      "Epoch:19/20... Training Step:5407... Training loss:2.0104... 0.1931 sec/batch\n",
      "Epoch:19/20... Training Step:5408... Training loss:1.9938... 0.2177 sec/batch\n",
      "Epoch:19/20... Training Step:5409... Training loss:1.9976... 0.1957 sec/batch\n",
      "Epoch:19/20... Training Step:5410... Training loss:1.9836... 0.2059 sec/batch\n",
      "Epoch:19/20... Training Step:5411... Training loss:1.9810... 0.1934 sec/batch\n",
      "Epoch:19/20... Training Step:5412... Training loss:1.9942... 0.1934 sec/batch\n",
      "Epoch:19/20... Training Step:5413... Training loss:2.0315... 0.1991 sec/batch\n",
      "Epoch:19/20... Training Step:5414... Training loss:1.9998... 0.2025 sec/batch\n",
      "Epoch:19/20... Training Step:5415... Training loss:2.0083... 0.2097 sec/batch\n",
      "Epoch:19/20... Training Step:5416... Training loss:2.0004... 0.1928 sec/batch\n",
      "Epoch:19/20... Training Step:5417... Training loss:1.9823... 0.2037 sec/batch\n",
      "Epoch:19/20... Training Step:5418... Training loss:2.0233... 0.2028 sec/batch\n",
      "Epoch:19/20... Training Step:5419... Training loss:1.9936... 0.1967 sec/batch\n",
      "Epoch:19/20... Training Step:5420... Training loss:1.9912... 0.1937 sec/batch\n",
      "Epoch:19/20... Training Step:5421... Training loss:2.0168... 0.1940 sec/batch\n",
      "Epoch:19/20... Training Step:5422... Training loss:2.0080... 0.2139 sec/batch\n",
      "Epoch:19/20... Training Step:5423... Training loss:2.0141... 0.1947 sec/batch\n",
      "Epoch:19/20... Training Step:5424... Training loss:1.9690... 0.2052 sec/batch\n",
      "Epoch:19/20... Training Step:5425... Training loss:1.9332... 0.1926 sec/batch\n",
      "Epoch:19/20... Training Step:5426... Training loss:1.9707... 0.1959 sec/batch\n",
      "Epoch:19/20... Training Step:5427... Training loss:1.9714... 0.1930 sec/batch\n",
      "Epoch:19/20... Training Step:5428... Training loss:1.9717... 0.1966 sec/batch\n",
      "Epoch:19/20... Training Step:5429... Training loss:1.9569... 0.2046 sec/batch\n",
      "Epoch:19/20... Training Step:5430... Training loss:1.9640... 0.1923 sec/batch\n",
      "Epoch:19/20... Training Step:5431... Training loss:1.9641... 0.1945 sec/batch\n",
      "Epoch:19/20... Training Step:5432... Training loss:1.9454... 0.1934 sec/batch\n",
      "Epoch:19/20... Training Step:5433... Training loss:1.9198... 0.2415 sec/batch\n",
      "Epoch:19/20... Training Step:5434... Training loss:1.9915... 0.1984 sec/batch\n",
      "Epoch:19/20... Training Step:5435... Training loss:1.9333... 0.1956 sec/batch\n",
      "Epoch:19/20... Training Step:5436... Training loss:1.9366... 0.1953 sec/batch\n",
      "Epoch:19/20... Training Step:5437... Training loss:1.9523... 0.1934 sec/batch\n",
      "Epoch:19/20... Training Step:5438... Training loss:1.9382... 0.1925 sec/batch\n",
      "Epoch:19/20... Training Step:5439... Training loss:1.9719... 0.2402 sec/batch\n",
      "Epoch:19/20... Training Step:5440... Training loss:1.9808... 0.1942 sec/batch\n",
      "Epoch:19/20... Training Step:5441... Training loss:1.9924... 0.2001 sec/batch\n",
      "Epoch:19/20... Training Step:5442... Training loss:2.0052... 0.1980 sec/batch\n",
      "Epoch:19/20... Training Step:5443... Training loss:1.9643... 0.2032 sec/batch\n",
      "Epoch:19/20... Training Step:5444... Training loss:1.9449... 0.1959 sec/batch\n",
      "Epoch:19/20... Training Step:5445... Training loss:1.9813... 0.2073 sec/batch\n",
      "Epoch:19/20... Training Step:5446... Training loss:1.9527... 0.2060 sec/batch\n",
      "Epoch:19/20... Training Step:5447... Training loss:1.9578... 0.2156 sec/batch\n",
      "Epoch:19/20... Training Step:5448... Training loss:1.9702... 0.1928 sec/batch\n",
      "Epoch:19/20... Training Step:5449... Training loss:1.9783... 0.1930 sec/batch\n",
      "Epoch:19/20... Training Step:5450... Training loss:1.9651... 0.1926 sec/batch\n",
      "Epoch:19/20... Training Step:5451... Training loss:1.9423... 0.2114 sec/batch\n",
      "Epoch:19/20... Training Step:5452... Training loss:1.9843... 0.1965 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:19/20... Training Step:5453... Training loss:1.9470... 0.2009 sec/batch\n",
      "Epoch:19/20... Training Step:5454... Training loss:1.9527... 0.1917 sec/batch\n",
      "Epoch:19/20... Training Step:5455... Training loss:1.9694... 0.2110 sec/batch\n",
      "Epoch:19/20... Training Step:5456... Training loss:1.9913... 0.2014 sec/batch\n",
      "Epoch:19/20... Training Step:5457... Training loss:1.9462... 0.1947 sec/batch\n",
      "Epoch:19/20... Training Step:5458... Training loss:1.9885... 0.2001 sec/batch\n",
      "Epoch:19/20... Training Step:5459... Training loss:1.9369... 0.1927 sec/batch\n",
      "Epoch:19/20... Training Step:5460... Training loss:1.9593... 0.2134 sec/batch\n",
      "Epoch:19/20... Training Step:5461... Training loss:1.9626... 0.2001 sec/batch\n",
      "Epoch:19/20... Training Step:5462... Training loss:2.0039... 0.1992 sec/batch\n",
      "Epoch:19/20... Training Step:5463... Training loss:2.0192... 0.1936 sec/batch\n",
      "Epoch:19/20... Training Step:5464... Training loss:1.9867... 0.2090 sec/batch\n",
      "Epoch:19/20... Training Step:5465... Training loss:2.0002... 0.1967 sec/batch\n",
      "Epoch:19/20... Training Step:5466... Training loss:1.9979... 0.1977 sec/batch\n",
      "Epoch:19/20... Training Step:5467... Training loss:1.9920... 0.2124 sec/batch\n",
      "Epoch:19/20... Training Step:5468... Training loss:1.9899... 0.2069 sec/batch\n",
      "Epoch:19/20... Training Step:5469... Training loss:1.9787... 0.1939 sec/batch\n",
      "Epoch:19/20... Training Step:5470... Training loss:1.9961... 0.1953 sec/batch\n",
      "Epoch:19/20... Training Step:5471... Training loss:1.9710... 0.2182 sec/batch\n",
      "Epoch:19/20... Training Step:5472... Training loss:2.0155... 0.1962 sec/batch\n",
      "Epoch:19/20... Training Step:5473... Training loss:1.9691... 0.1953 sec/batch\n",
      "Epoch:19/20... Training Step:5474... Training loss:1.9755... 0.1982 sec/batch\n",
      "Epoch:19/20... Training Step:5475... Training loss:2.0190... 0.1940 sec/batch\n",
      "Epoch:19/20... Training Step:5476... Training loss:1.9836... 0.1996 sec/batch\n",
      "Epoch:19/20... Training Step:5477... Training loss:1.9567... 0.2035 sec/batch\n",
      "Epoch:19/20... Training Step:5478... Training loss:2.0107... 0.2016 sec/batch\n",
      "Epoch:19/20... Training Step:5479... Training loss:1.9860... 0.2161 sec/batch\n",
      "Epoch:19/20... Training Step:5480... Training loss:2.0034... 0.1920 sec/batch\n",
      "Epoch:19/20... Training Step:5481... Training loss:2.0012... 0.1944 sec/batch\n",
      "Epoch:19/20... Training Step:5482... Training loss:1.9667... 0.1923 sec/batch\n",
      "Epoch:19/20... Training Step:5483... Training loss:1.9567... 0.2062 sec/batch\n",
      "Epoch:19/20... Training Step:5484... Training loss:1.9139... 0.2061 sec/batch\n",
      "Epoch:19/20... Training Step:5485... Training loss:1.9357... 0.1925 sec/batch\n",
      "Epoch:19/20... Training Step:5486... Training loss:1.9454... 0.1996 sec/batch\n",
      "Epoch:19/20... Training Step:5487... Training loss:1.9988... 0.1924 sec/batch\n",
      "Epoch:19/20... Training Step:5488... Training loss:1.9606... 0.1961 sec/batch\n",
      "Epoch:19/20... Training Step:5489... Training loss:1.9379... 0.2003 sec/batch\n",
      "Epoch:19/20... Training Step:5490... Training loss:1.9519... 0.2056 sec/batch\n",
      "Epoch:19/20... Training Step:5491... Training loss:1.9450... 0.1998 sec/batch\n",
      "Epoch:19/20... Training Step:5492... Training loss:1.9895... 0.2078 sec/batch\n",
      "Epoch:19/20... Training Step:5493... Training loss:2.0090... 0.1904 sec/batch\n",
      "Epoch:19/20... Training Step:5494... Training loss:1.9322... 0.2065 sec/batch\n",
      "Epoch:19/20... Training Step:5495... Training loss:1.9987... 0.1929 sec/batch\n",
      "Epoch:19/20... Training Step:5496... Training loss:1.9794... 0.1924 sec/batch\n",
      "Epoch:19/20... Training Step:5497... Training loss:1.9758... 0.1918 sec/batch\n",
      "Epoch:19/20... Training Step:5498... Training loss:1.9697... 0.1983 sec/batch\n",
      "Epoch:19/20... Training Step:5499... Training loss:1.9561... 0.2026 sec/batch\n",
      "Epoch:19/20... Training Step:5500... Training loss:1.9898... 0.1959 sec/batch\n",
      "Epoch:19/20... Training Step:5501... Training loss:1.9796... 0.2068 sec/batch\n",
      "Epoch:19/20... Training Step:5502... Training loss:1.9811... 0.1924 sec/batch\n",
      "Epoch:19/20... Training Step:5503... Training loss:1.9826... 0.1913 sec/batch\n",
      "Epoch:19/20... Training Step:5504... Training loss:1.9949... 0.2060 sec/batch\n",
      "Epoch:19/20... Training Step:5505... Training loss:2.0218... 0.1906 sec/batch\n",
      "Epoch:19/20... Training Step:5506... Training loss:1.9905... 0.1989 sec/batch\n",
      "Epoch:19/20... Training Step:5507... Training loss:1.9559... 0.2080 sec/batch\n",
      "Epoch:19/20... Training Step:5508... Training loss:1.9550... 0.1959 sec/batch\n",
      "Epoch:19/20... Training Step:5509... Training loss:1.9584... 0.2092 sec/batch\n",
      "Epoch:19/20... Training Step:5510... Training loss:1.9667... 0.1979 sec/batch\n",
      "Epoch:20/20... Training Step:5511... Training loss:2.0753... 0.1954 sec/batch\n",
      "Epoch:20/20... Training Step:5512... Training loss:1.9767... 0.1916 sec/batch\n",
      "Epoch:20/20... Training Step:5513... Training loss:1.9629... 0.1972 sec/batch\n",
      "Epoch:20/20... Training Step:5514... Training loss:2.0042... 0.1926 sec/batch\n",
      "Epoch:20/20... Training Step:5515... Training loss:1.9736... 0.2041 sec/batch\n",
      "Epoch:20/20... Training Step:5516... Training loss:1.9648... 0.1957 sec/batch\n",
      "Epoch:20/20... Training Step:5517... Training loss:1.9662... 0.2043 sec/batch\n",
      "Epoch:20/20... Training Step:5518... Training loss:1.9911... 0.2046 sec/batch\n",
      "Epoch:20/20... Training Step:5519... Training loss:1.9833... 0.1999 sec/batch\n",
      "Epoch:20/20... Training Step:5520... Training loss:1.9861... 0.1932 sec/batch\n",
      "Epoch:20/20... Training Step:5521... Training loss:1.9939... 0.2118 sec/batch\n",
      "Epoch:20/20... Training Step:5522... Training loss:1.9919... 0.1915 sec/batch\n",
      "Epoch:20/20... Training Step:5523... Training loss:1.9905... 0.1967 sec/batch\n",
      "Epoch:20/20... Training Step:5524... Training loss:1.9917... 0.1975 sec/batch\n",
      "Epoch:20/20... Training Step:5525... Training loss:1.9597... 0.2037 sec/batch\n",
      "Epoch:20/20... Training Step:5526... Training loss:1.9993... 0.1914 sec/batch\n",
      "Epoch:20/20... Training Step:5527... Training loss:1.9951... 0.2123 sec/batch\n",
      "Epoch:20/20... Training Step:5528... Training loss:2.0104... 0.2009 sec/batch\n",
      "Epoch:20/20... Training Step:5529... Training loss:1.9871... 0.2003 sec/batch\n",
      "Epoch:20/20... Training Step:5530... Training loss:1.9921... 0.1922 sec/batch\n",
      "Epoch:20/20... Training Step:5531... Training loss:2.0091... 0.1951 sec/batch\n",
      "Epoch:20/20... Training Step:5532... Training loss:1.9699... 0.1934 sec/batch\n",
      "Epoch:20/20... Training Step:5533... Training loss:1.9738... 0.1953 sec/batch\n",
      "Epoch:20/20... Training Step:5534... Training loss:2.0073... 0.2054 sec/batch\n",
      "Epoch:20/20... Training Step:5535... Training loss:1.9645... 0.1968 sec/batch\n",
      "Epoch:20/20... Training Step:5536... Training loss:1.9672... 0.2102 sec/batch\n",
      "Epoch:20/20... Training Step:5537... Training loss:1.9941... 0.1978 sec/batch\n",
      "Epoch:20/20... Training Step:5538... Training loss:1.9978... 0.2078 sec/batch\n",
      "Epoch:20/20... Training Step:5539... Training loss:2.0058... 0.1913 sec/batch\n",
      "Epoch:20/20... Training Step:5540... Training loss:2.0079... 0.1966 sec/batch\n",
      "Epoch:20/20... Training Step:5541... Training loss:1.9857... 0.2081 sec/batch\n",
      "Epoch:20/20... Training Step:5542... Training loss:1.9676... 0.1932 sec/batch\n",
      "Epoch:20/20... Training Step:5543... Training loss:1.9781... 0.2158 sec/batch\n",
      "Epoch:20/20... Training Step:5544... Training loss:1.9746... 0.2114 sec/batch\n",
      "Epoch:20/20... Training Step:5545... Training loss:2.0157... 0.1927 sec/batch\n",
      "Epoch:20/20... Training Step:5546... Training loss:1.9929... 0.1947 sec/batch\n",
      "Epoch:20/20... Training Step:5547... Training loss:1.9968... 0.2097 sec/batch\n",
      "Epoch:20/20... Training Step:5548... Training loss:2.0011... 0.2101 sec/batch\n",
      "Epoch:20/20... Training Step:5549... Training loss:1.9754... 0.1922 sec/batch\n",
      "Epoch:20/20... Training Step:5550... Training loss:1.9493... 0.1943 sec/batch\n",
      "Epoch:20/20... Training Step:5551... Training loss:1.9806... 0.2045 sec/batch\n",
      "Epoch:20/20... Training Step:5552... Training loss:2.0001... 0.2043 sec/batch\n",
      "Epoch:20/20... Training Step:5553... Training loss:2.0081... 0.2362 sec/batch\n",
      "Epoch:20/20... Training Step:5554... Training loss:1.9769... 0.2239 sec/batch\n",
      "Epoch:20/20... Training Step:5555... Training loss:2.0118... 0.2142 sec/batch\n",
      "Epoch:20/20... Training Step:5556... Training loss:2.0091... 0.1998 sec/batch\n",
      "Epoch:20/20... Training Step:5557... Training loss:2.0131... 0.2039 sec/batch\n",
      "Epoch:20/20... Training Step:5558... Training loss:1.9833... 0.2147 sec/batch\n",
      "Epoch:20/20... Training Step:5559... Training loss:1.9426... 0.1918 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20/20... Training Step:5560... Training loss:2.0081... 0.1957 sec/batch\n",
      "Epoch:20/20... Training Step:5561... Training loss:1.9639... 0.1939 sec/batch\n",
      "Epoch:20/20... Training Step:5562... Training loss:1.9723... 0.2069 sec/batch\n",
      "Epoch:20/20... Training Step:5563... Training loss:1.9814... 0.1929 sec/batch\n",
      "Epoch:20/20... Training Step:5564... Training loss:1.9288... 0.1922 sec/batch\n",
      "Epoch:20/20... Training Step:5565... Training loss:1.9499... 0.1938 sec/batch\n",
      "Epoch:20/20... Training Step:5566... Training loss:1.9514... 0.2152 sec/batch\n",
      "Epoch:20/20... Training Step:5567... Training loss:1.9559... 0.2058 sec/batch\n",
      "Epoch:20/20... Training Step:5568... Training loss:1.9641... 0.1937 sec/batch\n",
      "Epoch:20/20... Training Step:5569... Training loss:1.9840... 0.2154 sec/batch\n",
      "Epoch:20/20... Training Step:5570... Training loss:1.9794... 0.2064 sec/batch\n",
      "Epoch:20/20... Training Step:5571... Training loss:1.9845... 0.1932 sec/batch\n",
      "Epoch:20/20... Training Step:5572... Training loss:1.9580... 0.1997 sec/batch\n",
      "Epoch:20/20... Training Step:5573... Training loss:1.9395... 0.2112 sec/batch\n",
      "Epoch:20/20... Training Step:5574... Training loss:1.9626... 0.1990 sec/batch\n",
      "Epoch:20/20... Training Step:5575... Training loss:1.9575... 0.2014 sec/batch\n",
      "Epoch:20/20... Training Step:5576... Training loss:1.9200... 0.1941 sec/batch\n",
      "Epoch:20/20... Training Step:5577... Training loss:1.9214... 0.1918 sec/batch\n",
      "Epoch:20/20... Training Step:5578... Training loss:1.9411... 0.1989 sec/batch\n",
      "Epoch:20/20... Training Step:5579... Training loss:1.9423... 0.2059 sec/batch\n",
      "Epoch:20/20... Training Step:5580... Training loss:1.9298... 0.1919 sec/batch\n",
      "Epoch:20/20... Training Step:5581... Training loss:1.9201... 0.1935 sec/batch\n",
      "Epoch:20/20... Training Step:5582... Training loss:1.9344... 0.2047 sec/batch\n",
      "Epoch:20/20... Training Step:5583... Training loss:1.9355... 0.1978 sec/batch\n",
      "Epoch:20/20... Training Step:5584... Training loss:1.9312... 0.2261 sec/batch\n",
      "Epoch:20/20... Training Step:5585... Training loss:1.8998... 0.2027 sec/batch\n",
      "Epoch:20/20... Training Step:5586... Training loss:1.9231... 0.2046 sec/batch\n",
      "Epoch:20/20... Training Step:5587... Training loss:1.9203... 0.1962 sec/batch\n",
      "Epoch:20/20... Training Step:5588... Training loss:1.9258... 0.2048 sec/batch\n",
      "Epoch:20/20... Training Step:5589... Training loss:1.9495... 0.2145 sec/batch\n",
      "Epoch:20/20... Training Step:5590... Training loss:1.9398... 0.2075 sec/batch\n",
      "Epoch:20/20... Training Step:5591... Training loss:1.9048... 0.1912 sec/batch\n",
      "Epoch:20/20... Training Step:5592... Training loss:1.9103... 0.1930 sec/batch\n",
      "Epoch:20/20... Training Step:5593... Training loss:1.9440... 0.1951 sec/batch\n",
      "Epoch:20/20... Training Step:5594... Training loss:1.9844... 0.2039 sec/batch\n",
      "Epoch:20/20... Training Step:5595... Training loss:1.9719... 0.1962 sec/batch\n",
      "Epoch:20/20... Training Step:5596... Training loss:1.9395... 0.2152 sec/batch\n",
      "Epoch:20/20... Training Step:5597... Training loss:1.9652... 0.1927 sec/batch\n",
      "Epoch:20/20... Training Step:5598... Training loss:1.9630... 0.2011 sec/batch\n",
      "Epoch:20/20... Training Step:5599... Training loss:1.9532... 0.1951 sec/batch\n",
      "Epoch:20/20... Training Step:5600... Training loss:1.9641... 0.2158 sec/batch\n",
      "Epoch:20/20... Training Step:5601... Training loss:1.9603... 0.2005 sec/batch\n",
      "Epoch:20/20... Training Step:5602... Training loss:1.9516... 0.1906 sec/batch\n",
      "Epoch:20/20... Training Step:5603... Training loss:1.9775... 0.2059 sec/batch\n",
      "Epoch:20/20... Training Step:5604... Training loss:1.9577... 0.1935 sec/batch\n",
      "Epoch:20/20... Training Step:5605... Training loss:1.9450... 0.2064 sec/batch\n",
      "Epoch:20/20... Training Step:5606... Training loss:1.9723... 0.1929 sec/batch\n",
      "Epoch:20/20... Training Step:5607... Training loss:1.9623... 0.2164 sec/batch\n",
      "Epoch:20/20... Training Step:5608... Training loss:1.9912... 0.1917 sec/batch\n",
      "Epoch:20/20... Training Step:5609... Training loss:1.9450... 0.1920 sec/batch\n",
      "Epoch:20/20... Training Step:5610... Training loss:1.9726... 0.1926 sec/batch\n",
      "Epoch:20/20... Training Step:5611... Training loss:1.9624... 0.1946 sec/batch\n",
      "Epoch:20/20... Training Step:5612... Training loss:1.9681... 0.1947 sec/batch\n",
      "Epoch:20/20... Training Step:5613... Training loss:1.9909... 0.1997 sec/batch\n",
      "Epoch:20/20... Training Step:5614... Training loss:1.9683... 0.2021 sec/batch\n",
      "Epoch:20/20... Training Step:5615... Training loss:2.0134... 0.1936 sec/batch\n",
      "Epoch:20/20... Training Step:5616... Training loss:1.9702... 0.2065 sec/batch\n",
      "Epoch:20/20... Training Step:5617... Training loss:1.9857... 0.2060 sec/batch\n",
      "Epoch:20/20... Training Step:5618... Training loss:1.9456... 0.1923 sec/batch\n",
      "Epoch:20/20... Training Step:5619... Training loss:1.9717... 0.1936 sec/batch\n",
      "Epoch:20/20... Training Step:5620... Training loss:1.9714... 0.1918 sec/batch\n",
      "Epoch:20/20... Training Step:5621... Training loss:1.9518... 0.2157 sec/batch\n",
      "Epoch:20/20... Training Step:5622... Training loss:1.9358... 0.1941 sec/batch\n",
      "Epoch:20/20... Training Step:5623... Training loss:1.9214... 0.1943 sec/batch\n",
      "Epoch:20/20... Training Step:5624... Training loss:1.9802... 0.1927 sec/batch\n",
      "Epoch:20/20... Training Step:5625... Training loss:1.9378... 0.2050 sec/batch\n",
      "Epoch:20/20... Training Step:5626... Training loss:1.9760... 0.1928 sec/batch\n",
      "Epoch:20/20... Training Step:5627... Training loss:1.9844... 0.1967 sec/batch\n",
      "Epoch:20/20... Training Step:5628... Training loss:1.9549... 0.1922 sec/batch\n",
      "Epoch:20/20... Training Step:5629... Training loss:1.9812... 0.1921 sec/batch\n",
      "Epoch:20/20... Training Step:5630... Training loss:1.9636... 0.2108 sec/batch\n",
      "Epoch:20/20... Training Step:5631... Training loss:1.9736... 0.1919 sec/batch\n",
      "Epoch:20/20... Training Step:5632... Training loss:1.9783... 0.2053 sec/batch\n",
      "Epoch:20/20... Training Step:5633... Training loss:1.9459... 0.1922 sec/batch\n",
      "Epoch:20/20... Training Step:5634... Training loss:1.9865... 0.1928 sec/batch\n",
      "Epoch:20/20... Training Step:5635... Training loss:1.9626... 0.1966 sec/batch\n",
      "Epoch:20/20... Training Step:5636... Training loss:1.9925... 0.2043 sec/batch\n",
      "Epoch:20/20... Training Step:5637... Training loss:2.0006... 0.1917 sec/batch\n",
      "Epoch:20/20... Training Step:5638... Training loss:2.0073... 0.2065 sec/batch\n",
      "Epoch:20/20... Training Step:5639... Training loss:2.0163... 0.2099 sec/batch\n",
      "Epoch:20/20... Training Step:5640... Training loss:2.0425... 0.1926 sec/batch\n",
      "Epoch:20/20... Training Step:5641... Training loss:1.9918... 0.2062 sec/batch\n",
      "Epoch:20/20... Training Step:5642... Training loss:1.9890... 0.1948 sec/batch\n",
      "Epoch:20/20... Training Step:5643... Training loss:2.0119... 0.1943 sec/batch\n",
      "Epoch:20/20... Training Step:5644... Training loss:2.0102... 0.1994 sec/batch\n",
      "Epoch:20/20... Training Step:5645... Training loss:1.9928... 0.2109 sec/batch\n",
      "Epoch:20/20... Training Step:5646... Training loss:1.9670... 0.1994 sec/batch\n",
      "Epoch:20/20... Training Step:5647... Training loss:1.9751... 0.2022 sec/batch\n",
      "Epoch:20/20... Training Step:5648... Training loss:1.9802... 0.1917 sec/batch\n",
      "Epoch:20/20... Training Step:5649... Training loss:1.9780... 0.1948 sec/batch\n",
      "Epoch:20/20... Training Step:5650... Training loss:1.9714... 0.1910 sec/batch\n",
      "Epoch:20/20... Training Step:5651... Training loss:1.9765... 0.1925 sec/batch\n",
      "Epoch:20/20... Training Step:5652... Training loss:2.0504... 0.2029 sec/batch\n",
      "Epoch:20/20... Training Step:5653... Training loss:1.9870... 0.1914 sec/batch\n",
      "Epoch:20/20... Training Step:5654... Training loss:1.9574... 0.1961 sec/batch\n",
      "Epoch:20/20... Training Step:5655... Training loss:1.9796... 0.1988 sec/batch\n",
      "Epoch:20/20... Training Step:5656... Training loss:1.9909... 0.1970 sec/batch\n",
      "Epoch:20/20... Training Step:5657... Training loss:1.9477... 0.2036 sec/batch\n",
      "Epoch:20/20... Training Step:5658... Training loss:1.9640... 0.2033 sec/batch\n",
      "Epoch:20/20... Training Step:5659... Training loss:1.9652... 0.1937 sec/batch\n",
      "Epoch:20/20... Training Step:5660... Training loss:1.9703... 0.1941 sec/batch\n",
      "Epoch:20/20... Training Step:5661... Training loss:1.9372... 0.2114 sec/batch\n",
      "Epoch:20/20... Training Step:5662... Training loss:1.9557... 0.2011 sec/batch\n",
      "Epoch:20/20... Training Step:5663... Training loss:1.9818... 0.1923 sec/batch\n",
      "Epoch:20/20... Training Step:5664... Training loss:1.9353... 0.2039 sec/batch\n",
      "Epoch:20/20... Training Step:5665... Training loss:1.9531... 0.2074 sec/batch\n",
      "Epoch:20/20... Training Step:5666... Training loss:1.9926... 0.1903 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20/20... Training Step:5667... Training loss:1.9660... 0.1972 sec/batch\n",
      "Epoch:20/20... Training Step:5668... Training loss:1.9615... 0.1922 sec/batch\n",
      "Epoch:20/20... Training Step:5669... Training loss:1.9598... 0.1948 sec/batch\n",
      "Epoch:20/20... Training Step:5670... Training loss:1.9475... 0.2029 sec/batch\n",
      "Epoch:20/20... Training Step:5671... Training loss:1.9325... 0.1936 sec/batch\n",
      "Epoch:20/20... Training Step:5672... Training loss:1.9439... 0.1944 sec/batch\n",
      "Epoch:20/20... Training Step:5673... Training loss:1.9378... 0.1936 sec/batch\n",
      "Epoch:20/20... Training Step:5674... Training loss:1.9929... 0.2037 sec/batch\n",
      "Epoch:20/20... Training Step:5675... Training loss:1.9859... 0.2065 sec/batch\n",
      "Epoch:20/20... Training Step:5676... Training loss:2.0068... 0.2018 sec/batch\n",
      "Epoch:20/20... Training Step:5677... Training loss:2.0083... 0.2008 sec/batch\n",
      "Epoch:20/20... Training Step:5678... Training loss:2.0186... 0.1940 sec/batch\n",
      "Epoch:20/20... Training Step:5679... Training loss:1.9616... 0.1964 sec/batch\n",
      "Epoch:20/20... Training Step:5680... Training loss:1.9606... 0.1955 sec/batch\n",
      "Epoch:20/20... Training Step:5681... Training loss:1.9763... 0.1949 sec/batch\n",
      "Epoch:20/20... Training Step:5682... Training loss:1.9542... 0.1919 sec/batch\n",
      "Epoch:20/20... Training Step:5683... Training loss:1.9476... 0.1953 sec/batch\n",
      "Epoch:20/20... Training Step:5684... Training loss:1.9678... 0.2002 sec/batch\n",
      "Epoch:20/20... Training Step:5685... Training loss:1.9691... 0.1956 sec/batch\n",
      "Epoch:20/20... Training Step:5686... Training loss:1.9589... 0.2017 sec/batch\n",
      "Epoch:20/20... Training Step:5687... Training loss:1.9429... 0.1926 sec/batch\n",
      "Epoch:20/20... Training Step:5688... Training loss:1.9555... 0.1916 sec/batch\n",
      "Epoch:20/20... Training Step:5689... Training loss:1.9488... 0.1969 sec/batch\n",
      "Epoch:20/20... Training Step:5690... Training loss:1.9628... 0.1923 sec/batch\n",
      "Epoch:20/20... Training Step:5691... Training loss:1.9619... 0.1938 sec/batch\n",
      "Epoch:20/20... Training Step:5692... Training loss:1.9864... 0.2134 sec/batch\n",
      "Epoch:20/20... Training Step:5693... Training loss:1.9739... 0.2156 sec/batch\n",
      "Epoch:20/20... Training Step:5694... Training loss:1.9556... 0.1930 sec/batch\n",
      "Epoch:20/20... Training Step:5695... Training loss:1.9584... 0.1944 sec/batch\n",
      "Epoch:20/20... Training Step:5696... Training loss:1.9362... 0.1951 sec/batch\n",
      "Epoch:20/20... Training Step:5697... Training loss:1.9872... 0.2041 sec/batch\n",
      "Epoch:20/20... Training Step:5698... Training loss:1.9602... 0.1986 sec/batch\n",
      "Epoch:20/20... Training Step:5699... Training loss:1.9761... 0.1965 sec/batch\n",
      "Epoch:20/20... Training Step:5700... Training loss:1.9768... 0.1917 sec/batch\n",
      "Epoch:20/20... Training Step:5701... Training loss:1.9441... 0.1945 sec/batch\n",
      "Epoch:20/20... Training Step:5702... Training loss:1.9738... 0.1947 sec/batch\n",
      "Epoch:20/20... Training Step:5703... Training loss:2.0150... 0.1953 sec/batch\n",
      "Epoch:20/20... Training Step:5704... Training loss:1.9798... 0.1922 sec/batch\n",
      "Epoch:20/20... Training Step:5705... Training loss:1.9878... 0.1914 sec/batch\n",
      "Epoch:20/20... Training Step:5706... Training loss:1.9750... 0.1981 sec/batch\n",
      "Epoch:20/20... Training Step:5707... Training loss:1.9638... 0.1963 sec/batch\n",
      "Epoch:20/20... Training Step:5708... Training loss:1.9988... 0.1952 sec/batch\n",
      "Epoch:20/20... Training Step:5709... Training loss:1.9705... 0.1952 sec/batch\n",
      "Epoch:20/20... Training Step:5710... Training loss:1.9637... 0.1938 sec/batch\n",
      "Epoch:20/20... Training Step:5711... Training loss:1.9949... 0.2046 sec/batch\n",
      "Epoch:20/20... Training Step:5712... Training loss:1.9992... 0.1928 sec/batch\n",
      "Epoch:20/20... Training Step:5713... Training loss:1.9961... 0.1950 sec/batch\n",
      "Epoch:20/20... Training Step:5714... Training loss:1.9478... 0.2177 sec/batch\n",
      "Epoch:20/20... Training Step:5715... Training loss:1.9135... 0.1960 sec/batch\n",
      "Epoch:20/20... Training Step:5716... Training loss:1.9545... 0.1943 sec/batch\n",
      "Epoch:20/20... Training Step:5717... Training loss:1.9507... 0.1965 sec/batch\n",
      "Epoch:20/20... Training Step:5718... Training loss:1.9523... 0.1940 sec/batch\n",
      "Epoch:20/20... Training Step:5719... Training loss:1.9388... 0.1946 sec/batch\n",
      "Epoch:20/20... Training Step:5720... Training loss:1.9371... 0.2089 sec/batch\n",
      "Epoch:20/20... Training Step:5721... Training loss:1.9500... 0.2020 sec/batch\n",
      "Epoch:20/20... Training Step:5722... Training loss:1.9311... 0.1956 sec/batch\n",
      "Epoch:20/20... Training Step:5723... Training loss:1.9014... 0.2103 sec/batch\n",
      "Epoch:20/20... Training Step:5724... Training loss:1.9703... 0.2116 sec/batch\n",
      "Epoch:20/20... Training Step:5725... Training loss:1.9146... 0.2062 sec/batch\n",
      "Epoch:20/20... Training Step:5726... Training loss:1.9125... 0.2061 sec/batch\n",
      "Epoch:20/20... Training Step:5727... Training loss:1.9338... 0.1914 sec/batch\n",
      "Epoch:20/20... Training Step:5728... Training loss:1.9135... 0.1939 sec/batch\n",
      "Epoch:20/20... Training Step:5729... Training loss:1.9423... 0.1959 sec/batch\n",
      "Epoch:20/20... Training Step:5730... Training loss:1.9579... 0.2139 sec/batch\n",
      "Epoch:20/20... Training Step:5731... Training loss:1.9709... 0.2112 sec/batch\n",
      "Epoch:20/20... Training Step:5732... Training loss:1.9771... 0.2085 sec/batch\n",
      "Epoch:20/20... Training Step:5733... Training loss:1.9466... 0.1975 sec/batch\n",
      "Epoch:20/20... Training Step:5734... Training loss:1.9263... 0.1938 sec/batch\n",
      "Epoch:20/20... Training Step:5735... Training loss:1.9687... 0.1926 sec/batch\n",
      "Epoch:20/20... Training Step:5736... Training loss:1.9268... 0.1955 sec/batch\n",
      "Epoch:20/20... Training Step:5737... Training loss:1.9365... 0.1974 sec/batch\n",
      "Epoch:20/20... Training Step:5738... Training loss:1.9362... 0.1943 sec/batch\n",
      "Epoch:20/20... Training Step:5739... Training loss:1.9502... 0.2037 sec/batch\n",
      "Epoch:20/20... Training Step:5740... Training loss:1.9473... 0.1926 sec/batch\n",
      "Epoch:20/20... Training Step:5741... Training loss:1.9088... 0.1955 sec/batch\n",
      "Epoch:20/20... Training Step:5742... Training loss:1.9579... 0.1952 sec/batch\n",
      "Epoch:20/20... Training Step:5743... Training loss:1.9281... 0.2379 sec/batch\n",
      "Epoch:20/20... Training Step:5744... Training loss:1.9390... 0.1982 sec/batch\n",
      "Epoch:20/20... Training Step:5745... Training loss:1.9572... 0.1925 sec/batch\n",
      "Epoch:20/20... Training Step:5746... Training loss:1.9775... 0.2059 sec/batch\n",
      "Epoch:20/20... Training Step:5747... Training loss:1.9368... 0.2042 sec/batch\n",
      "Epoch:20/20... Training Step:5748... Training loss:1.9739... 0.1977 sec/batch\n",
      "Epoch:20/20... Training Step:5749... Training loss:1.9170... 0.2116 sec/batch\n",
      "Epoch:20/20... Training Step:5750... Training loss:1.9380... 0.1925 sec/batch\n",
      "Epoch:20/20... Training Step:5751... Training loss:1.9458... 0.1935 sec/batch\n",
      "Epoch:20/20... Training Step:5752... Training loss:1.9822... 0.2028 sec/batch\n",
      "Epoch:20/20... Training Step:5753... Training loss:1.9980... 0.1945 sec/batch\n",
      "Epoch:20/20... Training Step:5754... Training loss:1.9526... 0.1929 sec/batch\n",
      "Epoch:20/20... Training Step:5755... Training loss:1.9783... 0.2024 sec/batch\n",
      "Epoch:20/20... Training Step:5756... Training loss:1.9746... 0.2058 sec/batch\n",
      "Epoch:20/20... Training Step:5757... Training loss:1.9663... 0.1967 sec/batch\n",
      "Epoch:20/20... Training Step:5758... Training loss:1.9533... 0.2011 sec/batch\n",
      "Epoch:20/20... Training Step:5759... Training loss:1.9479... 0.1917 sec/batch\n",
      "Epoch:20/20... Training Step:5760... Training loss:1.9739... 0.1978 sec/batch\n",
      "Epoch:20/20... Training Step:5761... Training loss:1.9534... 0.1937 sec/batch\n",
      "Epoch:20/20... Training Step:5762... Training loss:1.9933... 0.2057 sec/batch\n",
      "Epoch:20/20... Training Step:5763... Training loss:1.9453... 0.1976 sec/batch\n",
      "Epoch:20/20... Training Step:5764... Training loss:1.9392... 0.1996 sec/batch\n",
      "Epoch:20/20... Training Step:5765... Training loss:1.9894... 0.1975 sec/batch\n",
      "Epoch:20/20... Training Step:5766... Training loss:1.9702... 0.1997 sec/batch\n",
      "Epoch:20/20... Training Step:5767... Training loss:1.9386... 0.1954 sec/batch\n",
      "Epoch:20/20... Training Step:5768... Training loss:1.9819... 0.1940 sec/batch\n",
      "Epoch:20/20... Training Step:5769... Training loss:1.9532... 0.2072 sec/batch\n",
      "Epoch:20/20... Training Step:5770... Training loss:1.9803... 0.1983 sec/batch\n",
      "Epoch:20/20... Training Step:5771... Training loss:1.9815... 0.1940 sec/batch\n",
      "Epoch:20/20... Training Step:5772... Training loss:1.9391... 0.1942 sec/batch\n",
      "Epoch:20/20... Training Step:5773... Training loss:1.9304... 0.1934 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20/20... Training Step:5774... Training loss:1.8888... 0.1936 sec/batch\n",
      "Epoch:20/20... Training Step:5775... Training loss:1.9232... 0.1977 sec/batch\n",
      "Epoch:20/20... Training Step:5776... Training loss:1.9297... 0.1982 sec/batch\n",
      "Epoch:20/20... Training Step:5777... Training loss:1.9770... 0.1910 sec/batch\n",
      "Epoch:20/20... Training Step:5778... Training loss:1.9346... 0.2001 sec/batch\n",
      "Epoch:20/20... Training Step:5779... Training loss:1.9203... 0.1930 sec/batch\n",
      "Epoch:20/20... Training Step:5780... Training loss:1.9296... 0.1991 sec/batch\n",
      "Epoch:20/20... Training Step:5781... Training loss:1.9257... 0.2011 sec/batch\n",
      "Epoch:20/20... Training Step:5782... Training loss:1.9690... 0.2150 sec/batch\n",
      "Epoch:20/20... Training Step:5783... Training loss:1.9972... 0.2013 sec/batch\n",
      "Epoch:20/20... Training Step:5784... Training loss:1.9122... 0.1968 sec/batch\n",
      "Epoch:20/20... Training Step:5785... Training loss:1.9717... 0.2003 sec/batch\n",
      "Epoch:20/20... Training Step:5786... Training loss:1.9518... 0.2007 sec/batch\n",
      "Epoch:20/20... Training Step:5787... Training loss:1.9601... 0.2139 sec/batch\n",
      "Epoch:20/20... Training Step:5788... Training loss:1.9579... 0.1972 sec/batch\n",
      "Epoch:20/20... Training Step:5789... Training loss:1.9396... 0.1954 sec/batch\n",
      "Epoch:20/20... Training Step:5790... Training loss:1.9693... 0.1929 sec/batch\n",
      "Epoch:20/20... Training Step:5791... Training loss:1.9501... 0.1966 sec/batch\n",
      "Epoch:20/20... Training Step:5792... Training loss:1.9647... 0.1944 sec/batch\n",
      "Epoch:20/20... Training Step:5793... Training loss:1.9708... 0.1987 sec/batch\n",
      "Epoch:20/20... Training Step:5794... Training loss:1.9712... 0.2030 sec/batch\n",
      "Epoch:20/20... Training Step:5795... Training loss:1.9966... 0.1928 sec/batch\n",
      "Epoch:20/20... Training Step:5796... Training loss:1.9661... 0.1955 sec/batch\n",
      "Epoch:20/20... Training Step:5797... Training loss:1.9370... 0.1982 sec/batch\n",
      "Epoch:20/20... Training Step:5798... Training loss:1.9423... 0.1920 sec/batch\n",
      "Epoch:20/20... Training Step:5799... Training loss:1.9275... 0.1920 sec/batch\n",
      "Epoch:20/20... Training Step:5800... Training loss:1.9391... 0.1916 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "# 每200步保存一个checkpoint\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps, \n",
    "               lstm_size=lstm_size, num_layers=num_layers, \n",
    "               learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)#  the maximum number of recent checkpoint files to keep.  As new files are created, older files are deleted.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss,\n",
    "                                                 model.final_state,\n",
    "                                                 model.optimizer],\n",
    "                                                 feed_dict=feed)\n",
    "            end = time.time()\n",
    "            print('Epoch:{}/{}...'.format(e+1, epochs),\n",
    "                  'Training Step:{}...'.format(counter),\n",
    "                  'Training loss:{:.4f}...'.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, 'checkpoints/i{}.ckpt'.format(counter))\n",
    "\n",
    "    saver.save(sess, 'checkpoints/i{}.ckpt'.format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/i5800.ckpt\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i5800.ckpt\n",
      "TheIN'TEN TATESLINS \n",
      "\n",
      "\"I what do you diveny hade to get to book.\n",
      "\n",
      "\"There to you're go all the plays, she sale a prissting of meromant, to cent to hiss to to a tower any wolkn the plice. The pirllarse was a bust tame. \n",
      "The moner the cealling was and said, \"How man, I'd going to take to but it it would and was the cars. The said, the mun who deart time a seed were have and he was stile at hear his and says \"Why as the better boy whone he says a letale said, \"What is she's the buct with shat into to be a chirdes thought of a strock to the somenter, and hus been the berided her hel and heard a bring said.\"\n",
      "\n",
      "\"This is a steation of the bort and went. The burn when houst a truck on she wenter. The stomp and then the women and theme aspiced, \"Ind a betione a let on the steep it with the some are stonting and soush a can what the strecent of the sentoon on the partoot all and his for her then tous and what he childen the formal out wifh the pas with the fine were of the parron was to the call thome. The man tead a coundy, she west to back of his say,, and so the crant on the said, so the stom to the tore should to and a betand of the propens ard the conter and says.\" The burst says \"The but was an oul, buck of the bulked on the back. The sheread wat stoped the back and ther he doung hore her hore. The comse could a callotion a llover tell a short.  \n",
      "It the burter is so stow the mear wish a serich sain.\n",
      "\n",
      "\"I walk buting to be than the prige and say, \"What's this, and I have a light will but the boy on the metting and he wind a prichtrical buct. \n",
      "\n",
      "I dook hit bory when a come of comes and are the still say, \"If in thrigh and stacteds at a berandes to but with his stick.. \n",
      "The mather takns his from the but of the stores atd of the san asks the steck it the teck of a stares, so the mating the sent some to the pleash talked in the sither. \n",
      "\n",
      "The carst and asking to the sold. Then't were to asks the stills all and with her bean the wasted trisked hor hurbed,, the dirsh told and and aldie the sand take on the cour and had but and shoutds he would toll the planion........ \n",
      "\n",
      "The barterss an aster, \"I'll ase you hears on you.\"At him and thought at a seal, sterting the bad the sear, and the stoller, the seat would some at som and he says, \"I can't say the mintinn wither who was a still on the sigut for the saris tores.\"\n",
      "\n",
      "The barthe says, \"I'mllave her thank in and had said a than thoued, the concer see wo drever the said that the mon wolks and says, \"You man have yeard your fants,\" so he deatheres and a light of the blows. I'm nitely allay, I don't take the bottor a shele were with this, she some in at a pilich and to thit a bartain.\"\n",
      "\n",
      "The bart asked this sert in the sechen with her supend the part to harpe to tho stor to the tame of a born sare on the condine. \n",
      "\n",
      "And stens and stacks at the badisais of the poriced on the clungen, and they have a but what housed.\n",
      "\n",
      "\"Ind them?\"\n",
      "\n",
      "\"I say \"So,\" the condoress stoped into the clarinate a childer ard trening another sterpiciones with hourners and a prean who did a lawer and said \"Hewer.\" sees it, the pat when the bunding to a comperting to the told.\n",
      "\n",
      "\"Wath, a be would goon to see he spord of tere all ther. \n",
      "\n",
      "A clees think and says, \"They say the store to and who dow's say, \"I'm three weation.\"\n",
      "\n",
      "The seat and though the mas and tome the beroran and the wonder said, \"The sonere statted, \"I'll he thing. I'm now your that shor in your same and and held is to the calle of the borts and wears time, teen in had he dookn the carter. \"They was taking to the pottor shars in the chullerer weathen were an a back of the clare of her ald handed theme. \n",
      "\n",
      "\"The seater as shat the cordent says and she said, \"You wint the bursers a long. The cousts of the muthin says, \"Yo  mas were?\" \n",
      "\n",
      "\"The secound says. \"I's sound in to come out.\"  The can will wish a par a canticile ofer the start. She said, \"Whot's the man'r walk batt that with a stomation?\"\n",
      "\n",
      "\"Your armand it the ban to the bet were to but a loth and asking to a say.\n",
      "\n",
      "When the chunkans she says the bur here has a bean asked to a bat and says the ploctur stond on a comal to have a back of a time and strest it world a light and arter the porsice..\n",
      "\n",
      "\"I misted it's along to bady him the planter the sain takes ardigit and he still his wan hes hig told and three her buchears, and he was to the trook thingers her the boy whit her ald hem his beftre some that the bat and seid, \"Who said, \"Ho say a shell any saids, seet to she will a but of the can with arait. \n",
      "\"I'd looks to tell oun on the rignes in the cars at a stare. Howny wo have house a somettions, and a bark of the coman and a chulder and they has so and hom out a sead with the said,\"\" \n",
      "\n",
      "\"So she seed hit went and streass a latter ond time, he asked, \"When the dar said and ther was her buting to to the sowent. And he stopes it the poncers of the can of the commet is to a car to say all when and asked the chouthes were the press that the camper and her but he was stopped her boor. \n",
      "To asteld to the blond so to the battrow with tremered.\n",
      "\n",
      "\"That's shemate is shougd the bat that some a chatte were will be we could a lettle tom of his still to but them, a look of the batten. He was teelly becaude that thing into than the bor ask and have it the fart time his frean three teld themen the campon and so foush hore and allowed, her shot ald would never a computing the fight and whis to the comet to the cordor thoul on of the bard a bott cent of the formont was ont and asked it. \n",
      "\n",
      "The mantered a but sear of the store said the pirest are see at the compones of a bat of the stare. \n",
      "\n",
      "The blender walks aroung the motions of the somethe to the cante to her. \n",
      "\n",
      "The mants to the murtar a bunts and a sectecing in and asted her a bor what he sear in throog ofte of to comate to say, I's a look and sayd, \"When he wast a clat a coms ard would, and he had to home of men all showed and says, \"What, I hould you car alout the bour wenters the poritate to he polling times to thoued waterer. They help he said, \"Ind, to have \n"
     ]
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')\n",
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]]=0\n",
    "    p = p / np.sum(p)\n",
    "    char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return char\n",
    "\n",
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime='The '):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction,\n",
    "                                          model.final_state],\n",
    "                                          feed_dict=feed)\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                x[0,0] = c\n",
    "                feed = {model.inputs: x,\n",
    "                        model.keep_prob: 1.,\n",
    "                        model.initial_state: new_state}\n",
    "                preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "                c = pick_top_n(preds, len(vocab))\n",
    "                samples.append(int_to_vocab[c])\n",
    "\n",
    "    return ''.join(samples)\n",
    "\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "print(checkpoint)\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime='The')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv /root/code/nmt/Untitled  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfpy3]",
   "language": "python",
   "name": "conda-env-tfpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
